{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":14491787,"sourceType":"datasetVersion","datasetId":9245165},{"sourceId":289055161,"sourceType":"kernelVersion"},{"sourceId":170608,"sourceType":"modelInstanceVersion","modelInstanceId":145161,"modelId":167678},{"sourceId":172574,"sourceType":"modelInstanceVersion","modelInstanceId":146897,"modelId":169361},{"sourceId":191689,"sourceType":"modelInstanceVersion","modelInstanceId":163393,"modelId":185749},{"sourceId":257276,"sourceType":"modelInstanceVersion","modelInstanceId":194874,"modelId":216774},{"sourceId":510391,"sourceType":"modelInstanceVersion","modelInstanceId":404485,"modelId":422384}],"dockerImageVersionId":31236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-14T07:12:25.627187Z","iopub.execute_input":"2026-01-14T07:12:25.627407Z","iopub.status.idle":"2026-01-14T07:13:32.614264Z","shell.execute_reply.started":"2026-01-14T07:12:25.627389Z","shell.execute_reply":"2026-01-14T07:13:32.613734Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: keras 3.10.0\nUninstalling keras-3.10.0:\n  Successfully uninstalled keras-3.10.0\nFound existing installation: matplotlib 3.10.0\nUninstalling matplotlib-3.10.0:\n  Successfully uninstalled matplotlib-3.10.0\nFound existing installation: scikit-learn 1.6.1\nUninstalling scikit-learn-1.6.1:\n  Successfully uninstalled scikit-learn-1.6.1\nFound existing installation: tensorflow 2.19.0\nUninstalling tensorflow-2.19.0:\n  Successfully uninstalled tensorflow-2.19.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\nimport subprocess\nimport warnings\nwarnings.simplefilter('ignore')\n\ndef set_env(input_archive, temp_dir):\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir, exist_ok=True)\n        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n    \n    subprocess.run([\n        sys.executable, \n        '-m', \n        'pip', \n        'install', \n        '--no-index', \n        '--find-links', \n        f'{temp_dir}/wheels', \n        'unsloth', \n        'trl', \n        'vllm', \n        'openai_harmony'\n    ], check=True)\n\nset_env(\n    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz', \n    temp_dir='/kaggle/tmp/setup'\n)\n\nsubprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T07:13:32.615299Z","iopub.execute_input":"2026-01-14T07:13:32.615455Z","iopub.status.idle":"2026-01-14T07:17:02.485815Z","shell.execute_reply.started":"2026-01-14T07:13:32.615437Z","shell.execute_reply":"2026-01-14T07:17:02.485443Z"}},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/tmp/setup/wheels\nProcessing /kaggle/tmp/setup/wheels/unsloth-2025.12.9-py3-none-any.whl\nProcessing /kaggle/tmp/setup/wheels/trl-0.24.0-py3-none-any.whl\nProcessing /kaggle/tmp/setup/wheels/vllm-0.11.2-cp38-abi3-manylinux1_x86_64.whl\nProcessing /kaggle/tmp/setup/wheels/openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/tmp/setup/wheels/unsloth_zoo-2025.12.7-py3-none-any.whl (from unsloth)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\nProcessing /kaggle/tmp/setup/wheels/tyro-1.0.3-py3-none-any.whl (from unsloth)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\nProcessing /kaggle/tmp/setup/wheels/xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl (from unsloth)\nProcessing /kaggle/tmp/setup/wheels/bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (from unsloth)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\nProcessing /kaggle/tmp/setup/wheels/datasets-4.3.0-py3-none-any.whl (from unsloth)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.17.1)\nRequirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\nRequirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm) (2025.11.3)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm) (5.5.2)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.32.5)\nRequirement already satisfied: blake3 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.0.8)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm) (9.0.0)\nRequirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.22.1)\nRequirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.119.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm) (3.13.2)\nRequirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.109.1)\nRequirement already satisfied: pydantic>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.12.5)\nRequirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm) (11.3.0)\nProcessing /kaggle/tmp/setup/wheels/prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (from vllm)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.12.0)\nProcessing /kaggle/tmp/setup/wheels/lm_format_enforcer-0.11.3-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/diskcache-5.6.3-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/lark-1.2.2-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nRequirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.15.0)\nRequirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (3.20.1)\nProcessing /kaggle/tmp/setup/wheels/partial_json_parser-0.2.1.1.post7-py3-none-any.whl (from vllm)\nRequirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (26.2.1)\nProcessing /kaggle/tmp/setup/wheels/msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/gguf-0.17.1-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/mistral_common-1.8.8-py3-none-any.whl (from mistral_common[image]>=1.8.5->vllm)\nRequirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.12.0.88)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm) (6.0.3)\nRequirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.17.0)\nProcessing /kaggle/tmp/setup/wheels/setuptools-80.9.0-py3-none-any.whl (from vllm)\nRequirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm) (0.8.1)\nProcessing /kaggle/tmp/setup/wheels/compressed_tensors-0.12.2-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/depyf-0.20.0-py3-none-any.whl (from vllm)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm) (3.1.1)\nProcessing /kaggle/tmp/setup/wheels/watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nRequirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm) (4.0.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm) (1.15.3)\nRequirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from vllm) (1.13.0)\nProcessing /kaggle/tmp/setup/wheels/pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/anthropic-0.71.0-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/model_hosting_container_standards-0.1.12-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from vllm)\nRequirement already satisfied: ray>=2.48.0 in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (2.52.1)\nProcessing /kaggle/tmp/setup/wheels/torch-2.9.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from unsloth)\nProcessing /kaggle/tmp/setup/wheels/torchaudio-2.9.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/torchvision-0.24.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from unsloth)\nProcessing /kaggle/tmp/setup/wheels/flashinfer_python-0.5.2-py3-none-any.whl (from vllm)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (4.12.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (1.9.0)\nRequirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.17.0)\nRequirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.11.1)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (1.3.1)\nProcessing /kaggle/tmp/setup/wheels/loguru-0.7.3-py3-none-any.whl (from compressed-tensors==0.12.2->vllm)\nProcessing /kaggle/tmp/setup/wheels/astor-0.8.1-py2.py3-none-any.whl (from depyf==0.20.0->vllm)\nRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.20.0->vllm) (0.4.0)\nProcessing /kaggle/tmp/setup/wheels/apache_tvm_ffi-0.1.7-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (8.3.1)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cudnn_frontend-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cutlass_dsl-4.3.4-cp312-cp312-manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\nRequirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (12.575.51)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (0.9.0)\nProcessing /kaggle/tmp/setup/wheels/interegular-0.3.3-py37-none-any.whl (from lm-format-enforcer==0.11.3->vllm)\nProcessing /kaggle/tmp/setup/wheels/llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from numba==0.61.2->vllm)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\nRequirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nRequirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2025.10.0)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\nProcessing /kaggle/tmp/setup/wheels/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from unsloth)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (22.0.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\nProcessing /kaggle/tmp/setup/wheels/multiprocess-0.70.16-py312-none-any.whl (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\nProcessing /kaggle/tmp/setup/wheels/fsspec-2025.9.0-py3-none-any.whl (from torch>=2.4.0->unsloth)\nRequirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.48.0)\nProcessing /kaggle/tmp/setup/wheels/fastapi_cli-0.0.20-py3-none-any.whl (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\nRequirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\nRequirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.38.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.1rc0)\nRequirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (4.25.1)\nProcessing /kaggle/tmp/setup/wheels/pydantic_extra_types-2.10.6-py3-none-any.whl (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\nRequirement already satisfied: jmespath in /usr/local/lib/python3.12/dist-packages (from model-hosting-container-standards<1.0.0->vllm) (1.0.1)\nINFO: pip is looking at multiple versions of model-hosting-container-standards to determine which version is compatible with other requirements. This could take a while.\nProcessing /kaggle/tmp/setup/wheels/fastapi-0.128.0-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/annotated_doc-0.0.4-py3-none-any.whl (from fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: pydantic-settings>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.11.0)\nProcessing /kaggle/tmp/setup/wheels/starlette-0.50.0-py3-none-any.whl (from fastapi[standard]>=0.115.0->vllm)\nProcessing /kaggle/tmp/setup/wheels/supervisor-4.3.0-py2.py3-none-any.whl (from model-hosting-container-standards<1.0.0->vllm)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (0.4.2)\nINFO: pip is looking at multiple versions of ray to determine which version is compatible with other requirements. This could take a while.\nProcessing /kaggle/tmp/setup/wheels/ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl (from ray[cgraph]>=2.48.0->vllm)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.2)\nINFO: pip is looking at multiple versions of ray[cgraph] to determine which version is compatible with other requirements. This could take a while.\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (13.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2025.11.12)\nProcessing /kaggle/tmp/setup/wheels/torchao-0.15.0+cu128-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from unsloth_zoo>=2025.12.7->unsloth)\nProcessing /kaggle/tmp/setup/wheels/cut_cross_entropy-25.1.1-py3-none-any.whl (from unsloth_zoo>=2025.12.7->unsloth)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.22.0)\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\nRequirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\nRequirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.20.0)\nProcessing /kaggle/tmp/setup/wheels/rich_toolkit-0.17.1-py3-none-any.whl (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nProcessing /kaggle/tmp/setup/wheels/fastapi_cloud_cli-0.8.0-py3-none-any.whl (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (2025.9.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.37.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.27.1)\nProcessing /kaggle/tmp/setup/wheels/cuda_python-13.1.1-py3-none-any.whl (from nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\nProcessing /kaggle/tmp/setup/wheels/pycountry-24.6.1-py3-none-any.whl (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.0.0->fastapi[standard]>=0.115.0->vllm) (1.1.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\nProcessing /kaggle/tmp/setup/wheels/httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nProcessing /kaggle/tmp/setup/wheels/uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3)\nProcessing /kaggle/tmp/setup/wheels/cuda_bindings-13.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\nProcessing /kaggle/tmp/setup/wheels/cuda_pathfinder-1.3.3-py3-none-any.whl (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\nProcessing /kaggle/tmp/setup/wheels/rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.42.1)\nProcessing /kaggle/tmp/setup/wheels/fastar-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\nInstalling collected packages: torchao, supervisor, uvloop, triton, setuptools, setproctitle, rignore, pycountry, pybase64, partial-json-parser, outlines_core, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cudnn-frontend, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multiprocess, msgspec, loguru, llvmlite, llguidance, lark, interegular, httptools, gguf, fsspec, fastar, diskcache, cuda-pathfinder, cbor2, astor, apache-tvm-ffi, annotated-doc, watchfiles, tyro, starlette, nvidia-cusparse-cu12, nvidia-cufft-cu12, numba, depyf, cuda-bindings, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai_harmony, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, cuda-python, anthropic, torch, ray, nvidia-cutlass-dsl, model-hosting-container-standards, fastapi-cloud-cli, fastapi-cli, datasets, xgrammar, xformers, torchvision, torchaudio, mistral_common, flashinfer-python, cut_cross_entropy, compressed-tensors, bitsandbytes, trl, unsloth_zoo, vllm, unsloth\n  Attempting uninstall: torchao\n    Found existing installation: torchao 0.10.0\n    Uninstalling torchao-0.10.0:\n      Successfully uninstalled torchao-0.10.0\n  Attempting uninstall: triton\n    Found existing installation: triton 3.4.0\n    Uninstalling triton-3.4.0:\n      Successfully uninstalled triton-3.4.0\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.2.0\n    Uninstalling setuptools-75.2.0:\n      Successfully uninstalled setuptools-75.2.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.6.77\n    Uninstalling nvidia-nvtx-cu12-12.6.77:\n      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n  Attempting uninstall: nvidia-nvshmem-cu12\n    Found existing installation: nvidia-nvshmem-cu12 3.4.5\n    Uninstalling nvidia-nvshmem-cu12-3.4.5:\n      Successfully uninstalled nvidia-nvshmem-cu12-3.4.5\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.27.3\n    Uninstalling nvidia-nccl-cu12-2.27.3:\n      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufile-cu12\n    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.18\n    Uninstalling multiprocess-0.70.18:\n      Successfully uninstalled multiprocess-0.70.18\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.43.0\n    Uninstalling llvmlite-0.43.0:\n      Successfully uninstalled llvmlite-0.43.0\n  Attempting uninstall: lark\n    Found existing installation: lark 1.3.0\n    Uninstalling lark-1.3.0:\n      Successfully uninstalled lark-1.3.0\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.10.0\n    Uninstalling fsspec-2025.10.0:\n      Successfully uninstalled fsspec-2025.10.0\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.48.0\n    Uninstalling starlette-0.48.0:\n      Successfully uninstalled starlette-0.48.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: numba\n    Found existing installation: numba 0.60.0\n    Uninstalling numba-0.60.0:\n      Successfully uninstalled numba-0.60.0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.119.1\n    Uninstalling fastapi-0.119.1:\n      Successfully uninstalled fastapi-0.119.1\n  Attempting uninstall: cuda-python\n    Found existing installation: cuda-python 12.6.2.post1\n    Uninstalling cuda-python-12.6.2.post1:\n      Successfully uninstalled cuda-python-12.6.2.post1\n  Attempting uninstall: torch\n    Found existing installation: torch 2.8.0+cu126\n    Uninstalling torch-2.8.0+cu126:\n      Successfully uninstalled torch-2.8.0+cu126\n  Attempting uninstall: ray\n    Found existing installation: ray 2.52.1\n    Uninstalling ray-2.52.1:\n      Successfully uninstalled ray-2.52.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.4.1\n    Uninstalling datasets-4.4.1:\n      Successfully uninstalled datasets-4.4.1\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.23.0+cu126\n    Uninstalling torchvision-0.23.0+cu126:\n      Successfully uninstalled torchvision-0.23.0+cu126\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.8.0+cu126\n    Uninstalling torchaudio-2.8.0+cu126:\n      Successfully uninstalled torchaudio-2.8.0+cu126\nSuccessfully installed annotated-doc-0.0.4 anthropic-0.71.0 apache-tvm-ffi-0.1.7 astor-0.8.1 bitsandbytes-0.49.0 cbor2-5.7.1 compressed-tensors-0.12.2 cuda-bindings-13.1.1 cuda-pathfinder-1.3.3 cuda-python-13.1.1 cut_cross_entropy-25.1.1 datasets-4.3.0 depyf-0.20.0 diskcache-5.6.3 fastapi-0.128.0 fastapi-cli-0.0.20 fastapi-cloud-cli-0.8.0 fastar-0.8.0 flashinfer-python-0.5.2 fsspec-2025.9.0 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 lark-1.2.2 llguidance-1.3.0 llvmlite-0.44.0 lm-format-enforcer-0.11.3 loguru-0.7.3 mistral_common-1.8.8 model-hosting-container-standards-0.1.12 msgspec-0.20.0 multiprocess-0.70.16 numba-0.61.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-frontend-1.17.0 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cutlass-dsl-4.3.4 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 openai_harmony-0.0.8 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post7 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.3 pycountry-24.6.1 pydantic-extra-types-2.10.6 ray-2.53.0 rich-toolkit-0.17.1 rignore-0.7.6 setproctitle-1.3.7 setuptools-80.9.0 starlette-0.50.0 supervisor-4.3.0 torch-2.9.0+cu128 torchao-0.15.0+cu128 torchaudio-2.9.0+cu128 torchvision-0.24.0+cu128 triton-3.5.0 trl-0.24.0 tyro-1.0.3 unsloth-2025.12.9 unsloth_zoo-2025.12.7 uvloop-0.22.1 vllm-0.11.2 watchfiles-1.1.1 xformers-0.0.33.post1 xgrammar-0.1.25\ncl100k_base.tiktoken\no200k_base.tiktoken\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkauldron 1.3.0 requires scikit-learn, which is not installed.\nkauldron 1.3.0 requires tensorflow, which is not installed.\nydata-profiling 4.18.0 requires matplotlib<=3.10,>=3.5, which is not installed.\npyldavis 3.4.1 requires scikit-learn>=1.0.0, which is not installed.\nstable-baselines3 2.1.0 requires matplotlib, which is not installed.\nsentence-transformers 5.1.1 requires scikit-learn, which is not installed.\nlibrosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\ncuml-cu12 25.6.0 requires scikit-learn>=1.5, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nbigframes 2.26.0 requires matplotlib>=3.7.1, which is not installed.\narviz 0.22.0 requires matplotlib>=3.8, which is not installed.\npynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\nshap 0.49.1 requires scikit-learn, which is not installed.\nfastai 2.8.4 requires matplotlib, which is not installed.\nfastai 2.8.4 requires scikit-learn, which is not installed.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, which is not installed.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.2.19 which is incompatible.\ncudf-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\ncuml-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npylibraft-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\ncuvs-cu12 25.6.1 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0+cu128 which is incompatible.\nrmm-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\npylibcudf-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['ls', '/kaggle/tmp/setup/tiktoken_encodings'], returncode=0)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"os.environ['TRANSFORMERS_NO_TF'] = '1'\nos.environ['TRANSFORMERS_NO_FLAX'] = '1'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\nos.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'\n\nimport gc\nimport re\nimport math\nimport time\nimport queue\nimport threading\nimport contextlib\nfrom typing import Optional\nfrom jupyter_client import KernelManager\nfrom collections import Counter, defaultdict\nfrom concurrent.futures import as_completed, ThreadPoolExecutor\n\nimport pandas as pd\nimport polars as pl\n\nfrom openai import OpenAI\n\nfrom openai_harmony import (\n    HarmonyEncodingName, \n    load_harmony_encoding, \n    SystemContent, \n    ReasoningEffort, \n    ToolNamespaceConfig, \n    Author, \n    Message, \n    Role, \n    TextContent, \n    Conversation\n)\n\nfrom transformers import set_seed\nimport kaggle_evaluation.aimo_3_inference_server\n\nclass CFG:\n\n    system_prompt = (\n        \"You are a world-class mathematical problem solver.\\n\"\n        \"Your goal is to produce the correct final answer with high reliability.\\n\\n\"\n    \n        \"OUTPUT RULE (STRICT):\\n\"\n        \"- The final answer must be a single non-negative integer between 0 and 99999.\\n\"\n        \"- You MUST output exactly one final answer in the form \\\\boxed{N}.\\n\\n\"\n        \n        \"### 1. CORE METHODOLOGY: PRE-COMPUTATION & PATTERN RECOGNITION\\n\"\n        \"- **Don't guess formulas.** Always start by writing a Python script to brute-force small cases (e.g., n=1 to 100).\\n\"\n        \"- **Ground Truth First:** Use the small-case results as the 'Ground Truth' to verify any mathematical hypothesis you form.\\n\"\n        \"- **Counter-examples:** Aggressively search for counter-examples. If a pattern holds for n=1..10 but fails at n=47, your formula is wrong.\\n\\n\"\n        \n        \"### 2. TECHNICAL & COMPUTATIONAL GUIDELINES\\n\"\n        \"- **Modular Arithmetic:** For large exponents (e.g., $3^{2025!}$), NEVER compute the full number. Use `pow(base, exp, mod)` in Python.\\n\"\n        \"- **Precision:** Be wary of floating-point errors. Use `fractions.Fraction`, `decimal.Decimal`, or `sympy` for exact arithmetic.\\n\"\n        \"- **Timeout Avoidance:** If a search space is huge ($10^{12}$), do not iterate linearly. Use number theory (CRT, Lucas Theorem) or meet-in-the-middle algorithms.\\n\\n\"\n        \n        \"### 3. CANDIDATE SEARCH (HARD LOGIC)\\n\"\n        \"- When optimizing a function f(N), do not just check the 'most likely' candidate. \\n\"\n        \"- **Candidate Coverage:** You MUST explicitly check neighbors (N-1, N+1), and distinct modular classes (e.g., N%3==0, N%3==1, N%3==2).\\n\"\n        \"- **Wide Scan:** If checking factors/divisors, scan a wide range of small primes before concluding.\\n\\n\"\n        \n        \"### 4. ONE-SHOT DEMONSTRATION (Generalization)\\n\"\n        \"User: Find the smallest integer n > 1 such that n^2 + n + 41 is composite.\\n\"\n        \"Assistant:\\n\"\n        \"**Plan:**\\n\"\n        \"1. I will write a Python function `is_prime(x)`.\\n\"\n        \"2. I will loop n from 2 to 100 to find the first failure case.\\n\"\n        \"3. I will NOT assume it is prime forever just because it looks like Euler's polynomial.\\n\"\n        \"**Code:**\\n\"\n        \"```python\\n\"\n        \"def is_prime(k):\\n\"\n        \"    if k < 2: return False\\n\"\n        \"    for i in range(2, int(k**0.5)+1):\\n\"\n        \"        if k % i == 0: return False\\n\"\n        \"    return True\\n\"\n        \"\\n\"\n        \"for n in range(2, 100):\\n\"\n        \"    val = n**2 + n + 41\\n\"\n        \"    if not is_prime(val):\\n\"\n        \"        print(f'Found composite at n={n}, val={val}')\\n\"\n        \"        break\\n\"\n        \"```\\n\"\n        \"**Output:** Found composite at n=40, val=1681\\n\"\n        \"**Final Answer:** \\\\boxed{40}\\n\\n\"\n        \n        \"### 5. EXECUTION STRUCTURE\\n\"\n        \"For the given problem, follow this format:\\n\"\n        \"1. **Analysis & Plan:** Briefly state the math approach.\\n\"\n        \"2. **Python Exploration:** Run small cases to see the pattern.\\n\"\n        \"3. **Candidates Generation:** Identify potential solutions or factors.\\n\"\n        \"4. **Verification Checklist:**\\n\"\n        \"   - [ ] Did I handle integer overflow/timeout?\\n\"\n        \"   - [ ] Did I verify the answer against small cases?\\n\"\n        \"   - [ ] Did I check edge cases (n=0, 1)?\\n\"\n        \"5. **Final Answer:** Output \\\\boxed{}.\"\n    )\n    \n    tool_prompt = (\n        'Use this tool to execute Python code. '\n        'Always verify your findings with code. '\n        'Before printing the final answer, verify edge cases and potential overflows.'\n    )\n    \n    preference_prompt = (\n        \"Tools available: `numpy`, `sympy`, `scipy`, `networkx`, `itertools`.\\n\"\n        \"Important: Use `pow(a, b, m)` for modular exponentiation.\\n\"\n        \"Construct a **Verification Checklist** before concluding.\"\n    )\n\n    served_model_name = 'gpt-oss'\n    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n    \n    kv_cache_dtype = 'fp8_e4m3'\n    dtype = 'auto'\n\n    high_problem_timeout = 900\n    base_problem_timeout = 300\n\n    notebook_limit = 17400\n    server_timeout = 180\n\n    session_timeout = 960\n    jupyter_timeout = 6\n    sandbox_timeout = 3\n\n    stream_interval = 200\n    context_tokens = 65536\n    buffer_tokens = 512\n    search_tokens = 32\n    top_logprobs = 5\n    batch_size = 256\n    early_stop = 4\n    attempts = 8\n    workers = 16\n    turns = 128\n    seed = 42\n\n    gpu_memory_utilization = 0.96\n    temperature = 1.0\n    min_p = 0.02\n    debug = True\n    debug_req = True\n    debug_resp = True\n    debug_limit = 3000\n\nset_seed(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T07:17:02.486425Z","iopub.execute_input":"2026-01-14T07:17:02.486554Z","iopub.status.idle":"2026-01-14T07:17:10.309479Z","shell.execute_reply.started":"2026-01-14T07:17:02.486542Z","shell.execute_reply":"2026-01-14T07:17:10.308989Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class AIMO3Template:\n\n    def __init__(self):\n        pass\n\n    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n        return (\n            SystemContent.new()\n            .with_model_identity(system_prompt)\n            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n            .with_tools(tool_config)\n        )\n\n    def apply_chat_template(\n        self, \n        system_prompt: str, \n        user_prompt: str, \n        tool_config: ToolNamespaceConfig\n    ) -> list[Message]:\n        system_content = self.get_system_content(system_prompt, tool_config)        \n        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n        return [system_message, user_message]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T07:17:10.310532Z","iopub.execute_input":"2026-01-14T07:17:10.310852Z","iopub.status.idle":"2026-01-14T07:17:10.314584Z","shell.execute_reply.started":"2026-01-14T07:17:10.310833Z","shell.execute_reply":"2026-01-14T07:17:10.314178Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class AIMO3Sandbox:\n\n    _port_lock = threading.Lock()\n    _next_port = 50000\n\n    @classmethod\n    def _get_next_ports(cls, count: int = 5) -> list[int]:\n        with cls._port_lock:\n            ports = list(range(cls._next_port, cls._next_port + count))\n            cls._next_port += count\n            return ports\n\n    def __init__(self, timeout: float):\n        self._default_timeout = timeout\n        self._owns_kernel = False\n        self._client = None\n        self._km = None\n        \n        ports = self._get_next_ports(5)\n\n        env = os.environ.copy()\n        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n        env['JUPYTER_PLATFORM_DIRS'] = '1'\n        env['PYTHONWARNINGS'] = 'ignore'\n        env['MPLBACKEND'] = 'Agg'\n\n        self._km = KernelManager()\n        self._km.shell_port = ports[0]\n        self._km.iopub_port = ports[1]\n        self._km.stdin_port = ports[2]\n        self._km.hb_port = ports[3]\n        self._km.control_port = ports[4]\n        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n\n        self._client = self._km.blocking_client()\n        self._client.start_channels()\n        self._client.wait_for_ready(timeout=self._default_timeout)\n        self._owns_kernel = True\n\n        self.execute(\n            'import math\\n'\n            'import numpy\\n'\n            'import sympy\\n'\n            'import itertools\\n'\n            'import collections\\n'\n            'import mpmath\\n'\n            'mpmath.mp.dps = 64\\n'\n        )\n\n    def _format_error(self, traceback: list[str]) -> str:\n        clean_lines = []\n        for frame in traceback:\n            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n                continue\n            clean_lines.append(clean_frame)\n        return ''.join(clean_lines)\n\n    def execute(self, code: str, timeout: float | None = None) -> str:\n        client = self._client\n        effective_timeout = timeout or self._default_timeout\n        \n        msg_id = client.execute(\n            code, \n            store_history=True, \n            allow_stdin=False, \n            stop_on_error=False\n        )\n\n        stdout_parts = []\n        stderr_parts = []\n        \n        start_time = time.time()\n\n        while True:\n            elapsed = time.time() - start_time\n\n            if elapsed > effective_timeout:\n                self._km.interrupt_kernel()\n\n                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n\n            try:\n                msg = client.get_iopub_msg(timeout=1.0)\n\n            except queue.Empty:\n                continue\n\n            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n                continue\n\n            msg_type = msg.get('msg_type')\n            content = msg.get('content', {})\n\n            if msg_type == 'stream':\n                text = content.get('text', '')\n\n                if content.get('name') == 'stdout':\n                    stdout_parts.append(text)\n\n                else:\n                    stderr_parts.append(text)\n\n            elif msg_type == 'error':\n                traceback_list = content.get('traceback', [])\n\n                stderr_parts.append(self._format_error(traceback_list))\n\n            elif msg_type in {'execute_result', 'display_data'}:\n                data = content.get('data', {})\n                text = data.get('text/plain')\n\n                if text:\n                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n\n            elif msg_type == 'status':\n                if content.get('execution_state') == 'idle':\n                    break\n\n        stdout = ''.join(stdout_parts)\n        stderr = ''.join(stderr_parts)\n\n        if stderr:\n            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n\n        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n\n    def close(self):\n        with contextlib.suppress(Exception):\n            if self._client:\n                self._client.stop_channels()\n\n        if self._owns_kernel and self._km is not None:\n            with contextlib.suppress(Exception):\n                self._km.shutdown_kernel(now=True)\n\n            with contextlib.suppress(Exception):\n                self._km.cleanup_resources()\n\n    def reset(self):\n        self.execute(\n            '%reset -f\\n'\n            'import math\\n'\n            'import numpy\\n'\n            'import sympy\\n'\n            'import itertools\\n'\n            'import collections\\n'\n            'import mpmath\\n'\n            'mpmath.mp.dps = 64\\n'\n        )\n\n    def __del__(self):\n        self.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T07:17:10.315173Z","iopub.execute_input":"2026-01-14T07:17:10.315333Z","iopub.status.idle":"2026-01-14T07:17:11.432903Z","shell.execute_reply.started":"2026-01-14T07:17:10.315319Z","shell.execute_reply":"2026-01-14T07:17:11.432465Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class AIMO3Tool:\n\n    def __init__(self, local_jupyter_timeout: float, tool_prompt: str, sandbox=None):\n        self._local_jupyter_timeout = local_jupyter_timeout\n        self._tool_prompt = tool_prompt\n        self._jupyter_session = sandbox\n        \n        self._owns_session = sandbox is None\n        \n        self._execution_lock = threading.Lock()\n        self._init_lock = threading.Lock()\n\n    def _ensure_session(self):\n\n        if self._jupyter_session is None:\n            with self._init_lock:\n                if self._jupyter_session is None:\n                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n\n    def _ensure_last_print(self, code: str) -> str:\n\n        lines = code.strip().split('\\n')\n\n        if not lines:\n            return code\n\n        last_line = lines[-1].strip()\n\n        if 'print' in last_line or 'import' in last_line:\n            return code\n\n        if not last_line:\n            return code\n\n        if last_line.startswith('#'):\n            return code\n\n        lines[-1] = 'print(' + last_line + ')'\n\n        return '\\n'.join(lines)\n\n    @property\n    def instruction(self) -> str:\n        return self._tool_prompt\n\n    @property\n    def tool_config(self) -> ToolNamespaceConfig:\n        return ToolNamespaceConfig(\n            name='python', \n            description=self.instruction, \n            tools=[]\n        )\n\n    def _make_response(self, output: str, channel: str | None = None) -> Message:\n        content = TextContent(text=output)\n        author = Author(role=Role.TOOL, name='python')\n        message = Message(author=author, content=[content]).with_recipient('assistant')\n\n        if channel:\n            message = message.with_channel(channel)\n\n        return message\n\n    def process_sync_plus(self, message: Message) -> list[Message]:\n        self._ensure_session()\n        raw_script = message.content[0].text\n        final_script = self._ensure_last_print(raw_script)\n\n        with self._execution_lock:\n            try:\n                output = self._jupyter_session.execute(final_script)\n\n            except TimeoutError as exc:\n                output = f'[ERROR] {exc}'\n\n        return [self._make_response(output, channel=message.channel)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T07:17:11.433571Z","iopub.execute_input":"2026-01-14T07:17:11.433700Z","iopub.status.idle":"2026-01-14T07:17:11.446117Z","shell.execute_reply.started":"2026-01-14T07:17:11.433688Z","shell.execute_reply":"2026-01-14T07:17:11.445781Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class AIMO3Solver:\n\n    def __init__(self, cfg, port: int = 8000):\n        self.cfg = cfg\n        self.port = port\n        self.base_url = f'http://0.0.0.0:{port}/v1'\n        self.api_key = 'sk-local'\n        self.template = AIMO3Template()\n        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n    \n        self._preload_model_weights()\n        \n        self.server_process = self._start_server()\n    \n        self.client = OpenAI(\n            base_url=self.base_url, \n            api_key=self.api_key, \n            timeout=self.cfg.session_timeout\n        )\n    \n        self._wait_for_server()\n        self._initialize_kernels()\n    \n        self.notebook_start_time = time.time()\n        self.problems_remaining = 50\n    \n    def _preload_model_weights(self) -> None:\n        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n        start_time = time.time()\n        \n        files_to_load = []\n        total_size = 0\n    \n        for root, _, files in os.walk(self.cfg.model_path):\n            for file_name in files:\n                file_path = os.path.join(root, file_name)\n    \n                if os.path.isfile(file_path):\n                    files_to_load.append(file_path)\n                    total_size += os.path.getsize(file_path)\n    \n        def _read_file(path: str) -> None:\n    \n            with open(path, 'rb') as file_object:\n                while file_object.read(1024 * 1024 * 1024):\n                    pass\n    \n        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n            list(executor.map(_read_file, files_to_load))\n    \n        elapsed = time.time() - start_time\n        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n    \n    def _start_server(self) -> subprocess.Popen:\n        cmd = [\n            sys.executable, \n            '-m', \n            'vllm.entrypoints.openai.api_server', \n            '--seed', \n            str(self.cfg.seed), \n            '--model', \n            self.cfg.model_path, \n            '--served-model-name', \n            self.cfg.served_model_name, \n            '--tensor-parallel-size', \n            '1', \n            '--max-num-seqs', \n            str(self.cfg.batch_size), \n            '--gpu-memory-utilization', \n            str(self.cfg.gpu_memory_utilization), \n            '--host', \n            '0.0.0.0', \n            '--port', \n            str(self.port), \n            '--dtype', \n            self.cfg.dtype, \n            '--kv-cache-dtype', \n            self.cfg.kv_cache_dtype, \n            '--max-model-len', \n            str(self.cfg.context_tokens), \n            '--stream-interval', \n            str(self.cfg.stream_interval), \n            '--async-scheduling', \n            '--disable-log-stats', \n            '--enable-prefix-caching'\n        ]\n    \n        self.log_file = open('vllm_server.log', 'w')\n    \n        return subprocess.Popen(\n            cmd, \n            stdout=self.log_file, \n            stderr=subprocess.STDOUT, \n            start_new_session=True\n        )\n    \n    def _wait_for_server(self):\n        print('Waiting for vLLM server...')\n        start_time = time.time()\n    \n        for _ in range(self.cfg.server_timeout):\n            return_code = self.server_process.poll()\n    \n            if return_code is not None:\n                self.log_file.flush()\n    \n                with open('vllm_server.log', 'r') as log_file:\n                    logs = log_file.read()\n    \n                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n    \n            try:\n                self.client.models.list()\n                elapsed = time.time() - start_time\n                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n    \n                return\n    \n            except Exception:\n                time.sleep(1)\n    \n        raise RuntimeError('Server failed to start (timeout).\\n')\n    \n    def _initialize_kernels(self) -> None:\n        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n        start_time = time.time()\n    \n        self.sandbox_pool = queue.Queue()\n    \n        def _create_sandbox():\n            \n            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n    \n        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n    \n            for future in as_completed(futures):\n                self.sandbox_pool.put(future.result())\n    \n        elapsed = time.time() - start_time\n        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n    \n    def _scan_for_answer(self, text: str) -> int | None:\n        pattern = r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}'\n        matches = re.findall(pattern, text)\n    \n        if matches:\n            try:\n                clean_value = matches[-1].replace(',', '')\n                value = int(clean_value)\n    \n                if 0 <= value <= 99999:\n                    return value\n    \n            except ValueError:\n                pass\n                \n        pattern = r'final\\s+answer\\s+is\\s*([0-9,]+)'\n        matches = re.findall(pattern, text, re.IGNORECASE)\n    \n        if matches:\n            try:\n                clean_value = matches[-1].replace(',', '')\n                value = int(clean_value)\n    \n                if 0 <= value <= 99999:\n                    return value\n    \n            except ValueError:\n                pass\n    \n        return None\n    \n    def _compute_mean_entropy(self, logprobs_buffer: list) -> float:\n        if not logprobs_buffer:\n            return float('inf')\n    \n        total_entropy = 0.0\n        token_count = 0\n    \n        for top_logprobs_dict in logprobs_buffer:\n            \n            if not isinstance(top_logprobs_dict, dict):\n                continue\n            \n            if not top_logprobs_dict:\n                continue\n            \n            token_entropy = 0.0\n            \n            for token_str, log_prob in top_logprobs_dict.items():\n                prob = math.exp(log_prob)\n                \n                if prob > 0:\n                    token_entropy -= prob * math.log2(prob)\n            \n            total_entropy += token_entropy\n            token_count += 1\n    \n        if token_count == 0:\n            return float('inf')\n    \n        return total_entropy / token_count\n\n    def _get_debug_snippet(self, text: str) -> str:\n        limit = self.cfg.debug_limit\n        if len(text) <= limit:\n            return text\n        \n        head = text[:100]\n        tail_len = limit - 100\n        tail = text[-tail_len:]\n        return f\"{head}\\n ... \\n{tail}\"\n\n    def _format_markdown_content(self, text: str, mode: str = \"quote\") -> str:\n        \"\"\"\n         #\n        mode: \"code\" ()  \"quote\" ()\n        \"\"\"\n        if not text:\n            return \"\"\n            \n        # 1.  #  #  prepend \\\n        lines = text.split('\\n')\n        escaped_lines = [f\"\\\\{line}\" if line.startswith('#') else line for line in lines]\n        processed_text = '\\n'.join(escaped_lines)\n\n        # 2. \n        if mode == \"text\":\n            return f\"```text\\n{processed_text}\\n```\\n\"\n        elif mode == \"python\":\n            return f\"```python\\n{processed_text}\\n```\\n\"\n        elif mode == \"quote\":\n            #  \"> \"\n            return '\\n'.join([f\"> {line}\" for line in escaped_lines]) + \"\\n\"\n        elif mode == \"\":\n            return processed_text + \"\\n\"\n        else:\n            return f\"```\\n{processed_text}\\n```\\n\"\n\n    def _process_attempt(\n        self, \n        problem: str, \n        system_prompt: str, \n        attempt_index: int, \n        stop_event: threading.Event, \n        deadline: float\n    ) -> dict:\n        ## DEBUG\n        attempt_log = []\n        if self.cfg.debug:\n            attempt_log.append(f\"## Attempt {attempt_index + 1}\\n\")\n\n        if stop_event.is_set() or time.time() > deadline:\n            return {\n                'Attempt': attempt_index + 1, \n                'Answer': None, \n                'Python Calls': 0, \n                'Python Errors': 0, \n                'Response Length': 0, \n                'Entropy': float('inf'),\n                'Log': \"\\n\".join(attempt_log)\n            }\n    \n        local_tool = None\n        sandbox = None\n        python_calls = 0\n        python_errors = 0\n        total_tokens = 0\n        final_answer = None\n        \n        logprobs_buffer = []\n    \n        attempt_seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n    \n        try:\n            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n    \n            local_tool = AIMO3Tool(\n                local_jupyter_timeout=self.cfg.jupyter_timeout, \n                tool_prompt=self.cfg.tool_prompt, \n                sandbox=sandbox\n            )\n    \n            encoding = self.encoding\n            messages = self.template.apply_chat_template(\n                system_prompt, \n                problem, \n                local_tool.tool_config\n            )\n    \n            conversation = Conversation.from_messages(messages)\n    \n            for turn_i in range(self.cfg.turns):\n                if stop_event.is_set() or time.time() > deadline:\n                    break\n    \n                prompt_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n    \n                if max_tokens < self.cfg.buffer_tokens:\n                    break\n\n                ## DEBUG\n                if self.cfg.debug and self.cfg.debug_req:\n                        # convert prompt_ids (tensors) back to readable text\n                        # which includes LLM special symbols like <|im_start|>\n                        full_request_text = encoding.decode(prompt_ids)\n                        \n                        snippet = self._get_debug_snippet(full_request_text)\n                        formatted_req = self._format_markdown_content(snippet)\n                        attempt_log.append(f\"### Turn {turn_i} - Raw Request to Model:\")\n                        attempt_log.append(formatted_req)\n\n                stream = self.client.completions.create(\n                    model=self.cfg.served_model_name, \n                    temperature=self.cfg.temperature, \n                    logprobs=self.cfg.top_logprobs, \n                    max_tokens=max_tokens, \n                    prompt=prompt_ids, \n                    seed=attempt_seed, \n                    stream=True, \n                    extra_body={\n                        'min_p': self.cfg.min_p, \n                        'stop_token_ids': self.stop_token_ids, \n                        'return_token_ids': True\n                    }\n                )\n\n                ## DEBUG\n                full_response_text = \"\"\n\n                try:\n                    token_buffer = []\n                    text_chunks = []\n    \n                    for chunk in stream:\n                        if stop_event.is_set() or time.time() > deadline:\n                            break\n    \n                        new_tokens = chunk.choices[0].token_ids\n                        new_text = chunk.choices[0].text\n    \n                        if new_tokens:\n                            token_buffer.extend(new_tokens)\n                            total_tokens += len(new_tokens)\n                            text_chunks.append(new_text)\n                            ## DEBUG\n                            if self.cfg.debug and self.cfg.debug_resp:\n                                full_response_text += new_text\n\n                            chunk_logprobs = chunk.choices[0].logprobs\n                            \n                            if chunk_logprobs is not None:\n                                if chunk_logprobs.top_logprobs:\n                                    logprobs_buffer.extend(chunk_logprobs.top_logprobs)\n    \n                        if '}' in new_text:\n                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n                            answer = self._scan_for_answer(search_text)\n    \n                            if answer is not None:\n                                final_answer = answer\n                                break\n    \n                finally:\n                    stream.close()\n\n                ## DEBUG\n                if self.cfg.debug and full_response_text:\n                    attempt_log.append(f\"### Turn {turn_i} - Model Response:\")\n                    formatted_resp = self._format_markdown_content(full_response_text)\n                    attempt_log.append(formatted_resp)\n\n                if final_answer is not None:\n                    break\n    \n                if not token_buffer:\n                    break\n    \n                new_messages = encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n                conversation.messages.extend(new_messages)\n                last_message = new_messages[-1]\n    \n                if last_message.channel == 'final':\n                    answer_text = last_message.content[0].text\n                    final_answer = self._scan_for_answer(answer_text)\n                    break\n    \n                if last_message.recipient == 'python':\n                    python_calls += 1\n                    tool_responses = local_tool.process_sync_plus(last_message)\n                    response_text = tool_responses[0].content[0].text\n\n                    ## DEBUG\n                    if self.cfg.debug:\n                        code_content = last_message.content[0].text\n                        attempt_log.append(f\"### Turn {turn_i} - Python Call:\")\n                        attempt_log.append(f\"```python\\n{code_content}\\n```\\n\")\n\n                        attempt_log.append(f\"### Turn {turn_i} - Python Output:\")\n                        snippet_out = self._get_debug_snippet(response_text)\n                        formatted_out = self._format_markdown_content(snippet_out, mode=\"text\")\n                        attempt_log.append(f\"{formatted_out}\\n\")\n\n                    if response_text.startswith('[ERROR]') or 'Traceback' in response_text or 'Error:' in response_text:\n                        python_errors += 1\n    \n                    conversation.messages.extend(tool_responses)\n    \n        except Exception as exc:\n            python_errors += 1\n            ## DEBUG\n            if self.cfg.debug:\n                attempt_log.append(f\"\\n**EXCEPTION:** {str(exc)}\\n\")\n\n        finally:\n            if sandbox is not None:\n                sandbox.reset()\n                self.sandbox_pool.put(sandbox)\n    \n        mean_entropy = self._compute_mean_entropy(logprobs_buffer)\n    \n        return {\n            'Attempt': attempt_index + 1, \n            'Response Length': total_tokens, \n            'Python Calls': python_calls, \n            'Python Errors': python_errors, \n            'Entropy': mean_entropy, \n            'Answer': final_answer,\n            'Log': \"\\n\".join(attempt_log)\n        }\n    \n    def _select_answer(self, detailed_results: list) -> int:\n        answer_weights = defaultdict(float)\n        answer_votes = defaultdict(int)\n\n        for result in detailed_results:\n            answer = result['Answer']\n            entropy = result['Entropy']\n            \n            if answer is not None:\n                weight = 1.0 / max(entropy, 1e-9)\n                \n                answer_weights[answer] += weight\n                answer_votes[answer] += 1\n\n        scored_answers = []\n\n        for answer, total_weight in answer_weights.items():\n            scored_answers.append({\n                'answer': answer, \n                'votes': answer_votes[answer], \n                'score': total_weight\n            })\n\n        scored_answers.sort(key=lambda x: x['score'], reverse=True)\n\n        vote_data = []\n\n        for item in scored_answers:\n            vote_data.append((\n                item['answer'], \n                item['votes'], \n                item['score']\n            ))\n\n        vote_dataframe = pd.DataFrame(\n            vote_data, \n            columns=['Answer', 'Votes', 'Score']\n        )\n\n        vote_dataframe = vote_dataframe.round({'Score': 3})\n        display(vote_dataframe)\n        \n        if not scored_answers:\n            print('\\nFinal Answer: 0\\n')\n            return vote_dataframe, 0\n\n        final_answer = scored_answers[0]['answer']    \n        print(f'\\nFinal Answer: {final_answer}\\n')\n        return vote_dataframe, final_answer\n    \n    ## DEUBG\n    def write_debug_logs(self, detailed_results: list, vote_dataframe: pd.DataFrame, problem: str, problem_id: str = \"UNK\"):\n        if not self.cfg.debug: return\n\n        try:\n            summary_lines = [\"\\n## Summary Stats\\n\"]\n            if detailed_results:\n                df = pd.DataFrame(detailed_results)\n                cols = [c for c in df.columns if c != 'Log']\n                summary_lines.append(df[cols].to_markdown(index=False))\n                summary_lines.append(\"\\n\\n\")\n                \n            if not vote_dataframe.empty:\n                summary_lines.append(\"## Vote Counts\\n\")\n                summary_lines.append(vote_dataframe.to_markdown(index=False))\n                summary_lines.append(\"\\n\")\n\n            # Header -> Summary ->  Attempt \n            final_log_content = [f\"# Problem ID: {problem_id}\\n\"]\n            final_log_content.append(f\"**System Prompt:**\\n> {self.cfg.system_prompt}\\n\\n\")\n            final_log_content.append(f\"**User Prompt:**\\n> {problem}\\n\")\n\n            final_log_content.extend(summary_lines)\n            final_log_content.append(\"\\n---\\n\")\n            \n            #  Attempt \n            #  detailed_results  Log  ()\n            #  _process_attempt  Log \n            sorted_results = sorted(detailed_results, key=lambda x: x['Attempt'])\n            for res in sorted_results:\n                log_content = res.get('Log', '')\n                if log_content:\n                    final_log_content.append(log_content)\n                    final_log_content.append(\"\\n---\\n\")\n\n            # \n            output_path = f\"{problem_id}.md\" \n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\".join(final_log_content))\n            print(f\"Debug log written to {output_path}\")\n\n        except Exception as e:\n            print(f\"Failed to write debug log: {e}\")\n\n    def solve_problem(self, problem: str, problem_id: str = \"UNK\") -> int:\n        print(f'\\nProblem: {problem}\\n')\n        \n        user_input = f'{problem} {self.cfg.preference_prompt}'\n    \n        elapsed_global = time.time() - self.notebook_start_time\n        time_left = self.cfg.notebook_limit - elapsed_global\n        problems_left_others = max(0, self.problems_remaining - 1)\n        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n    \n        budget = time_left - reserved_time\n        budget = min(budget, self.cfg.high_problem_timeout)\n        budget = max(budget, self.cfg.base_problem_timeout)\n    \n        deadline = time.time() + budget\n    \n        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n    \n        tasks = []\n    \n        for attempt_index in range(self.cfg.attempts):\n            tasks.append((self.cfg.system_prompt, attempt_index))\n    \n        detailed_results = []\n        valid_answers = []\n    \n        stop_event = threading.Event()\n    \n        executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n    \n        try:\n            futures = []\n    \n            for (system_prompt, attempt_index) in tasks:\n                future = executor.submit(\n                    self._process_attempt, \n                    user_input, \n                    system_prompt, \n                    attempt_index, \n                    stop_event, \n                    deadline\n                )\n    \n                futures.append(future)\n    \n            for future in as_completed(futures):\n                try:\n                    result = future.result()\n                    detailed_results.append(result)\n    \n                    if result['Answer'] is not None:\n                        valid_answers.append(result['Answer'])\n    \n                    counts = Counter(valid_answers).most_common(1)\n    \n                    if counts and counts[0][1] >= self.cfg.early_stop:\n                        stop_event.set()\n    \n                        for f in futures:\n                            f.cancel()\n    \n                        break\n    \n                except Exception as exc:\n                    print(f'Future failed: {exc}')\n                    continue\n    \n        finally:\n            stop_event.set()\n            executor.shutdown(wait=True, cancel_futures=True)\n            \n            self.problems_remaining = max(0, self.problems_remaining - 1)\n\n        if detailed_results:\n            results_dataframe = pd.DataFrame(detailed_results)\n            results_dataframe['Entropy'] = results_dataframe['Entropy'].round(3)\n            results_dataframe['Answer'] = results_dataframe['Answer'].astype('Int64')\n            \n            cols = [c for c in results_dataframe.columns if c != 'Log']\n            display(results_dataframe[cols])\n\n        if not valid_answers:\n            print('\\nResult: 0\\n')\n            vote_data, final_answer = pd.DataFrame(columns=['Answer', 'Votes', 'Score']), 0\n        else:\n            vote_data, final_answer = self._select_answer(detailed_results)\n        \n        self.write_debug_logs(detailed_results, vote_data, problem, problem_id=problem_id)\n        return final_answer\n\n    def __del__(self):\n        if hasattr(self, 'server_process'):\n            self.server_process.terminate()\n            self.server_process.wait()\n    \n        if hasattr(self, 'log_file'):\n            self.log_file.close()\n    \n        if hasattr(self, 'sandbox_pool'):\n            while not self.sandbox_pool.empty():\n                try:\n                    sb = self.sandbox_pool.get_nowait()\n                    sb.close()\n    \n                except Exception:\n                    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T07:17:11.446765Z","iopub.execute_input":"2026-01-14T07:17:11.446909Z","iopub.status.idle":"2026-01-14T07:17:11.479386Z","shell.execute_reply.started":"2026-01-14T07:17:11.446896Z","shell.execute_reply":"2026-01-14T07:17:11.479030Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"solver = AIMO3Solver(CFG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T07:17:11.479911Z","iopub.execute_input":"2026-01-14T07:17:11.480036Z","iopub.status.idle":"2026-01-14T07:20:25.344342Z","shell.execute_reply.started":"2026-01-14T07:17:11.480024Z","shell.execute_reply":"2026-01-14T07:20:25.343906Z"}},"outputs":[{"name":"stdout","text":"Loading model weights from /kaggle/input/gpt-oss-120b/transformers/default/1 into OS Page Cache...\nProcessed 26 files (65.28 GB) in 57.36 seconds.\n\nWaiting for vLLM server...\nServer is ready (took 133.19 seconds).\n\nInitializing 16 persistent Jupyter kernels...\nKernels initialized in 2.94 seconds.\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n    id_value = id_.item(0)\n    question_text = question.item(0)\n    gc.disable()\n    final_answer = solver.solve_problem(question_text, problem_id=str(id_value))\n    gc.enable()\n    gc.collect()\n    return pl.DataFrame({'id': id_value, 'answer': final_answer})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T07:20:25.344930Z","iopub.execute_input":"2026-01-14T07:20:25.345076Z","iopub.status.idle":"2026-01-14T07:20:25.348337Z","shell.execute_reply.started":"2026-01-14T07:20:25.345062Z","shell.execute_reply":"2026-01-14T07:20:25.347951Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    CFG.debug = False\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        # ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n        # ('/kaggle/input/aimo-p3-hard/test2.csv',)\n        ('/kaggle/input/aimo-p3-hard/p10.csv',)\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T07:20:25.349364Z","iopub.execute_input":"2026-01-14T07:20:25.349536Z","iopub.status.idle":"2026-01-14T07:30:48.000218Z","shell.execute_reply.started":"2026-01-14T07:20:25.349522Z","shell.execute_reply":"2026-01-14T07:30:47.999584Z"}},"outputs":[{"name":"stdout","text":"\nProblem: Let $n \\geq 6$ be a positive integer. We call a positive integer $n$-Norwegian if it has three distinct positive divisors whose sum is equal to $n$. Let $f(n)$ denote the smallest $n$-Norwegian positive integer. Let $M=3^{2025!}$ and for a non-negative integer $c$ define \n\\begin{equation*}\n    g(c)=\\frac{1}{2025!}\\left\\lfloor \\frac{2025! f(M+c)}{M}\\right\\rfloor.\n\\end{equation*}\nWe can write \n\\begin{equation*}\n    g(0)+g(4M)+g(1848374)+g(10162574)+g(265710644)+g(44636594)=\\frac{p}{q}\n\\end{equation*}\nwhere $p$ and $q$ are coprime positive integers. What is the remainder when $p+q$ is divided by $99991$?\n\nBudget: 900.00 seconds | Deadline: 1768376125.78\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer\n0        7            23206            14              1    0.679   27243\n1        8            30986            53              3    0.720    8687\n2        1            31497            36              3    0.766    8687\n3        2            37775            69              8    0.689   41754\n4        3            40564            32              1    0.680   57280\n5        4            40350            72             22    0.680    8687\n6        5            50236            41              7    0.704      23\n7        6            59716            39              4    0.736    <NA>","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Attempt</th>\n      <th>Response Length</th>\n      <th>Python Calls</th>\n      <th>Python Errors</th>\n      <th>Entropy</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>23206</td>\n      <td>14</td>\n      <td>1</td>\n      <td>0.679</td>\n      <td>27243</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>30986</td>\n      <td>53</td>\n      <td>3</td>\n      <td>0.720</td>\n      <td>8687</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>31497</td>\n      <td>36</td>\n      <td>3</td>\n      <td>0.766</td>\n      <td>8687</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>37775</td>\n      <td>69</td>\n      <td>8</td>\n      <td>0.689</td>\n      <td>41754</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>40564</td>\n      <td>32</td>\n      <td>1</td>\n      <td>0.680</td>\n      <td>57280</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>40350</td>\n      <td>72</td>\n      <td>22</td>\n      <td>0.680</td>\n      <td>8687</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>50236</td>\n      <td>41</td>\n      <td>7</td>\n      <td>0.704</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n      <td>59716</td>\n      <td>39</td>\n      <td>4</td>\n      <td>0.736</td>\n      <td>&lt;NA&gt;</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   Answer  Votes  Score\n0    8687      3  4.166\n1   27243      1  1.473\n2   57280      1  1.470\n3   41754      1  1.451\n4      23      1  1.421","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Answer</th>\n      <th>Votes</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8687</td>\n      <td>3</td>\n      <td>4.166</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27243</td>\n      <td>1</td>\n      <td>1.473</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57280</td>\n      <td>1</td>\n      <td>1.470</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>41754</td>\n      <td>1</td>\n      <td>1.451</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23</td>\n      <td>1</td>\n      <td>1.421</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nFinal Answer: 8687\n\nDebug log written to P10.md\n","output_type":"stream"}],"execution_count":10}]}