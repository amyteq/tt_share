{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-13T12:57:56.514023Z",
     "iopub.status.busy": "2026-01-13T12:57:56.513817Z",
     "iopub.status.idle": "2026-01-13T12:59:04.272135Z",
     "shell.execute_reply": "2026-01-13T12:59:04.271595Z",
     "shell.execute_reply.started": "2026-01-13T12:57:56.514008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T12:59:04.273222Z",
     "iopub.status.busy": "2026-01-13T12:59:04.273079Z",
     "iopub.status.idle": "2026-01-13T13:02:46.725509Z",
     "shell.execute_reply": "2026-01-13T13:02:46.725063Z",
     "shell.execute_reply.started": "2026-01-13T12:59:04.273206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def set_env(input_archive, temp_dir):\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n",
    "    \n",
    "    subprocess.run([\n",
    "        sys.executable, \n",
    "        '-m', \n",
    "        'pip', \n",
    "        'install', \n",
    "        '--no-index', \n",
    "        '--find-links', \n",
    "        f'{temp_dir}/wheels', \n",
    "        'unsloth', \n",
    "        'trl', \n",
    "        'vllm', \n",
    "        'openai_harmony'\n",
    "    ], \n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    "    check=True)\n",
    "\n",
    "set_env(\n",
    "    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz', \n",
    "    temp_dir='/kaggle/tmp/setup'\n",
    ")\n",
    "\n",
    "subprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:30:13.643307Z",
     "iopub.status.busy": "2026-01-13T13:30:13.642809Z",
     "iopub.status.idle": "2026-01-13T13:30:13.655093Z",
     "shell.execute_reply": "2026-01-13T13:30:13.654648Z",
     "shell.execute_reply.started": "2026-01-13T13:30:13.643290Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
    "os.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import contextlib\n",
    "from typing import Optional\n",
    "from jupyter_client import KernelManager\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from openai_harmony import (\n",
    "    HarmonyEncodingName, \n",
    "    load_harmony_encoding, \n",
    "    SystemContent, \n",
    "    ReasoningEffort, \n",
    "    ToolNamespaceConfig, \n",
    "    Author, \n",
    "    Message, \n",
    "    Role, \n",
    "    TextContent, \n",
    "    Conversation\n",
    ")\n",
    "\n",
    "from transformers import set_seed\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "\n",
    "class CFG:\n",
    "\n",
    "    system_prompt = (\n",
    "        'You are a world-class International Mathematical Olympiad (IMO) competitor. '\n",
    "        'The final answer must be a non-negative integer between 0 and 99999. '\n",
    "        'You must place the final integer answer inside \\\\boxed{}.'\n",
    "    )\n",
    "    \n",
    "    tool_prompt = (\n",
    "        'Use this tool to execute Python code. '\n",
    "        'The environment is a stateful Jupyter notebook. '\n",
    "        'You must use print() to output results.'\n",
    "    )\n",
    "    \n",
    "    preference_prompt = (\n",
    "        'You have access to `math`, `numpy` and `sympy` to solve the problem.'\n",
    "    )\n",
    "\n",
    "    served_model_name = 'gpt-oss'\n",
    "    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
    "    \n",
    "    kv_cache_dtype = 'fp8_e4m3'\n",
    "    dtype = 'auto'\n",
    "\n",
    "    high_problem_timeout = 900\n",
    "    base_problem_timeout = 300\n",
    "\n",
    "    notebook_limit = 17400\n",
    "    server_timeout = 180\n",
    "\n",
    "    session_timeout = 960\n",
    "    jupyter_timeout = 6\n",
    "    sandbox_timeout = 3\n",
    "\n",
    "    stream_interval = 200\n",
    "    context_tokens = 65536\n",
    "    buffer_tokens = 512\n",
    "    search_tokens = 32\n",
    "    top_logprobs = 5\n",
    "    batch_size = 256\n",
    "    early_stop = 4\n",
    "    attempts = 8\n",
    "    workers = 16\n",
    "    turns = 128\n",
    "    seed = 42\n",
    "\n",
    "    gpu_memory_utilization = 0.96\n",
    "    temperature = 1.0\n",
    "    min_p = 0.02\n",
    "    debug = True\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:30:13.656026Z",
     "iopub.status.busy": "2026-01-13T13:30:13.655874Z",
     "iopub.status.idle": "2026-01-13T13:30:13.661382Z",
     "shell.execute_reply": "2026-01-13T13:30:13.660963Z",
     "shell.execute_reply.started": "2026-01-13T13:30:13.656012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AIMO3Template:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n",
    "        return (\n",
    "            SystemContent.new()\n",
    "            .with_model_identity(system_prompt)\n",
    "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
    "            .with_tools(tool_config)\n",
    "        )\n",
    "\n",
    "    def apply_chat_template(\n",
    "        self, \n",
    "        system_prompt: str, \n",
    "        user_prompt: str, \n",
    "        tool_config: ToolNamespaceConfig\n",
    "    ) -> list[Message]:\n",
    "        system_content = self.get_system_content(system_prompt, tool_config)        \n",
    "        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
    "        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n",
    "        return [system_message, user_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:30:13.662110Z",
     "iopub.status.busy": "2026-01-13T13:30:13.661951Z",
     "iopub.status.idle": "2026-01-13T13:30:13.674440Z",
     "shell.execute_reply": "2026-01-13T13:30:13.674072Z",
     "shell.execute_reply.started": "2026-01-13T13:30:13.662096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AIMO3Sandbox:\n",
    "\n",
    "    _port_lock = threading.Lock()\n",
    "    _next_port = 50000\n",
    "\n",
    "    @classmethod\n",
    "    def _get_next_ports(cls, count: int = 5) -> list[int]:\n",
    "        with cls._port_lock:\n",
    "            ports = list(range(cls._next_port, cls._next_port + count))\n",
    "            cls._next_port += count\n",
    "            return ports\n",
    "\n",
    "    def __init__(self, timeout: float):\n",
    "        self._default_timeout = timeout\n",
    "        self._owns_kernel = False\n",
    "        self._client = None\n",
    "        self._km = None\n",
    "        \n",
    "        ports = self._get_next_ports(5)\n",
    "\n",
    "        env = os.environ.copy()\n",
    "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n",
    "        env['JUPYTER_PLATFORM_DIRS'] = '1'\n",
    "        env['PYTHONWARNINGS'] = 'ignore'\n",
    "        env['MPLBACKEND'] = 'Agg'\n",
    "\n",
    "        self._km = KernelManager()\n",
    "        self._km.shell_port = ports[0]\n",
    "        self._km.iopub_port = ports[1]\n",
    "        self._km.stdin_port = ports[2]\n",
    "        self._km.hb_port = ports[3]\n",
    "        self._km.control_port = ports[4]\n",
    "        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n",
    "\n",
    "        self._client = self._km.blocking_client()\n",
    "        self._client.start_channels()\n",
    "        self._client.wait_for_ready(timeout=self._default_timeout)\n",
    "        self._owns_kernel = True\n",
    "\n",
    "        self.execute(\n",
    "            'import math\\n'\n",
    "            'import numpy\\n'\n",
    "            'import sympy\\n'\n",
    "            'import itertools\\n'\n",
    "            'import collections\\n'\n",
    "            'import mpmath\\n'\n",
    "            'mpmath.mp.dps = 64\\n'\n",
    "        )\n",
    "\n",
    "    def _format_error(self, traceback: list[str]) -> str:\n",
    "        clean_lines = []\n",
    "        for frame in traceback:\n",
    "            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n",
    "            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n",
    "                continue\n",
    "            clean_lines.append(clean_frame)\n",
    "        return ''.join(clean_lines)\n",
    "\n",
    "    def execute(self, code: str, timeout: float | None = None) -> str:\n",
    "        client = self._client\n",
    "        effective_timeout = timeout or self._default_timeout\n",
    "        \n",
    "        msg_id = client.execute(\n",
    "            code, \n",
    "            store_history=True, \n",
    "            allow_stdin=False, \n",
    "            stop_on_error=False\n",
    "        )\n",
    "\n",
    "        stdout_parts = []\n",
    "        stderr_parts = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            if elapsed > effective_timeout:\n",
    "                self._km.interrupt_kernel()\n",
    "\n",
    "                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n",
    "\n",
    "            try:\n",
    "                msg = client.get_iopub_msg(timeout=1.0)\n",
    "\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "\n",
    "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
    "                continue\n",
    "\n",
    "            msg_type = msg.get('msg_type')\n",
    "            content = msg.get('content', {})\n",
    "\n",
    "            if msg_type == 'stream':\n",
    "                text = content.get('text', '')\n",
    "\n",
    "                if content.get('name') == 'stdout':\n",
    "                    stdout_parts.append(text)\n",
    "\n",
    "                else:\n",
    "                    stderr_parts.append(text)\n",
    "\n",
    "            elif msg_type == 'error':\n",
    "                traceback_list = content.get('traceback', [])\n",
    "\n",
    "                stderr_parts.append(self._format_error(traceback_list))\n",
    "\n",
    "            elif msg_type in {'execute_result', 'display_data'}:\n",
    "                data = content.get('data', {})\n",
    "                text = data.get('text/plain')\n",
    "\n",
    "                if text:\n",
    "                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n",
    "\n",
    "            elif msg_type == 'status':\n",
    "                if content.get('execution_state') == 'idle':\n",
    "                    break\n",
    "\n",
    "        stdout = ''.join(stdout_parts)\n",
    "        stderr = ''.join(stderr_parts)\n",
    "\n",
    "        if stderr:\n",
    "            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n",
    "\n",
    "        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n",
    "\n",
    "    def close(self):\n",
    "        with contextlib.suppress(Exception):\n",
    "            if self._client:\n",
    "                self._client.stop_channels()\n",
    "\n",
    "        if self._owns_kernel and self._km is not None:\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.shutdown_kernel(now=True)\n",
    "\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.cleanup_resources()\n",
    "\n",
    "    def reset(self):\n",
    "        self.execute(\n",
    "            '%reset -f\\n'\n",
    "            'import math\\n'\n",
    "            'import numpy\\n'\n",
    "            'import sympy\\n'\n",
    "            'import itertools\\n'\n",
    "            'import collections\\n'\n",
    "            'import mpmath\\n'\n",
    "            'mpmath.mp.dps = 64\\n'\n",
    "        )\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:30:13.826537Z",
     "iopub.status.busy": "2026-01-13T13:30:13.826352Z",
     "iopub.status.idle": "2026-01-13T13:30:13.832676Z",
     "shell.execute_reply": "2026-01-13T13:30:13.832076Z",
     "shell.execute_reply.started": "2026-01-13T13:30:13.826524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AIMO3Tool:\n",
    "\n",
    "    def __init__(self, local_jupyter_timeout: float, tool_prompt: str, sandbox=None):\n",
    "        self._local_jupyter_timeout = local_jupyter_timeout\n",
    "        self._tool_prompt = tool_prompt\n",
    "        self._jupyter_session = sandbox\n",
    "        \n",
    "        self._owns_session = sandbox is None\n",
    "        \n",
    "        self._execution_lock = threading.Lock()\n",
    "        self._init_lock = threading.Lock()\n",
    "\n",
    "    def _ensure_session(self):\n",
    "\n",
    "        if self._jupyter_session is None:\n",
    "            with self._init_lock:\n",
    "                if self._jupyter_session is None:\n",
    "                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n",
    "\n",
    "    def _ensure_last_print(self, code: str) -> str:\n",
    "\n",
    "        lines = code.strip().split('\\n')\n",
    "\n",
    "        if not lines:\n",
    "            return code\n",
    "\n",
    "        last_line = lines[-1].strip()\n",
    "\n",
    "        if 'print' in last_line or 'import' in last_line:\n",
    "            return code\n",
    "\n",
    "        if not last_line:\n",
    "            return code\n",
    "\n",
    "        if last_line.startswith('#'):\n",
    "            return code\n",
    "\n",
    "        lines[-1] = 'print(' + last_line + ')'\n",
    "\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    @property\n",
    "    def instruction(self) -> str:\n",
    "        return self._tool_prompt\n",
    "\n",
    "    @property\n",
    "    def tool_config(self) -> ToolNamespaceConfig:\n",
    "        return ToolNamespaceConfig(\n",
    "            name='python', \n",
    "            description=self.instruction, \n",
    "            tools=[]\n",
    "        )\n",
    "\n",
    "    def _make_response(self, output: str, channel: str | None = None) -> Message:\n",
    "        content = TextContent(text=output)\n",
    "        author = Author(role=Role.TOOL, name='python')\n",
    "        message = Message(author=author, content=[content]).with_recipient('assistant')\n",
    "\n",
    "        if channel:\n",
    "            message = message.with_channel(channel)\n",
    "\n",
    "        return message\n",
    "\n",
    "    def process_sync_plus(self, message: Message) -> list[Message]:\n",
    "        self._ensure_session()\n",
    "        raw_script = message.content[0].text\n",
    "        final_script = self._ensure_last_print(raw_script)\n",
    "\n",
    "        with self._execution_lock:\n",
    "            try:\n",
    "                output = self._jupyter_session.execute(final_script)\n",
    "\n",
    "            except TimeoutError as exc:\n",
    "                output = f'[ERROR] {exc}'\n",
    "\n",
    "        return [self._make_response(output, channel=message.channel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:30:13.835266Z",
     "iopub.status.busy": "2026-01-13T13:30:13.835108Z",
     "iopub.status.idle": "2026-01-13T13:30:13.868693Z",
     "shell.execute_reply": "2026-01-13T13:30:13.868315Z",
     "shell.execute_reply.started": "2026-01-13T13:30:13.835253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AIMO3Solver:\n",
    "\n",
    "    def __init__(self, cfg, port: int = 8000):\n",
    "        self.cfg = cfg\n",
    "        self.port = port\n",
    "        self.base_url = f'http://0.0.0.0:{port}/v1'\n",
    "        self.api_key = 'sk-local'\n",
    "        self.template = AIMO3Template()\n",
    "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
    "    \n",
    "        self._preload_model_weights()\n",
    "        \n",
    "        self.server_process = self._start_server()\n",
    "    \n",
    "        self.client = OpenAI(\n",
    "            base_url=self.base_url, \n",
    "            api_key=self.api_key, \n",
    "            timeout=self.cfg.session_timeout\n",
    "        )\n",
    "    \n",
    "        self._wait_for_server()\n",
    "        self._initialize_kernels()\n",
    "    \n",
    "        self.notebook_start_time = time.time()\n",
    "        self.problems_remaining = 50\n",
    "    \n",
    "    def _preload_model_weights(self) -> None:\n",
    "        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        files_to_load = []\n",
    "        total_size = 0\n",
    "    \n",
    "        for root, _, files in os.walk(self.cfg.model_path):\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "    \n",
    "                if os.path.isfile(file_path):\n",
    "                    files_to_load.append(file_path)\n",
    "                    total_size += os.path.getsize(file_path)\n",
    "    \n",
    "        def _read_file(path: str) -> None:\n",
    "    \n",
    "            with open(path, 'rb') as file_object:\n",
    "                while file_object.read(1024 * 1024 * 1024):\n",
    "                    pass\n",
    "    \n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
    "            list(executor.map(_read_file, files_to_load))\n",
    "    \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n",
    "    \n",
    "    def _start_server(self) -> subprocess.Popen:\n",
    "        cmd = [\n",
    "            sys.executable, \n",
    "            '-m', \n",
    "            'vllm.entrypoints.openai.api_server', \n",
    "            '--seed', \n",
    "            str(self.cfg.seed), \n",
    "            '--model', \n",
    "            self.cfg.model_path, \n",
    "            '--served-model-name', \n",
    "            self.cfg.served_model_name, \n",
    "            '--tensor-parallel-size', \n",
    "            '1', \n",
    "            '--max-num-seqs', \n",
    "            str(self.cfg.batch_size), \n",
    "            '--gpu-memory-utilization', \n",
    "            str(self.cfg.gpu_memory_utilization), \n",
    "            '--host', \n",
    "            '0.0.0.0', \n",
    "            '--port', \n",
    "            str(self.port), \n",
    "            '--dtype', \n",
    "            self.cfg.dtype, \n",
    "            '--kv-cache-dtype', \n",
    "            self.cfg.kv_cache_dtype, \n",
    "            '--max-model-len', \n",
    "            str(self.cfg.context_tokens), \n",
    "            '--stream-interval', \n",
    "            str(self.cfg.stream_interval), \n",
    "            '--async-scheduling', \n",
    "            '--disable-log-stats', \n",
    "            '--enable-prefix-caching'\n",
    "        ]\n",
    "    \n",
    "        self.log_file = open('vllm_server.log', 'w')\n",
    "    \n",
    "        return subprocess.Popen(\n",
    "            cmd, \n",
    "            stdout=self.log_file, \n",
    "            stderr=subprocess.STDOUT, \n",
    "            start_new_session=True\n",
    "        )\n",
    "    \n",
    "    def _wait_for_server(self):\n",
    "        print('Waiting for vLLM server...')\n",
    "        start_time = time.time()\n",
    "    \n",
    "        for _ in range(self.cfg.server_timeout):\n",
    "            return_code = self.server_process.poll()\n",
    "    \n",
    "            if return_code is not None:\n",
    "                self.log_file.flush()\n",
    "    \n",
    "                with open('vllm_server.log', 'r') as log_file:\n",
    "                    logs = log_file.read()\n",
    "    \n",
    "                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n",
    "    \n",
    "            try:\n",
    "                self.client.models.list()\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n",
    "    \n",
    "                return\n",
    "    \n",
    "            except Exception:\n",
    "                time.sleep(1)\n",
    "    \n",
    "        raise RuntimeError('Server failed to start (timeout).\\n')\n",
    "    \n",
    "    def _initialize_kernels(self) -> None:\n",
    "        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n",
    "        start_time = time.time()\n",
    "    \n",
    "        self.sandbox_pool = queue.Queue()\n",
    "    \n",
    "        def _create_sandbox():\n",
    "            \n",
    "            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n",
    "    \n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
    "            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n",
    "    \n",
    "            for future in as_completed(futures):\n",
    "                self.sandbox_pool.put(future.result())\n",
    "    \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n",
    "    \n",
    "    def _scan_for_answer(self, text: str) -> int | None:\n",
    "        pattern = r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}'\n",
    "        matches = re.findall(pattern, text)\n",
    "    \n",
    "        if matches:\n",
    "            try:\n",
    "                clean_value = matches[-1].replace(',', '')\n",
    "                value = int(clean_value)\n",
    "    \n",
    "                if 0 <= value <= 99999:\n",
    "                    return value\n",
    "    \n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "        pattern = r'final\\s+answer\\s+is\\s*([0-9,]+)'\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    \n",
    "        if matches:\n",
    "            try:\n",
    "                clean_value = matches[-1].replace(',', '')\n",
    "                value = int(clean_value)\n",
    "    \n",
    "                if 0 <= value <= 99999:\n",
    "                    return value\n",
    "    \n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    def _compute_mean_entropy(self, logprobs_buffer: list) -> float:\n",
    "        if not logprobs_buffer:\n",
    "            return float('inf')\n",
    "    \n",
    "        total_entropy = 0.0\n",
    "        token_count = 0\n",
    "    \n",
    "        for top_logprobs_dict in logprobs_buffer:\n",
    "            \n",
    "            if not isinstance(top_logprobs_dict, dict):\n",
    "                continue\n",
    "            \n",
    "            if not top_logprobs_dict:\n",
    "                continue\n",
    "            \n",
    "            token_entropy = 0.0\n",
    "            \n",
    "            for token_str, log_prob in top_logprobs_dict.items():\n",
    "                prob = math.exp(log_prob)\n",
    "                \n",
    "                if prob > 0:\n",
    "                    token_entropy -= prob * math.log2(prob)\n",
    "            \n",
    "            total_entropy += token_entropy\n",
    "            token_count += 1\n",
    "    \n",
    "        if token_count == 0:\n",
    "            return float('inf')\n",
    "    \n",
    "        return total_entropy / token_count\n",
    "    \n",
    "    def _process_attempt(\n",
    "        self, \n",
    "        problem: str, \n",
    "        system_prompt: str, \n",
    "        attempt_index: int, \n",
    "        stop_event: threading.Event, \n",
    "        deadline: float\n",
    "    ) -> dict:\n",
    "        ## DEBUG\n",
    "        attempt_log = []\n",
    "        if self.cfg.debug:\n",
    "            attempt_log.append(f\"## Attempt {attempt_index + 1}\\n\")\n",
    "            attempt_log.append(f\"**System Prompt:**\\n> {system_prompt}\\n\")\n",
    "            attempt_log.append(f\"**User Prompt:**\\n> {problem}\\n\")\n",
    "\n",
    "        if stop_event.is_set() or time.time() > deadline:\n",
    "            return {\n",
    "                'Attempt': attempt_index + 1, \n",
    "                'Answer': None, \n",
    "                'Python Calls': 0, \n",
    "                'Python Errors': 0, \n",
    "                'Response Length': 0, \n",
    "                'Entropy': float('inf'),\n",
    "                'Log': \"\\n\".join(attempt_log)\n",
    "            }\n",
    "    \n",
    "        local_tool = None\n",
    "        sandbox = None\n",
    "        python_calls = 0\n",
    "        python_errors = 0\n",
    "        total_tokens = 0\n",
    "        final_answer = None\n",
    "        \n",
    "        logprobs_buffer = []\n",
    "    \n",
    "        attempt_seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n",
    "    \n",
    "        try:\n",
    "            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n",
    "    \n",
    "            local_tool = AIMO3Tool(\n",
    "                local_jupyter_timeout=self.cfg.jupyter_timeout, \n",
    "                tool_prompt=self.cfg.tool_prompt, \n",
    "                sandbox=sandbox\n",
    "            )\n",
    "    \n",
    "            encoding = self.encoding\n",
    "            messages = self.template.apply_chat_template(\n",
    "                system_prompt, \n",
    "                problem, \n",
    "                local_tool.tool_config\n",
    "            )\n",
    "    \n",
    "            conversation = Conversation.from_messages(messages)\n",
    "    \n",
    "            for turn_i in range(self.cfg.turns):\n",
    "                if stop_event.is_set() or time.time() > deadline:\n",
    "                    break\n",
    "    \n",
    "                prompt_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
    "                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n",
    "    \n",
    "                if max_tokens < self.cfg.buffer_tokens:\n",
    "                    break\n",
    "    \n",
    "                stream = self.client.completions.create(\n",
    "                    model=self.cfg.served_model_name, \n",
    "                    temperature=self.cfg.temperature, \n",
    "                    logprobs=self.cfg.top_logprobs, \n",
    "                    max_tokens=max_tokens, \n",
    "                    prompt=prompt_ids, \n",
    "                    seed=attempt_seed, \n",
    "                    stream=True, \n",
    "                    extra_body={\n",
    "                        'min_p': self.cfg.min_p, \n",
    "                        'stop_token_ids': self.stop_token_ids, \n",
    "                        'return_token_ids': True\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                ## DEBUG\n",
    "                full_response_text = \"\"\n",
    "\n",
    "                try:\n",
    "                    token_buffer = []\n",
    "                    text_chunks = []\n",
    "    \n",
    "                    for chunk in stream:\n",
    "                        if stop_event.is_set() or time.time() > deadline:\n",
    "                            break\n",
    "    \n",
    "                        new_tokens = chunk.choices[0].token_ids\n",
    "                        new_text = chunk.choices[0].text\n",
    "    \n",
    "                        if new_tokens:\n",
    "                            token_buffer.extend(new_tokens)\n",
    "                            total_tokens += len(new_tokens)\n",
    "                            text_chunks.append(new_text)\n",
    "                            ## DEBUG\n",
    "                            full_response_text += new_text\n",
    "\n",
    "                            chunk_logprobs = chunk.choices[0].logprobs\n",
    "                            \n",
    "                            if chunk_logprobs is not None:\n",
    "                                if chunk_logprobs.top_logprobs:\n",
    "                                    logprobs_buffer.extend(chunk_logprobs.top_logprobs)\n",
    "    \n",
    "                        if '}' in new_text:\n",
    "                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n",
    "                            answer = self._scan_for_answer(search_text)\n",
    "    \n",
    "                            if answer is not None:\n",
    "                                final_answer = answer\n",
    "                                break\n",
    "    \n",
    "                finally:\n",
    "                    stream.close()\n",
    "\n",
    "                ## DEBUG\n",
    "                if self.cfg.debug and full_response_text:\n",
    "                    attempt_log.append(f\"### Turn {turn_i} - Model Response:\")\n",
    "                    attempt_log.append(f\"{full_response_text}\\n\")\n",
    "\n",
    "                if final_answer is not None:\n",
    "                    break\n",
    "    \n",
    "                if not token_buffer:\n",
    "                    break\n",
    "    \n",
    "                new_messages = encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n",
    "                conversation.messages.extend(new_messages)\n",
    "                last_message = new_messages[-1]\n",
    "    \n",
    "                if last_message.channel == 'final':\n",
    "                    answer_text = last_message.content[0].text\n",
    "                    final_answer = self._scan_for_answer(answer_text)\n",
    "                    break\n",
    "    \n",
    "                if last_message.recipient == 'python':\n",
    "                    python_calls += 1\n",
    "                    \n",
    "                    ## DEBUG\n",
    "                    if self.cfg.debug:\n",
    "                        code_content = last_message.content[0].text\n",
    "                        attempt_log.append(\"### Turn {} - Python Call:\".format(turn_i))\n",
    "                        attempt_log.append(f\"```python\\n{code_content}\\n```\\n\")\n",
    "                    \n",
    "                    tool_responses = local_tool.process_sync_plus(last_message)\n",
    "    \n",
    "                    response_text = tool_responses[0].content[0].text\n",
    "\n",
    "                    ## DEBUG\n",
    "                    if self.cfg.debug:\n",
    "                        attempt_log.append(\"### Turn {} - Python Output:\".format(turn_i))\n",
    "                        # 限制输出长度，防止日志过大\n",
    "                        log_output = response_text[:2000] + '...' if len(response_text) > 2000 else response_text\n",
    "                        attempt_log.append(f\"```text\\n{log_output}\\n```\\n\")\n",
    "\n",
    "                    if response_text.startswith('[ERROR]') or 'Traceback' in response_text or 'Error:' in response_text:\n",
    "                        python_errors += 1\n",
    "    \n",
    "                    conversation.messages.extend(tool_responses)\n",
    "    \n",
    "        except Exception as exc:\n",
    "            python_errors += 1\n",
    "            ## DEBUG\n",
    "            if self.cfg.debug:\n",
    "                attempt_log.append(f\"\\n**EXCEPTION:** {str(exc)}\\n\")\n",
    "\n",
    "        finally:\n",
    "            if sandbox is not None:\n",
    "                sandbox.reset()\n",
    "                self.sandbox_pool.put(sandbox)\n",
    "    \n",
    "        mean_entropy = self._compute_mean_entropy(logprobs_buffer)\n",
    "    \n",
    "        return {\n",
    "            'Attempt': attempt_index + 1, \n",
    "            'Response Length': total_tokens, \n",
    "            'Python Calls': python_calls, \n",
    "            'Python Errors': python_errors, \n",
    "            'Entropy': mean_entropy, \n",
    "            'Answer': final_answer,\n",
    "            'Log': \"\\n\".join(attempt_log)\n",
    "        }\n",
    "    \n",
    "    def _select_answer(self, detailed_results: list) -> int:\n",
    "        answer_weights = defaultdict(float)\n",
    "        answer_votes = defaultdict(int)\n",
    "\n",
    "        for result in detailed_results:\n",
    "            answer = result['Answer']\n",
    "            entropy = result['Entropy']\n",
    "            \n",
    "            if answer is not None:\n",
    "                weight = 1.0 / max(entropy, 1e-9)\n",
    "                \n",
    "                answer_weights[answer] += weight\n",
    "                answer_votes[answer] += 1\n",
    "\n",
    "        scored_answers = []\n",
    "\n",
    "        for answer, total_weight in answer_weights.items():\n",
    "            scored_answers.append({\n",
    "                'answer': answer, \n",
    "                'votes': answer_votes[answer], \n",
    "                'score': total_weight\n",
    "            })\n",
    "\n",
    "        scored_answers.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        vote_data = []\n",
    "\n",
    "        for item in scored_answers:\n",
    "            vote_data.append((\n",
    "                item['answer'], \n",
    "                item['votes'], \n",
    "                item['score']\n",
    "            ))\n",
    "\n",
    "        vote_dataframe = pd.DataFrame(\n",
    "            vote_data, \n",
    "            columns=['Answer', 'Votes', 'Score']\n",
    "        )\n",
    "\n",
    "        vote_dataframe = vote_dataframe.round({'Score': 3})\n",
    "        display(vote_dataframe)\n",
    "        \n",
    "        if not scored_answers:\n",
    "            print('\\nFinal Answer: 0\\n')\n",
    "            return vote_dataframe, 0\n",
    "\n",
    "        final_answer = scored_answers[0]['answer']    \n",
    "        print(f'\\nFinal Answer: {final_answer}\\n')\n",
    "        return vote_dataframe, final_answer\n",
    "    \n",
    "    ## DEUBG\n",
    "    def write_debug_logs(self, detailed_results: list, vote_dataframe: pd.DataFrame, problem_id: str = \"UNK\"):\n",
    "        if not self.cfg.debug: return\n",
    "\n",
    "        try:\n",
    "            summary_lines = [\"\\n# Summary Stats\\n\"]\n",
    "            if detailed_results:\n",
    "                df = pd.DataFrame(detailed_results)\n",
    "                cols = [c for c in df.columns if c != 'Log']\n",
    "                summary_lines.append(df[cols].to_markdown(index=False))\n",
    "                summary_lines.append(\"\\n\\n\")\n",
    "                \n",
    "            if not vote_dataframe.empty:\n",
    "                summary_lines.append(\"## Vote Counts\\n\")\n",
    "                summary_lines.append(vote_dataframe.to_markdown(index=False))\n",
    "                summary_lines.append(\"\\n\")\n",
    "\n",
    "            # 拼接所有内容：Header -> Summary -> 各个 Attempt 的详细日志\n",
    "            final_log_content = [f\"# Problem ID: {problem_id}\\n\"]\n",
    "            final_log_content.extend(summary_lines)\n",
    "            final_log_content.append(\"\\n---\\n\")\n",
    "            \n",
    "            # 按 Attempt 顺序排序日志\n",
    "            # 注意：如果 detailed_results 里没有 Log 字段 (比如被删了)，这里要防守一下\n",
    "            # 但我们在 _process_attempt 里是保证返回 Log 的\n",
    "            sorted_results = sorted(detailed_results, key=lambda x: x['Attempt'])\n",
    "            for res in sorted_results:\n",
    "                log_content = res.get('Log', '')\n",
    "                if log_content:\n",
    "                    final_log_content.append(log_content)\n",
    "                    final_log_content.append(\"\\n---\\n\")\n",
    "\n",
    "            # 写入文件\n",
    "            output_path = f\"{problem_id}.md\" \n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\".join(final_log_content))\n",
    "            print(f\"Debug log written to {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to write debug log: {e}\")\n",
    "\n",
    "    def solve_problem(self, problem: str, problem_id: str = \"UNK\") -> int:\n",
    "        print(f'\\nProblem: {problem}\\n')\n",
    "        \n",
    "        user_input = f'{problem} {self.cfg.preference_prompt}'\n",
    "    \n",
    "        elapsed_global = time.time() - self.notebook_start_time\n",
    "        time_left = self.cfg.notebook_limit - elapsed_global\n",
    "        problems_left_others = max(0, self.problems_remaining - 1)\n",
    "        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n",
    "    \n",
    "        budget = time_left - reserved_time\n",
    "        budget = min(budget, self.cfg.high_problem_timeout)\n",
    "        budget = max(budget, self.cfg.base_problem_timeout)\n",
    "    \n",
    "        deadline = time.time() + budget\n",
    "    \n",
    "        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n",
    "    \n",
    "        tasks = []\n",
    "    \n",
    "        for attempt_index in range(self.cfg.attempts):\n",
    "            tasks.append((self.cfg.system_prompt, attempt_index))\n",
    "    \n",
    "        detailed_results = []\n",
    "        valid_answers = []\n",
    "    \n",
    "        stop_event = threading.Event()\n",
    "    \n",
    "        executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n",
    "    \n",
    "        try:\n",
    "            futures = []\n",
    "    \n",
    "            for (system_prompt, attempt_index) in tasks:\n",
    "                future = executor.submit(\n",
    "                    self._process_attempt, \n",
    "                    user_input, \n",
    "                    system_prompt, \n",
    "                    attempt_index, \n",
    "                    stop_event, \n",
    "                    deadline\n",
    "                )\n",
    "    \n",
    "                futures.append(future)\n",
    "    \n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    detailed_results.append(result)\n",
    "    \n",
    "                    if result['Answer'] is not None:\n",
    "                        valid_answers.append(result['Answer'])\n",
    "    \n",
    "                    counts = Counter(valid_answers).most_common(1)\n",
    "    \n",
    "                    if counts and counts[0][1] >= self.cfg.early_stop:\n",
    "                        stop_event.set()\n",
    "    \n",
    "                        for f in futures:\n",
    "                            f.cancel()\n",
    "    \n",
    "                        break\n",
    "    \n",
    "                except Exception as exc:\n",
    "                    print(f'Future failed: {exc}')\n",
    "                    continue\n",
    "    \n",
    "        finally:\n",
    "            stop_event.set()\n",
    "            executor.shutdown(wait=True, cancel_futures=True)\n",
    "            \n",
    "            self.problems_remaining = max(0, self.problems_remaining - 1)\n",
    "\n",
    "        if detailed_results:\n",
    "            results_dataframe = pd.DataFrame(detailed_results)\n",
    "            results_dataframe['Entropy'] = results_dataframe['Entropy'].round(3)\n",
    "            results_dataframe['Answer'] = results_dataframe['Answer'].astype('Int64')\n",
    "            \n",
    "            cols = [c for c in results_dataframe.columns if c != 'Log']\n",
    "            display(results_dataframe[cols])\n",
    "\n",
    "        if not valid_answers:\n",
    "            print('\\nResult: 0\\n')\n",
    "            vote_data, final_answer = pd.DataFrame(columns=['Answer', 'Votes', 'Score']), 0\n",
    "        else:\n",
    "            vote_data, final_answer = self._select_answer(detailed_results)\n",
    "        \n",
    "        self.write_debug_logs(detailed_results, vote_data, problem_id=problem_id)\n",
    "        return final_answer\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'server_process'):\n",
    "            self.server_process.terminate()\n",
    "            self.server_process.wait()\n",
    "    \n",
    "        if hasattr(self, 'log_file'):\n",
    "            self.log_file.close()\n",
    "    \n",
    "        if hasattr(self, 'sandbox_pool'):\n",
    "            while not self.sandbox_pool.empty():\n",
    "                try:\n",
    "                    sb = self.sandbox_pool.get_nowait()\n",
    "                    sb.close()\n",
    "    \n",
    "                except Exception:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:30:13.869322Z",
     "iopub.status.busy": "2026-01-13T13:30:13.869198Z",
     "iopub.status.idle": "2026-01-13T13:31:16.430589Z",
     "shell.execute_reply": "2026-01-13T13:31:16.430110Z",
     "shell.execute_reply.started": "2026-01-13T13:30:13.869310Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights from /kaggle/input/gpt-oss-120b/transformers/default/1 into OS Page Cache...\n",
      "Processed 26 files (65.28 GB) in 4.57 seconds.\n",
      "\n",
      "Waiting for vLLM server...\n",
      "Server is ready (took 54.31 seconds).\n",
      "\n",
      "Initializing 16 persistent Jupyter kernels...\n",
      "Kernels initialized in 1.49 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "solver = AIMO3Solver(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:31:16.431304Z",
     "iopub.status.busy": "2026-01-13T13:31:16.431174Z",
     "iopub.status.idle": "2026-01-13T13:31:16.434522Z",
     "shell.execute_reply": "2026-01-13T13:31:16.434131Z",
     "shell.execute_reply.started": "2026-01-13T13:31:16.431292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
    "    id_value = id_.item(0)\n",
    "    question_text = question.item(0)\n",
    "    gc.disable()\n",
    "    final_answer = solver.solve_problem(question_text, problem_id=str(id_value))\n",
    "    gc.enable()\n",
    "    gc.collect()\n",
    "    return pl.DataFrame({'id': id_value, 'answer': final_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T13:31:46.324415Z",
     "iopub.status.busy": "2026-01-13T13:31:46.324192Z",
     "iopub.status.idle": "2026-01-13T13:49:39.884371Z",
     "shell.execute_reply": "2026-01-13T13:49:39.883906Z",
     "shell.execute_reply.started": "2026-01-13T13:31:46.324399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem: A tournament is held with $2^{20}$ runners each of which has a different running speed. In each race, two runners compete against each other with the faster runner always winning the race. The competition consists of $20$ rounds with each runner starting with a score of $0$. In each round, the runners are paired in such a way that in each pair, both runners have the same score at the beginning of the round. The winner of each race in the $i^{\\text{th}}$ round receives $2^{20-i}$ points and the loser gets no points.\n",
      "\n",
      "At the end of the tournament, we rank the competitors according to their scores. Let $N$ denote the number of possible orderings of the competitors at the end of the tournament. Let $k$ be the largest positive integer such that $10^k$ divides $N$. What is the remainder when $k$ is divided by $10^{5}$?\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1768312006.37\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>9109</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.799</td>\n",
       "      <td>21818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>10887</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799</td>\n",
       "      <td>21818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11593</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.906</td>\n",
       "      <td>21818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>12031</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985</td>\n",
       "      <td>62140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15255</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.828</td>\n",
       "      <td>21818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer\n",
       "0        8             9109            12              1    0.799   21818\n",
       "1        4            10887             5              0    0.799   21818\n",
       "2        3            11593             1              0    0.906   21818\n",
       "3        6            12031             5              0    0.985   62140\n",
       "4        1            15255            12              1    0.828   21818"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21818</td>\n",
       "      <td>4</td>\n",
       "      <td>4.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62140</td>\n",
       "      <td>1</td>\n",
       "      <td>1.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0   21818      4  4.816\n",
       "1   62140      1  1.015"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 21818\n",
      "\n",
      "Debug log written to P5.md\n",
      "\n",
      "Problem: Let $\\mathcal{F}$ be the set of functions $\\alpha \\colon \\mathbb{Z}\\to \\mathbb{Z}$ for which there are only finitely many $n \\in \\mathbb{Z}$ such that $\\alpha(n) \\neq 0$. \n",
      "\n",
      "For two functions $\\alpha$ and $\\beta$ in $\\mathcal{F}$, define their product $\\alpha\\star\\beta$ to be $\\sum\\limits_{n\\in\\mathbb{Z}} \\alpha(n)\\cdot \\beta(n)$. Also, for $n\\in\\mathbb{Z}$, define a shift operator $S_n \\colon \\mathcal{F}\\to \\mathcal{F}$ by $S_n(\\alpha)(t)=\\alpha(t+n)$ for all $t \\in \\mathbb{Z}$.\n",
      "\n",
      "A function $\\alpha \\in \\mathcal{F}$ is called \\emph{shifty} if \n",
      "\\begin{itemize}\n",
      "    \\item $\\alpha(m)=0$ for all integers $m<0$ and $m>8$ and\n",
      "    \\item There exists $\\beta \\in \\mathcal{F}$ and integers $k \\neq l$ such that for all $n \\in \\mathbb{Z}$\n",
      "    \\begin{equation*}\n",
      "        S_n(\\alpha)\\star\\beta =\n",
      "        \\begin{cases}\n",
      "            1 & n \\in \\{k,l\\} \\\\\n",
      "            0 & n \\not \\in \\{k,l\\}\n",
      "        \\end{cases}\n",
      "        \\; .\n",
      "    \\end{equation*}\n",
      "\\end{itemize}\n",
      "How many shifty functions are there in $\\mathcal{F}$?\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1768312152.23\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>14277</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.794</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>15241</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.809</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>18224</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.848</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>23762</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.772</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>23804</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.766</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>22283</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>0.827</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer\n",
       "0        6            14277             8              1    0.794     114\n",
       "1        8            15241            11              3    0.809     160\n",
       "2        5            18224            16              1    0.848     160\n",
       "3        7            23762            17              0    0.772     160\n",
       "4        3            23804            25              4    0.766     214\n",
       "5        1            22283            24              7    0.827     160"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>4</td>\n",
       "      <td>4.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214</td>\n",
       "      <td>1</td>\n",
       "      <td>1.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>1.259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0     160      4  4.920\n",
       "1     214      1  1.305\n",
       "2     114      1  1.259"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 160\n",
      "\n",
      "Debug log written to P9.md\n",
      "\n",
      "Problem: Let $n \\geq 6$ be a positive integer. We call a positive integer $n$-Norwegian if it has three distinct positive divisors whose sum is equal to $n$. Let $f(n)$ denote the smallest $n$-Norwegian positive integer. Let $M=3^{2025!}$ and for a non-negative integer $c$ define \n",
      "\\begin{equation*}\n",
      "    g(c)=\\frac{1}{2025!}\\left\\lfloor \\frac{2025! f(M+c)}{M}\\right\\rfloor.\n",
      "\\end{equation*}\n",
      "We can write \n",
      "\\begin{equation*}\n",
      "    g(0)+g(4M)+g(1848374)+g(10162574)+g(265710644)+g(44636594)=\\frac{p}{q}\n",
      "\\end{equation*}\n",
      "where $p$ and $q$ are coprime positive integers. What is the remainder when $p+q$ is divided by $99991$?\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1768312396.63\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>25791</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.775</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>38390</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>0.740</td>\n",
       "      <td>51379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>45294</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.754</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>45254</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>54078</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688</td>\n",
       "      <td>41754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>52941</td>\n",
       "      <td>83</td>\n",
       "      <td>7</td>\n",
       "      <td>0.715</td>\n",
       "      <td>41754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>61101</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>0.730</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>62669</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>0.724</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer\n",
       "0        7            25791            26              1    0.775      23\n",
       "1        8            38390            31              3    0.740   51379\n",
       "2        5            45294            18              1    0.754       9\n",
       "3        6            45254            28              0    0.700      23\n",
       "4        4            54078            60              4    0.688   41754\n",
       "5        2            52941            83              7    0.715   41754\n",
       "6        1            61101            29              3    0.730    <NA>\n",
       "7        3            62669            57              2    0.724    <NA>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41754</td>\n",
       "      <td>2</td>\n",
       "      <td>2.853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51379</td>\n",
       "      <td>1</td>\n",
       "      <td>1.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0   41754      2  2.853\n",
       "1      23      2  2.719\n",
       "2   51379      1  1.351\n",
       "3       9      1  1.326"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 41754\n",
      "\n",
      "Debug log written to P10.md\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    CFG.debug = False\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        # ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
    "        ('/kaggle/input/aimo-p3-hard/test2.csv',)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "datasetId": 9245165,
     "sourceId": 14484164,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 289055161,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 167678,
     "modelInstanceId": 145161,
     "sourceId": 170608,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 169361,
     "modelInstanceId": 146897,
     "sourceId": 172574,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 185749,
     "modelInstanceId": 163393,
     "sourceId": 191689,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 216774,
     "modelInstanceId": 194874,
     "sourceId": 257276,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 422384,
     "modelInstanceId": 404485,
     "sourceId": 510391,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
