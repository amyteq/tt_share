{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":289055161,"sourceType":"kernelVersion"},{"sourceId":510391,"sourceType":"modelInstanceVersion","modelInstanceId":404485,"modelId":422384}],"dockerImageVersionId":31260,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# planner prompts test\n%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'","metadata":{"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T02:17:01.546020Z","iopub.execute_input":"2026-01-18T02:17:01.546208Z","iopub.status.idle":"2026-01-18T02:18:06.431289Z","shell.execute_reply.started":"2026-01-18T02:17:01.546194Z","shell.execute_reply":"2026-01-18T02:18:06.430816Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: keras 3.10.0\nUninstalling keras-3.10.0:\n  Successfully uninstalled keras-3.10.0\nFound existing installation: matplotlib 3.10.0\nUninstalling matplotlib-3.10.0:\n  Successfully uninstalled matplotlib-3.10.0\nFound existing installation: scikit-learn 1.6.1\nUninstalling scikit-learn-1.6.1:\n  Successfully uninstalled scikit-learn-1.6.1\nFound existing installation: tensorflow 2.19.0\nUninstalling tensorflow-2.19.0:\n  Successfully uninstalled tensorflow-2.19.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\nimport subprocess\nimport warnings\nwarnings.simplefilter('ignore')\n\ndef set_env(input_archive, temp_dir):\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir, exist_ok=True)\n        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n    \n    subprocess.run([\n        sys.executable, \n        '-m', \n        'pip', \n        'install', \n        '--no-index', \n        '--find-links', \n        f'{temp_dir}/wheels', \n        'unsloth', \n        'trl', \n        'vllm', \n        'openai_harmony'\n    ], \n    stdout=subprocess.DEVNULL,\n    stderr=subprocess.DEVNULL,\n    check=True)\n\nset_env(\n    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz', \n    temp_dir='/kaggle/tmp/setup'\n)\n\nsubprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T02:18:06.432223Z","iopub.execute_input":"2026-01-18T02:18:06.432368Z","iopub.status.idle":"2026-01-18T02:21:11.515460Z","shell.execute_reply.started":"2026-01-18T02:18:06.432350Z","shell.execute_reply":"2026-01-18T02:21:11.515112Z"}},"outputs":[{"name":"stdout","text":"cl100k_base.tiktoken\no200k_base.tiktoken\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['ls', '/kaggle/tmp/setup/tiktoken_encodings'], returncode=0)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"os.environ['TRANSFORMERS_NO_TF'] = '1'\nos.environ['TRANSFORMERS_NO_FLAX'] = '1'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\nos.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'\n\nimport gc\nimport re\nimport math\nimport time\nimport queue\nimport threading\nimport contextlib\nfrom collections import deque\nfrom typing import Optional\nfrom jupyter_client import KernelManager\nfrom collections import Counter, defaultdict\nfrom concurrent.futures import as_completed, ThreadPoolExecutor\n\nimport pandas as pd\nimport polars as pl\n\nfrom openai import OpenAI\n\nfrom openai_harmony import (\n    HarmonyEncodingName, \n    load_harmony_encoding, \n    SystemContent, \n    ReasoningEffort, \n    ToolNamespaceConfig, \n    Author, \n    Message, \n    Role, \n    TextContent, \n    Conversation\n)\n\nfrom transformers import set_seed\nimport kaggle_evaluation.aimo_3_inference_server\n\nclass CFG:\n\n    attempts_mode = \"serial\"\n    serial_context_char_limit = 1536\n    serial_plan_max_tokens = 384\n    serial_aux_temperature = 0.2\n\n    # --- NEW: planner (separate session) ---\n    planner_system_prompt = (\n        'You are an expert IMO problem-solving PLANNER. '\n        'Your job is to produce a short plan to guide another solver. '\n        'Do NOT solve the problem. Do NOT output any final answer or \\\\boxed{}. '\n        'Do NOT write Python code. Output must be concise and actionable.'\n    )\n\n    planner_prompt = (\n        'Output MUST follow this exact template (no extra text):\\n'\n        'PLAN:\\n'\n        '- <bullet 1>\\n'\n        '- <bullet 2>\\n'\n        '- <bullet 3>\\n'\n        '- <bullet 4>\\n'\n        '- <bullet 5>\\n'\n        'PLAN_DIGEST:\\n'\n        '- <one bullet, <=256 chars>\\n'\n        'END_PLAN\\n'\n        'Rules: bullets ONLY; 5-8 PLAN bullets; each bullet <=120 chars; '\n        'no math derivations, no meta talk, no code, no \\\\boxed{}.'\n    )\n    \n    planner_digest_max_chars = 256\n    planner_history_keep = 8\n    planner_sanitize = True\n\n    served_model_name = 'gpt-oss'\n    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n\n    kv_cache_dtype = 'fp8_e4m3'\n    dtype = 'auto'\n\n    server_timeout = 180\n    session_timeout = 960\n\n    stream_interval = 200\n    context_tokens = 65536\n    buffer_tokens = 512\n    search_tokens = 32\n    top_logprobs = 5\n    batch_size = 256\n    early_stop = 2\n    attempts = 8\n    workers = 16\n    turns = 128\n    seed = 42\n\n    gpu_memory_utilization = 0.96\n    temperature = 1.0\n    min_p = 0.02\n\nset_seed(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T02:21:11.516019Z","iopub.execute_input":"2026-01-18T02:21:11.516142Z","iopub.status.idle":"2026-01-18T02:21:19.194054Z","shell.execute_reply.started":"2026-01-18T02:21:11.516129Z","shell.execute_reply":"2026-01-18T02:21:19.193622Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class AIMO3Template:\n\n    def __init__(self):\n        pass\n\n    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n        return (\n            SystemContent.new()\n            .with_model_identity(system_prompt)\n            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n            .with_tools(tool_config)\n        )\n\n    def apply_chat_template(\n        self, \n        system_prompt: str, \n        user_prompt: str, \n        tool_config: ToolNamespaceConfig\n    ) -> list[Message]:\n        system_content = self.get_system_content(system_prompt, tool_config)        \n        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n        return [system_message, user_message]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T02:21:19.195003Z","iopub.execute_input":"2026-01-18T02:21:19.195262Z","iopub.status.idle":"2026-01-18T02:21:19.198694Z","shell.execute_reply.started":"2026-01-18T02:21:19.195246Z","shell.execute_reply":"2026-01-18T02:21:19.198356Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class AIMO3Server:\n\n    def __init__(self, cfg, port: int = 8000):\n        self.cfg = cfg\n        self.port = port\n        self.base_url = f'http://localhost:{port}/v1'\n        self.api_key = 'sk-local'\n        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n    \n        self._preload_model_weights()\n        \n        self.server_process = self._start_server()\n    \n        self.client = OpenAI(\n            base_url=self.base_url, \n            api_key=self.api_key, \n            timeout=self.cfg.session_timeout\n        )\n    \n        self._wait_for_server()\n        # self._initialize_kernels()\n\n    def _preload_model_weights(self) -> None:\n        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n        start_time = time.time()\n        \n        files_to_load = []\n        total_size = 0\n    \n        for root, _, files in os.walk(self.cfg.model_path):\n            for file_name in files:\n                file_path = os.path.join(root, file_name)\n    \n                if os.path.isfile(file_path):\n                    files_to_load.append(file_path)\n                    total_size += os.path.getsize(file_path)\n    \n        def _read_file(path: str) -> None:\n    \n            with open(path, 'rb') as file_object:\n                while file_object.read(1024 * 1024 * 1024):\n                    pass\n    \n        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n            list(executor.map(_read_file, files_to_load))\n    \n        elapsed = time.time() - start_time\n        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n    \n    def _start_server(self) -> subprocess.Popen:\n        cmd = [\n            sys.executable, \n            '-m', \n            'vllm.entrypoints.openai.api_server', \n            '--seed', \n            str(self.cfg.seed), \n            '--model', \n            self.cfg.model_path, \n            '--served-model-name', \n            self.cfg.served_model_name, \n            '--tensor-parallel-size', \n            '1', \n            '--max-num-seqs', \n            str(self.cfg.batch_size), \n            '--gpu-memory-utilization', \n            str(self.cfg.gpu_memory_utilization), \n            '--host', \n            '0.0.0.0', \n            '--port', \n            str(self.port), \n            '--dtype', \n            self.cfg.dtype, \n            '--kv-cache-dtype', \n            self.cfg.kv_cache_dtype, \n            '--max-model-len', \n            str(self.cfg.context_tokens), \n            '--stream-interval', \n            str(self.cfg.stream_interval), \n            '--async-scheduling', \n            '--disable-log-stats', \n            '--enable-prefix-caching'\n        ]\n    \n        self.log_file = open('vllm_server.log', 'w')\n    \n        return subprocess.Popen(\n            cmd, \n            stdout=self.log_file, \n            stderr=subprocess.STDOUT, \n            start_new_session=True\n        )\n    \n    def _wait_for_server(self):\n        print('Waiting for vLLM server...')\n        start_time = time.time()\n    \n        for _ in range(self.cfg.server_timeout):\n            return_code = self.server_process.poll()\n    \n            if return_code is not None:\n                self.log_file.flush()\n    \n                with open('vllm_server.log', 'r') as log_file:\n                    logs = log_file.read()\n    \n                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n    \n            try:\n                self.client.models.list()\n                elapsed = time.time() - start_time\n                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n    \n                return\n    \n            except Exception:\n                time.sleep(1)\n    \n        raise RuntimeError('Server failed to start (timeout).\\n')\n\n\n    def _initialize_kernels(self) -> None:\n        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n        start_time = time.time()\n    \n        self.sandbox_pool = queue.Queue()\n    \n        def _create_sandbox():\n            # return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n            return None\n    \n        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n    \n            for future in as_completed(futures):\n                self.sandbox_pool.put(future.result())\n    \n        elapsed = time.time() - start_time\n        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T02:41:32.502547Z","iopub.execute_input":"2026-01-18T02:41:32.502754Z","iopub.status.idle":"2026-01-18T02:41:32.511679Z","shell.execute_reply.started":"2026-01-18T02:41:32.502737Z","shell.execute_reply":"2026-01-18T02:41:32.511273Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"if \"server\" in globals(): server\nserver = AIMO3Server(CFG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T02:41:32.512379Z","iopub.execute_input":"2026-01-18T02:41:32.512515Z","iopub.status.idle":"2026-01-18T02:41:36.953563Z","shell.execute_reply.started":"2026-01-18T02:41:32.512502Z","shell.execute_reply":"2026-01-18T02:41:36.953130Z"}},"outputs":[{"name":"stdout","text":"Loading model weights from /kaggle/input/gpt-oss-120b/transformers/default/1 into OS Page Cache...\nProcessed 26 files (65.28 GB) in 4.26 seconds.\n\nWaiting for vLLM server...\nServer is ready (took 0.00 seconds).\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class AIMO3Planner:\n\n    def __init__(self, cfg, port: int = 8000):\n        self.cfg = cfg\n        self.base_url = f'http://localhost:{port}/v1'\n        self.api_key = 'sk-local'\n        self.client = OpenAI(\n            base_url=self.base_url, \n            api_key=self.api_key, \n            timeout=self.cfg.session_timeout\n        )\n        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n        self.template = AIMO3Template()\n\n    def _sanitize(self, text: str) -> str:\n        if not text:\n            return \"\"\n        t = text.strip().replace(\"```\", \"\")\n        lines = [ln.strip() for ln in t.splitlines() if ln.strip()]\n\n        bad = (\n            \"the user asks\", \"user asks\", \"summarize\", \"output only\",\n            \"valid channels\", \"system_prompt\", \"tool_prompt\", \"preference_prompt\",\n        )\n\n        out = []\n        for ln in lines:\n            low = ln.lower()\n            if any(b in low for b in bad):\n                continue\n            if \"\\\\boxed\" in ln or \"boxed\" in low:\n                continue\n            # keep only bullets / headers we expect\n            if low.startswith(\"plan:\") or low.startswith(\"plan_digest:\"):\n                out.append(ln)\n                continue\n            if re.match(r\"^(\\-|\\*|•|\\d+[\\.\\)])\\s+\", ln):\n                out.append(ln)\n                continue\n\n        return \"\\n\".join(out).strip()\n\n    def _parse_from(self, raw: str):\n        # parse sections\n        plan_part = raw\n        digest_part = \"\"\n        m = re.search(r\"(?im)^\\s*PLAN_DIGEST\\s*:\\s*$\", raw)\n        if m:\n            plan_part = raw[: m.start()].strip()\n            digest_part = raw[m.end():].strip()\n\n        # remove PLAN: header if present\n        plan_part = re.sub(r\"(?im)^\\s*PLAN\\s*:\\s*$\", \"\", plan_part).strip()\n\n        # keep plan short (avoid plan blow-up)\n        plan_part = plan_part[: self.cfg.serial_context_char_limit].strip()\n\n        # build digest fallback\n        digest_line = \"\"\n        if digest_part:\n            # first bullet line\n            for ln in digest_part.splitlines():\n                ln = ln.strip()\n                if ln.startswith((\"-\", \"*\", \"•\")):\n                    digest_line = ln.lstrip(\"-*• \").strip()\n                    break\n        if not digest_line:\n            # fallback: first plan bullet\n            for ln in plan_part.splitlines():\n                ln = ln.strip()\n                if ln.startswith((\"-\", \"*\", \"•\")):\n                    digest_line = ln.lstrip(\"-*• \").strip()\n                    break\n\n        if len(digest_line) > self.cfg.planner_digest_max_chars:\n            digest_line = digest_line[: self.cfg.planner_digest_max_chars].strip()\n        return plan_part, digest_line\n\n    def _validate_plan_text(self, text: str) -> tuple[bool, str]:\n        t = (text or \"\").strip()\n        if \"PLAN:\" not in t or \"PLAN_DIGEST:\" not in t:\n            return (False, \"missing headers\")\n        if \"END_PLAN\" not in t:\n            return (False, \"missing END_PLAN\")\n        # count bullets between PLAN and PLAN_DIGEST\n        plan_block = t.split(\"PLAN:\", 1)[1].split(\"PLAN_DIGEST:\", 1)[0]\n        bullets = [ln for ln in plan_block.splitlines() if ln.strip().startswith((\"-\", \"*\", \"•\"))]\n        if len(bullets) < 5:\n            return (False, f\"too few bullets: {len(bullets)}\")\n        digest_block = t.split(\"PLAN_DIGEST:\", 1)[1]\n        digest_bullets = [ln for ln in digest_block.splitlines() if ln.strip().startswith((\"-\", \"*\", \"•\"))]\n        if not digest_bullets:\n            return (False, \"empty digest\")\n        return (True, \"ok\")\n\n    def _make_digest_fallback(self, plan_text: str) -> str:\n        # take first 1-2 bullets and compress\n        lines = [ln.strip() for ln in (plan_text or \"\").splitlines()]\n        bullets = [ln.lstrip(\"-*• \").strip() for ln in lines if ln.startswith((\"-\", \"*\", \"•\"))]\n        s = (bullets[0] if bullets else \"\").strip()\n        if not s:\n            s = \"Try a different approach; enforce small scans + modular checks + caching.\"\n        return s[: self.cfg.planner_digest_max_chars].strip()\n\n    def _build_history_block(self, history: list[dict]) -> str:\n        # program-side structured history (stable, no prompt pollution)\n        if not history:\n            return \"ATTEMPT HISTORY: (none)\\n\"\n\n        lines = [\"ATTEMPT HISTORY (structured):\"]\n        for r in history[-self.cfg.planner_history_keep:]:\n            digest = (r.get(\"PlanDigest\") or \"\").replace(\"\\n\", \" \").strip()\n            if len(digest) > self.cfg.planner_digest_max_chars:\n                digest = digest[: self.cfg.planner_digest_max_chars] + \"...\"\n            lines.append(\n                f\"- Attempt {r.get('Attempt')}: \"\n                f\"Answer={r.get('Answer')}, Entropy={float(r.get('Entropy', 1e9)):.3f}, \"\n                f\"PyCalls={int(r.get('Python Calls', 0) or 0)}, PyErr={int(r.get('Python Errors', 0) or 0)}; \"\n                f\"PlanDigest={digest}\"\n            )\n        return \"\\n\".join(lines) + \"\\n\"\n\n    def _gen_one_shot_text(\n        self,\n        user_text: str,\n        seed: int,\n        max_new_tokens: int,\n        system_prompt: str | None = None,\n        temperature: float | None = None,\n    ) -> str:\n        \"\"\"single-turn generation (planner / summary etc). tools/sandbox disabled.\"\"\"\n        sp = system_prompt or self.cfg.system_prompt\n        temp = self.cfg.serial_aux_temperature if temperature is None else float(temperature)\n        dummy_tool_cfg = ToolNamespaceConfig(name=\"python\", description=\"\", tools=[])\n\n        messages = self.template.apply_chat_template(sp, user_text, dummy_tool_cfg)\n        conversation = Conversation.from_messages(messages)\n\n        prompt_ids = self.encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n        # max_tokens = self.cfg.context_tokens - len(prompt_ids) - self.cfg.buffer_tokens\n        max_tokens = self.cfg.serial_plan_max_tokens\n        if max_tokens <= 0:\n            return \"\"\n\n        max_tokens = min(max_tokens, max_new_tokens)\n\n        stream = self.client.completions.create(\n            model=self.cfg.served_model_name,\n            temperature=temp,\n            logprobs=None,\n            max_tokens=max_tokens,\n            prompt=prompt_ids,\n            seed=seed,\n            stream=True,\n            extra_body={\n                \"min_p\": self.cfg.min_p,\n                \"stop_token_ids\": self.stop_token_ids,\n                \"return_token_ids\": True,\n            },\n        )\n\n        chunks = []\n        try:\n            for chunk in stream:\n                txt = chunk.choices[0].text\n                if txt:\n                    chunks.append(txt)\n        finally:\n            stream.close()\n\n        return \"\".join(chunks).strip()\n        \n    def gen_plan(self, problem_text: str, history: list[dict], attempt_index: int) -> tuple[str, str, str]:\n        prompt = (\n            f\"{self.cfg.planner_prompt}\\n\\n\"\n            f\"PROBLEM:\\n{problem_text}\\n\\n\"\n            f\"{self._build_history_block(history)}\\n\"\n            f\"Now produce PLAN and PLAN_DIGEST.\"\n        )\n\n        seed = int((self.cfg.seed + 777) * (attempt_index + 1) ** 2)\n\n        raw0 = self._gen_one_shot_text(\n            prompt,\n            seed=seed,\n            max_new_tokens=self.cfg.serial_plan_max_tokens,\n            system_prompt=self.cfg.planner_system_prompt,\n            temperature=self.cfg.serial_aux_temperature,\n        )\n\n        ok, reason = self._validate_plan_text(raw0)\n        if not ok:\n            # one-shot repair: rewrite ONLY, no new content\n            repair_prompt = (\n                \"REWRITE the following into the EXACT required template. \"\n                \"Do not add any extra commentary.\\n\\n\"\n                \"BAD_OUTPUT:\\n\"\n                f\"{raw0}\\n\\n\"\n                \"REQUIRED_TEMPLATE:\\n\"\n                f\"{self.cfg.planner_prompt}\\n\"\n            )\n            raw0 = self._gen_one_shot_text(\n                repair_prompt,\n                seed=seed + 1,\n                max_new_tokens=256,\n                system_prompt=self.cfg.planner_system_prompt,\n                temperature=0.0,   # repair 用 0 温度更稳\n            ).strip()\n        raw = raw0\n        if self.cfg.planner_sanitize:\n            raw = self._sanitize(raw0)\n\n            # fallback: keep raw (trim) rather than empty\n            if not raw.strip():\n                raw = raw0[: self.cfg.serial_context_char_limit].strip()\n\n        plan_part, digest_line = self._parse_from(raw)\n        if not digest_line.strip():\n            digest_line = self._make_digest_fallback(plan_part)\n\n        return plan_part, digest_line, raw0, raw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T02:41:36.954395Z","iopub.execute_input":"2026-01-18T02:41:36.954544Z","iopub.status.idle":"2026-01-18T02:41:36.970014Z","shell.execute_reply.started":"2026-01-18T02:41:36.954530Z","shell.execute_reply":"2026-01-18T02:41:36.969631Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"problem = \"\"\"Let $n \\geq 6$ be a positive integer. We call a positive integer $n$-Norwegian if it has three distinct positive divisors whose sum is equal to $n$. Let $f(n)$ denote the smallest $n$-Norwegian positive integer. Let $M=3^{2025!}$ and for a non-negative integer $c$ define \n\\begin{equation*}\n    g(c)=\\frac{1}{2025!}\\left\\lfloor \\frac{2025! f(M+c)}{M}\\right\\rfloor.\n\\end{equation*}\nWe can write \n\\begin{equation*}\n    g(0)+g(4M)+g(1848374)+g(10162574)+g(265710644)+g(44636594)=\\frac{p}{q}\n\\end{equation*}\nwhere $p$ and $q$ are coprime positive integers. What is the remainder when $p+q$ is divided by $99991$?\"\"\"\n\nplanner = AIMO3Planner(CFG)\nplan, plan_digest, plan_raw, plan_san = planner.gen_plan(problem, [], attempt_index=1)\nprint(f\"plan:\\n{plan}\\n\\nplan_digest:\\n{plan_digest}\\n\\nplan_raw:\\n{plan_raw}\\n\\nplan_sanitized:\\n{plan_san}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T02:41:36.970520Z","iopub.execute_input":"2026-01-18T02:41:36.970650Z","iopub.status.idle":"2026-01-18T02:41:40.498721Z","shell.execute_reply.started":"2026-01-18T02:41:36.970636Z","shell.execute_reply":"2026-01-18T02:41:40.498320Z"}},"outputs":[{"name":"stdout","text":"plan:\nanalysisThe user asks: \"REWRITE the following into the EXACT required template. Do not add any extra commentary.\"\n\nThey provide a BAD_OUTPUT: \"analysisWe need to parse the problem carefully...\" and then a REQUIRED_TEMPLATE: \"Output MUST follow this exact template (no extra text): PLAN: - <bullet 1> - <bullet 2> - <bullet 3> - <bullet 4> - <bullet 5> PLAN_DIGEST: - <one bullet, <=256 chars> END_PLAN\"\n\nThus the user wants us to rewrite the solution plan (the analysis) into the required template. They gave a BAD_OUTPUT that includes analysis and a plan but not in the required format. They want us to produce a plan in bullet points (5-8 bullets) each <=120 chars, and a PLAN_DIGEST bullet summarizing in <=256 chars. No extra commentary.\n\nThus we need to produce a plan for solving the problem. The problem is about n-Norwegian numbers, f(n), M = 3^{2025!}, g(c) defined as floor(2025! * f(M + c) / M) / 2025!. Then sum g(0) + g(4M) + ... = p\n\nplan_digest:\nTry a different approach; enforce small scans + modular checks + caching.\n\nplan_raw:\nanalysisThe user asks: \"REWRITE the following into the EXACT required template. Do not add any extra commentary.\"\n\nThey provide a BAD_OUTPUT: \"analysisWe need to parse the problem carefully...\" and then a REQUIRED_TEMPLATE: \"Output MUST follow this exact template (no extra text): PLAN: - <bullet 1> - <bullet 2> - <bullet 3> - <bullet 4> - <bullet 5> PLAN_DIGEST: - <one bullet, <=256 chars> END_PLAN\"\n\nThus the user wants us to rewrite the solution plan (the analysis) into the required template. They gave a BAD_OUTPUT that includes analysis and a plan but not in the required format. They want us to produce a plan in bullet points (5-8 bullets) each <=120 chars, and a PLAN_DIGEST bullet summarizing in <=256 chars. No extra commentary.\n\nThus we need to produce a plan for solving the problem. The problem is about n-Norwegian numbers, f(n), M = 3^{2025!}, g(c) defined as floor(2025! * f(M + c) / M) / 2025!. Then sum g(0) + g(4M) + ... = p\n\nplan_sanitized:\nanalysisThe user asks: \"REWRITE the following into the EXACT required template. Do not add any extra commentary.\"\n\nThey provide a BAD_OUTPUT: \"analysisWe need to parse the problem carefully...\" and then a REQUIRED_TEMPLATE: \"Output MUST follow this exact template (no extra text): PLAN: - <bullet 1> - <bullet 2> - <bullet 3> - <bullet 4> - <bullet 5> PLAN_DIGEST: - <one bullet, <=256 chars> END_PLAN\"\n\nThus the user wants us to rewrite the solution plan (the analysis) into the required template. They gave a BAD_OUTPUT that includes analysis and a plan but not in the required format. They want us to produce a plan in bullet points (5-8 bullets) each <=120 chars, and a PLAN_DIGEST bullet summarizing in <=256 chars. No extra commentary.\n\nThus we need to produce a plan for solving the problem. The problem is about n-Norwegian numbers, f(n), M = 3^{2025!}, g(c) defined as floor(2025! * f(M + c) / M) / 2025!. Then sum g(0) + g(4M) + ... = p\n","output_type":"stream"}],"execution_count":10}]}