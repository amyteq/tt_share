{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":14501829,"sourceType":"datasetVersion","datasetId":9245165},{"sourceId":289055161,"sourceType":"kernelVersion"},{"sourceId":510391,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":404485,"modelId":422384}],"dockerImageVersionId":31260,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:20:52.536180Z","iopub.execute_input":"2026-01-17T08:20:52.536776Z","iopub.status.idle":"2026-01-17T08:21:56.737769Z","shell.execute_reply.started":"2026-01-17T08:20:52.536757Z","shell.execute_reply":"2026-01-17T08:21:56.737276Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: keras 3.10.0\nUninstalling keras-3.10.0:\n  Successfully uninstalled keras-3.10.0\nFound existing installation: matplotlib 3.10.0\nUninstalling matplotlib-3.10.0:\n  Successfully uninstalled matplotlib-3.10.0\nFound existing installation: scikit-learn 1.6.1\nUninstalling scikit-learn-1.6.1:\n  Successfully uninstalled scikit-learn-1.6.1\nFound existing installation: tensorflow 2.19.0\nUninstalling tensorflow-2.19.0:\n  Successfully uninstalled tensorflow-2.19.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\nimport subprocess\nimport warnings\nwarnings.simplefilter('ignore')\n\ndef set_env(input_archive, temp_dir):\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir, exist_ok=True)\n        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n    \n    subprocess.run([\n        sys.executable, \n        '-m', \n        'pip', \n        'install', \n        '--no-index', \n        '--find-links', \n        f'{temp_dir}/wheels', \n        'unsloth', \n        'trl', \n        'vllm', \n        'openai_harmony'\n    ], \n    stdout=subprocess.DEVNULL,\n    stderr=subprocess.DEVNULL,\n    check=True)\n\nset_env(\n    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz', \n    temp_dir='/kaggle/tmp/setup'\n)\n\nsubprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:21:56.738723Z","iopub.execute_input":"2026-01-17T08:21:56.738881Z","iopub.status.idle":"2026-01-17T08:25:11.821688Z","shell.execute_reply.started":"2026-01-17T08:21:56.738863Z","shell.execute_reply":"2026-01-17T08:25:11.821321Z"}},"outputs":[{"name":"stdout","text":"cl100k_base.tiktoken\no200k_base.tiktoken\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['ls', '/kaggle/tmp/setup/tiktoken_encodings'], returncode=0)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"os.environ['TRANSFORMERS_NO_TF'] = '1'\nos.environ['TRANSFORMERS_NO_FLAX'] = '1'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\nos.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'\n\nimport gc\nimport re\nimport math\nimport time\nimport queue\nimport threading\nimport contextlib\nfrom collections import deque\nfrom typing import Optional\nfrom jupyter_client import KernelManager\nfrom collections import Counter, defaultdict\nfrom concurrent.futures import as_completed, ThreadPoolExecutor\n\nimport pandas as pd\nimport polars as pl\n\nfrom openai import OpenAI\n\nfrom openai_harmony import (\n    HarmonyEncodingName, \n    load_harmony_encoding, \n    SystemContent, \n    ReasoningEffort, \n    ToolNamespaceConfig, \n    Author, \n    Message, \n    Role, \n    TextContent, \n    Conversation\n)\n\nfrom transformers import set_seed\nimport kaggle_evaluation.aimo_3_inference_server\n\nclass CFG:\n\n    attempts_mode = \"serial\"\n    serial_context_char_limit = 1400\n    serial_plan_max_tokens = 200\n    serial_aux_temperature = 0.2\n\n    system_prompt = (\n        'You are a world-class International Mathematical Olympiad (IMO) competitor. '\n        'The final answer must be a non-negative integer between 0 and 99999. '\n        'You must place the final integer answer inside \\\\boxed{}.'\n        'Use \\\\boxed{} exactly once at the very end (never for intermediate results).'\n        'If you cannot finish within time, output your best verified result anyway as \\\\boxed{N}.'\n    )\n    \n    tool_prompt = (\n        'Use this tool to execute Python code. '\n        'The environment is a stateful Jupyter notebook. '\n        'You must use print() to output results. '\n        'Python safety: \\n'\n        '- never compute huge integers directly. \\n'\n        '- ALWAYS use modular pow(a, e, mod) for big exponents; NEVER call pow(a, huge_e) without mod (or mod=None). Use sympy.factorint(n) when divisors matter. \\n'\n        '- Never use while True (must have explicit bounds). \\n'\n        '- Complexity budget: keep each Python call fast (<~2 seconds); start with small bounds and scale up only if needed; \\n'\n        '- avoid large nested loops or wide scans—if a scan times out, reduce the bound or change the method (use modular/factorization/sieving). \\n'\n        '- Use explicit namespaces for number theory helpers (math.gcd/math.lcm or sympy.gcd); do not use bare gcd/lcm names. \\n'\n        '- Dict preview must use list(d.items())[:k] (never d[:k]). \\n'\n        'Timeout-avoidance rules:\\n'\n        '- Batch work: do NOT call python repeatedly for small steps; write one cell that computes all needed values.\\n'\n        '- Before any scan/loop: start with a tiny bound (<=200 or <=2000), time it, then expand by x2/x3 only if fast.\\n'\n        '- If you see \"[ERROR] Execution timed out\", do NOT rerun the same code; shrink bounds or add caching.\\n'\n        '- If computing many ratios/candidates, use caching and early-break; avoid list comprehensions calling expensive funcs.\\n'\n    )\n    \n    preference_prompt = (\n        'You have access to `math`, `numpy` and `sympy` to solve the problem.'\n        'Prefer verifiable approaches: reduce to modular arithmetic / factorization / small candidate sets. '\n        'If an argument depends on choosing the best among many integers, define a candidate set and a coverage strategy (prove a bound or do a bounded scan + verification).'\n    )\n\n    # --- NEW: planner (separate session) ---\n    planner_system_prompt = (\n        'You are an expert IMO problem-solving PLANNER. '\n        'Your job is to produce a short plan to guide another solver. '\n        'Do NOT solve the problem. Do NOT output any final answer or \\\\boxed{}. '\n        'Do NOT write Python code. Output must be concise and actionable.'\n    )\n\n    planner_prompt = (\n        'Return EXACTLY two sections:\\n'\n        'PLAN:\\n'\n        '- 5-8 bullet steps; include what to verify with Python and what small scans/bounds to use.\\n'\n        'PLAN_DIGEST:\\n'\n        '- 1 bullet, <= 140 characters, capturing the unique strategy of this attempt.\\n'\n        \"Rules: no meta talk (no 'the user asks', no 'I will'), no \\\\boxed{}, no code.\"\n    )\n    \n    planner_digest_max_chars = 140\n    planner_history_keep = 8\n    planner_sanitize = True\n\n    served_model_name = 'gpt-oss'\n    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n\n    # not working??!!\n    # served_model_name = 'gpt-oss-sft'\n    # model_path = '/kaggle/input/gpt-oss-sft-aimo3/transformers/default/1'\n    \n    kv_cache_dtype = 'fp8_e4m3'\n    dtype = 'auto'\n\n    high_problem_timeout = 900\n    base_problem_timeout = 300\n\n    notebook_limit = 17400\n    server_timeout = 180\n\n    session_timeout = 960\n    jupyter_timeout = 10    # 6\n    sandbox_timeout = 3\n\n    stream_interval = 200\n    context_tokens = 65536\n    buffer_tokens = 512\n    search_tokens = 32\n    top_logprobs = 5\n    batch_size = 256\n    early_stop = 3\n    attempts = 8\n    workers = 16\n    turns = 128\n    seed = 42\n\n    gpu_memory_utilization = 0.96\n    temperature = 1.0\n    min_p = 0.02\n    enable_error_penalty = True\n    error_penalty_lambda = 0.7\n    debug = True\n    debug_req = True\n    debug_resp = True\n    debug_limit = 3000\n\nset_seed(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:25:11.822311Z","iopub.execute_input":"2026-01-17T08:25:11.822448Z","iopub.status.idle":"2026-01-17T08:25:18.645961Z","shell.execute_reply.started":"2026-01-17T08:25:11.822435Z","shell.execute_reply":"2026-01-17T08:25:18.645517Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class AIMO3Template:\n\n    def __init__(self):\n        pass\n\n    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n        return (\n            SystemContent.new()\n            .with_model_identity(system_prompt)\n            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n            .with_tools(tool_config)\n        )\n\n    def apply_chat_template(\n        self, \n        system_prompt: str, \n        user_prompt: str, \n        tool_config: ToolNamespaceConfig\n    ) -> list[Message]:\n        system_content = self.get_system_content(system_prompt, tool_config)        \n        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n        return [system_message, user_message]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:25:18.646932Z","iopub.execute_input":"2026-01-17T08:25:18.647216Z","iopub.status.idle":"2026-01-17T08:25:18.650728Z","shell.execute_reply.started":"2026-01-17T08:25:18.647199Z","shell.execute_reply":"2026-01-17T08:25:18.650355Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class AIMO3Sandbox:\n\n    _port_lock = threading.Lock()\n    _next_port = 50000\n\n    @classmethod\n    def _get_next_ports(cls, count: int = 5) -> list[int]:\n        with cls._port_lock:\n            ports = list(range(cls._next_port, cls._next_port + count))\n            cls._next_port += count\n            return ports\n\n    def __init__(self, timeout: float):\n        self._default_timeout = timeout\n        self._owns_kernel = False\n        self._client = None\n        self._km = None\n        \n        ports = self._get_next_ports(5)\n\n        env = os.environ.copy()\n        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n        env['JUPYTER_PLATFORM_DIRS'] = '1'\n        env['PYTHONWARNINGS'] = 'ignore'\n        env['MPLBACKEND'] = 'Agg'\n\n        self._km = KernelManager()\n        self._km.shell_port = ports[0]\n        self._km.iopub_port = ports[1]\n        self._km.stdin_port = ports[2]\n        self._km.hb_port = ports[3]\n        self._km.control_port = ports[4]\n        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n\n        self._client = self._km.blocking_client()\n        self._client.start_channels()\n        self._client.wait_for_ready(timeout=self._default_timeout)\n        self._owns_kernel = True\n\n        self.execute(\n            'import math\\n'\n            'import numpy\\n'\n            'import sympy\\n'\n            'import itertools\\n'\n            'import collections\\n'\n            'import mpmath\\n'\n            'mpmath.mp.dps = 64\\n'\n        )\n\n    def _format_error(self, traceback: list[str]) -> str:\n        clean_lines = []\n        for frame in traceback:\n            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n                continue\n            clean_lines.append(clean_frame)\n        return ''.join(clean_lines)\n\n    def execute(self, code: str, timeout: float | None = None) -> str:\n        client = self._client\n        effective_timeout = timeout or self._default_timeout\n        \n        msg_id = client.execute(\n            code, \n            store_history=True, \n            allow_stdin=False, \n            stop_on_error=False\n        )\n\n        stdout_parts = []\n        stderr_parts = []\n        \n        start_time = time.time()\n\n        while True:\n            elapsed = time.time() - start_time\n\n            if elapsed > effective_timeout:\n                self._km.interrupt_kernel()\n\n                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n\n            try:\n                msg = client.get_iopub_msg(timeout=1.0)\n\n            except queue.Empty:\n                continue\n\n            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n                continue\n\n            msg_type = msg.get('msg_type')\n            content = msg.get('content', {})\n\n            if msg_type == 'stream':\n                text = content.get('text', '')\n\n                if content.get('name') == 'stdout':\n                    stdout_parts.append(text)\n\n                else:\n                    stderr_parts.append(text)\n\n            elif msg_type == 'error':\n                traceback_list = content.get('traceback', [])\n\n                stderr_parts.append(self._format_error(traceback_list))\n\n            elif msg_type in {'execute_result', 'display_data'}:\n                data = content.get('data', {})\n                text = data.get('text/plain')\n\n                if text:\n                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n\n            elif msg_type == 'status':\n                if content.get('execution_state') == 'idle':\n                    break\n\n        stdout = ''.join(stdout_parts)\n        stderr = ''.join(stderr_parts)\n\n        if stderr:\n            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n\n        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n\n    def close(self):\n        with contextlib.suppress(Exception):\n            if self._client:\n                self._client.stop_channels()\n\n        if self._owns_kernel and self._km is not None:\n            with contextlib.suppress(Exception):\n                self._km.shutdown_kernel(now=True)\n\n            with contextlib.suppress(Exception):\n                self._km.cleanup_resources()\n\n    def reset(self):\n        self.execute(\n            '%reset -f\\n'\n            'import math\\n'\n            'import numpy\\n'\n            'import sympy\\n'\n            'import itertools\\n'\n            'import collections\\n'\n            'import mpmath\\n'\n            'mpmath.mp.dps = 64\\n'\n        )\n\n    def __del__(self):\n        self.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:25:18.651244Z","iopub.execute_input":"2026-01-17T08:25:18.651368Z","iopub.status.idle":"2026-01-17T08:25:19.283903Z","shell.execute_reply.started":"2026-01-17T08:25:18.651356Z","shell.execute_reply":"2026-01-17T08:25:19.283423Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class AIMO3Tool:\n\n    def __init__(self, local_jupyter_timeout: float, tool_prompt: str, sandbox=None):\n        self._local_jupyter_timeout = local_jupyter_timeout\n        self._tool_prompt = tool_prompt\n        self._jupyter_session = sandbox\n        \n        self._owns_session = sandbox is None\n        \n        self._execution_lock = threading.Lock()\n        self._init_lock = threading.Lock()\n\n    def _ensure_session(self):\n\n        if self._jupyter_session is None:\n            with self._init_lock:\n                if self._jupyter_session is None:\n                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n\n    def _ensure_last_print(self, code: str) -> str:\n\n        lines = code.strip().split('\\n')\n\n        if not lines:\n            return code\n\n        last_line = lines[-1].strip()\n\n        if 'print' in last_line or 'import' in last_line:\n            return code\n\n        if not last_line:\n            return code\n\n        if last_line.startswith('#'):\n            return code\n\n        lines[-1] = 'print(' + last_line + ')'\n\n        return '\\n'.join(lines)\n\n    @property\n    def instruction(self) -> str:\n        return self._tool_prompt\n\n    @property\n    def tool_config(self) -> ToolNamespaceConfig:\n        return ToolNamespaceConfig(\n            name='python', \n            description=self.instruction, \n            tools=[]\n        )\n\n    def _make_response(self, output: str, channel: str | None = None) -> Message:\n        content = TextContent(text=output)\n        author = Author(role=Role.TOOL, name='python')\n        message = Message(author=author, content=[content]).with_recipient('assistant')\n\n        if channel:\n            message = message.with_channel(channel)\n\n        return message\n\n    def process_sync_plus(self, message: Message) -> list[Message]:\n        self._ensure_session()\n        raw_script = message.content[0].text\n        final_script = self._ensure_last_print(raw_script)\n\n        with self._execution_lock:\n            try:\n                output = self._jupyter_session.execute(final_script)\n\n            except TimeoutError as exc:\n                output = f'[ERROR] {exc}'\n\n        return [self._make_response(output, channel=message.channel)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:25:19.284544Z","iopub.execute_input":"2026-01-17T08:25:19.284693Z","iopub.status.idle":"2026-01-17T08:25:19.297479Z","shell.execute_reply.started":"2026-01-17T08:25:19.284679Z","shell.execute_reply":"2026-01-17T08:25:19.297066Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class AIMO3Logger:\n    def __init__(self, cfg):\n        self.cfg = cfg\n\n    def get_debug_snippet(self, text: str) -> str:\n        limit = self.cfg.debug_limit\n        if not text or len(text) <= limit:\n            return text or \"\"\n        head = text[:100]\n        tail_len = limit - 100\n        tail = text[-tail_len:]\n        return f\"{head}\\n ... \\n{tail}\"\n\n    def format_markdown(self, text: str, mode: str = \"quote\") -> str:\n        if not text:\n            return \"\"\n        lines = text.split('\\n')\n        escaped_lines = [f\"\\\\{line}\" if line.startswith('#') else line for line in lines]\n        processed_text = '\\n'.join(escaped_lines)\n        if mode in [\"markdown\", \"text\", \"python\"]:\n            return f\"```{mode}\\n{processed_text}\\n```\\n\"\n        if mode == \"quote\":\n            return '\\n'.join([f\"> {line}\" for line in escaped_lines]) + \"\\n\"\n        if mode == \"\":\n            return processed_text + \"\\n\"\n        return f\"```\\n{processed_text}\\n```\\n\"\n\n    def log_planner_block(self, plan_raw: str, plan_sanitized: str, plan_digest: str) -> str:\n        raw_snip = self.get_debug_snippet(plan_raw)\n        san_snip = self.get_debug_snippet(plan_sanitized)\n        digest = plan_digest.strip()\n\n        out = []\n        out.append(\"### Planner Output (raw)\\n\")\n        out.append(self.format_markdown(raw_snip, mode=\"text\"))\n        out.append(\"### Planner Output (sanitized)\\n\")\n        out.append(self.format_markdown(san_snip, mode=\"text\"))\n        out.append(\"### Plan Digest\\n\")\n        out.append(self.format_markdown(digest, mode=\"text\"))\n        return \"\".join(out)\n\n    def write_debug_logs(self, detailed_results, vote_dataframe, problem, problem_id=\"UNK\", problem_time=\"\"):\n        if not self.cfg.debug:\n            return\n        try:\n            summary_lines = [\"\\n## Summary Stats\\n\"]\n            if detailed_results:\n                df = pd.DataFrame(detailed_results)\n                cols = [c for c in df.columns if c not in ['Log', 'Plan', 'PlanRaw', 'PlanSanitized', 'PlanDigest']]\n                summary_lines.append(df[cols].to_markdown(index=False))\n                summary_lines.append(\"\\n\\n\")\n\n            if not vote_dataframe.empty:\n                summary_lines.append(\"## Vote Counts\\n\")\n                summary_lines.append(vote_dataframe.to_markdown(index=False))\n                summary_lines.append(\"\\n\")\n\n            final_log_content = [f\"# Problem ID: {problem_id}\\n\"]\n            final_log_content.append(f\"Problem spent time: **{problem_time}**\\n\\n\")\n            final_log_content.append(f\"**Problem:**\\n{self.format_markdown(problem)}\\n\")\n            final_log_content.append(f\"**system_prompt:**\\n{self.format_markdown(self.cfg.system_prompt)}\\n\")\n            final_log_content.append(f\"**tool_prompt:**\\n{self.format_markdown(self.cfg.tool_prompt)}\\n\")\n            final_log_content.append(f\"**preference_prompt:**\\n{self.format_markdown(self.cfg.preference_prompt)}\\n\")\n            final_log_content.append(f\"**planner_system_prompt:**\\n{self.format_markdown(self.cfg.planner_system_prompt)}\\n\")\n            final_log_content.append(f\"**planner_prompt:**\\n{self.format_markdown(self.cfg.planner_prompt)}\\n\")\n            final_log_content.append(f\"attempts_mode: **{self.cfg.attempts_mode}**, served_model_name: **{self.cfg.served_model_name}**\\n\")\n            final_log_content.extend(summary_lines)\n            final_log_content.append(\"\\n===\\n\")\n\n            sorted_results = sorted(detailed_results, key=lambda x: x['Attempt'])\n            for res in sorted_results:\n                log_content = res.get('Log', '')\n                if log_content:\n                    final_log_content.append(log_content)\n                    final_log_content.append(\"\\n===\\n\")\n\n            output_path = f\"{problem_id}.md\"\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\".join(final_log_content))\n            print(f\"Debug log written to {output_path}\")\n        except Exception as e:\n            print(f\"Failed to write debug log: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:25:19.298085Z","iopub.execute_input":"2026-01-17T08:25:19.298230Z","iopub.status.idle":"2026-01-17T08:25:19.313295Z","shell.execute_reply.started":"2026-01-17T08:25:19.298216Z","shell.execute_reply":"2026-01-17T08:25:19.312906Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class AIMO3Planner:\n    def __init__(self, cfg, template: AIMO3Template, client: OpenAI, logger: AIMO3Logger = None):\n        self.cfg = cfg\n        self.template = template\n        self.client = client\n        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n\n    def _sanitize(self, text: str) -> str:\n        if not text:\n            return \"\"\n        t = text.strip().replace(\"```\", \"\")\n        lines = [ln.strip() for ln in t.splitlines() if ln.strip()]\n\n        bad = (\n            \"the user asks\", \"user asks\", \"summarize\", \"output only\",\n            \"valid channels\", \"system_prompt\", \"tool_prompt\", \"preference_prompt\",\n        )\n\n        out = []\n        for ln in lines:\n            low = ln.lower()\n            if any(b in low for b in bad):\n                continue\n            if \"\\\\boxed\" in ln or \"boxed\" in low:\n                continue\n            # keep only bullets / headers we expect\n            if low.startswith(\"plan:\") or low.startswith(\"plan_digest:\"):\n                out.append(ln)\n                continue\n            if re.match(r\"^(\\-|\\*|•|\\d+[\\.\\)])\\s+\", ln):\n                out.append(ln)\n                continue\n\n        return \"\\n\".join(out).strip()\n\n    def _build_history_block(self, history: list[dict]) -> str:\n        # program-side structured history (stable, no prompt pollution)\n        if not history:\n            return \"ATTEMPT HISTORY: (none)\\n\"\n\n        lines = [\"ATTEMPT HISTORY (structured):\"]\n        for r in history[-self.cfg.planner_history_keep:]:\n            digest = (r.get(\"PlanDigest\") or \"\").replace(\"\\n\", \" \").strip()\n            if len(digest) > self.cfg.planner_digest_max_chars:\n                digest = digest[: self.cfg.planner_digest_max_chars] + \"...\"\n            lines.append(\n                f\"- Attempt {r.get('Attempt')}: \"\n                f\"Answer={r.get('Answer')}, Entropy={float(r.get('Entropy', 1e9)):.3f}, \"\n                f\"PyCalls={int(r.get('Python Calls', 0) or 0)}, PyErr={int(r.get('Python Errors', 0) or 0)}; \"\n                f\"PlanDigest={digest}\"\n            )\n        return \"\\n\".join(lines) + \"\\n\"\n\n    def _gen_one_shot_text(\n        self,\n        user_text: str,\n        seed: int,\n        max_new_tokens: int,\n        system_prompt: str | None = None,\n        temperature: float | None = None,\n    ) -> str:\n        \"\"\"single-turn generation (planner / summary etc). tools/sandbox disabled.\"\"\"\n        sp = system_prompt or self.cfg.system_prompt\n        temp = self.cfg.serial_aux_temperature if temperature is None else float(temperature)\n        dummy_tool_cfg = ToolNamespaceConfig(name=\"python\", description=\"\", tools=[])\n\n        messages = self.template.apply_chat_template(sp, user_text, dummy_tool_cfg)\n        conversation = Conversation.from_messages(messages)\n\n        prompt_ids = self.encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n        max_tokens = self.cfg.context_tokens - len(prompt_ids) - self.cfg.buffer_tokens\n        if max_tokens <= 0:\n            return \"\"\n\n        max_tokens = min(max_tokens, max_new_tokens)\n\n        stream = self.client.completions.create(\n            model=self.cfg.served_model_name,\n            temperature=temp,\n            logprobs=None,\n            max_tokens=max_tokens,\n            prompt=prompt_ids,\n            seed=seed,\n            stream=True,\n            extra_body={\n                \"min_p\": self.cfg.min_p,\n                \"stop_token_ids\": self.stop_token_ids,\n                \"return_token_ids\": True,\n            },\n        )\n\n        chunks = []\n        try:\n            for chunk in stream:\n                txt = chunk.choices[0].text\n                if txt:\n                    chunks.append(txt)\n        finally:\n            stream.close()\n\n        return \"\".join(chunks).strip()\n        \n    def gen_plan(self, problem_text: str, history: list[dict], attempt_index: int) -> tuple[str, str, str]:\n        prompt = (\n            f\"{self.cfg.planner_prompt}\\n\\n\"\n            f\"PROBLEM:\\n{problem_text}\\n\\n\"\n            f\"{self._build_history_block(history)}\\n\"\n            f\"Now produce PLAN and PLAN_DIGEST.\"\n        )\n\n        seed = int((self.cfg.seed + 777) * (attempt_index + 1) ** 2)\n\n        raw0 = self._gen_one_shot_text(\n            prompt,\n            seed=seed,\n            max_new_tokens=self.cfg.serial_plan_max_tokens,\n            system_prompt=self.cfg.planner_system_prompt,\n            temperature=self.cfg.serial_aux_temperature,\n        )\n        raw = raw0\n\n        if self.cfg.planner_sanitize:\n            raw = self._sanitize(raw0)\n\n            # fallback: keep raw (trim) rather than empty\n            if not raw.strip():\n                raw = raw0[: self.cfg.serial_context_char_limit].strip()\n\n        # parse sections\n        plan_part = raw\n        digest_part = \"\"\n        m = re.search(r\"(?im)^\\s*PLAN_DIGEST\\s*:\\s*$\", raw)\n        if m:\n            plan_part = raw[: m.start()].strip()\n            digest_part = raw[m.end():].strip()\n\n        # remove PLAN: header if present\n        plan_part = re.sub(r\"(?im)^\\s*PLAN\\s*:\\s*$\", \"\", plan_part).strip()\n\n        # keep plan short (avoid plan blow-up)\n        plan_part = plan_part[: self.cfg.serial_context_char_limit].strip()\n\n        # build digest fallback\n        digest_line = \"\"\n        if digest_part:\n            # first bullet line\n            for ln in digest_part.splitlines():\n                ln = ln.strip()\n                if ln.startswith((\"-\", \"*\", \"•\")):\n                    digest_line = ln.lstrip(\"-*• \").strip()\n                    break\n        if not digest_line:\n            # fallback: first plan bullet\n            for ln in plan_part.splitlines():\n                ln = ln.strip()\n                if ln.startswith((\"-\", \"*\", \"•\")):\n                    digest_line = ln.lstrip(\"-*• \").strip()\n                    break\n\n        if len(digest_line) > self.cfg.planner_digest_max_chars:\n            digest_line = digest_line[: self.cfg.planner_digest_max_chars].strip()\n\n        return plan_part, digest_line, raw0, raw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:25:19.313812Z","iopub.execute_input":"2026-01-17T08:25:19.313962Z","iopub.status.idle":"2026-01-17T08:25:19.329265Z","shell.execute_reply.started":"2026-01-17T08:25:19.313935Z","shell.execute_reply":"2026-01-17T08:25:19.328868Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class AIMO3Solver:\n\n    def __init__(self, cfg, port: int = 8000):\n        self.cfg = cfg\n        self.port = port\n        self.base_url = f'http://0.0.0.0:{port}/v1'\n        self.api_key = 'sk-local'\n        self.template = AIMO3Template()\n        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n    \n        self._preload_model_weights()\n        \n        self.server_process = self._start_server()\n    \n        self.client = OpenAI(\n            base_url=self.base_url, \n            api_key=self.api_key, \n            timeout=self.cfg.session_timeout\n        )\n    \n        self._wait_for_server()\n        self._initialize_kernels()\n    \n        self.notebook_start_time = time.time()\n        self.problems_remaining = 50\n        self.logger = AIMO3Logger(cfg)\n        self.planner = AIMO3Planner(cfg, self.template, self.client, self.logger)\n    \n    def _preload_model_weights(self) -> None:\n        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n        start_time = time.time()\n        \n        files_to_load = []\n        total_size = 0\n    \n        for root, _, files in os.walk(self.cfg.model_path):\n            for file_name in files:\n                file_path = os.path.join(root, file_name)\n    \n                if os.path.isfile(file_path):\n                    files_to_load.append(file_path)\n                    total_size += os.path.getsize(file_path)\n    \n        def _read_file(path: str) -> None:\n    \n            with open(path, 'rb') as file_object:\n                while file_object.read(1024 * 1024 * 1024):\n                    pass\n    \n        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n            list(executor.map(_read_file, files_to_load))\n    \n        elapsed = time.time() - start_time\n        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n    \n    def _start_server(self) -> subprocess.Popen:\n        cmd = [\n            sys.executable, \n            '-m', \n            'vllm.entrypoints.openai.api_server', \n            '--seed', \n            str(self.cfg.seed), \n            '--model', \n            self.cfg.model_path, \n            '--served-model-name', \n            self.cfg.served_model_name, \n            '--tensor-parallel-size', \n            '1', \n            '--max-num-seqs', \n            str(self.cfg.batch_size), \n            '--gpu-memory-utilization', \n            str(self.cfg.gpu_memory_utilization), \n            '--host', \n            '0.0.0.0', \n            '--port', \n            str(self.port), \n            '--dtype', \n            self.cfg.dtype, \n            '--kv-cache-dtype', \n            self.cfg.kv_cache_dtype, \n            '--max-model-len', \n            str(self.cfg.context_tokens), \n            '--stream-interval', \n            str(self.cfg.stream_interval), \n            '--async-scheduling', \n            '--disable-log-stats', \n            '--enable-prefix-caching'\n        ]\n    \n        self.log_file = open('vllm_server.log', 'w')\n    \n        return subprocess.Popen(\n            cmd, \n            stdout=self.log_file, \n            stderr=subprocess.STDOUT, \n            start_new_session=True\n        )\n    \n    def _wait_for_server(self):\n        print('Waiting for vLLM server...')\n        start_time = time.time()\n    \n        for _ in range(self.cfg.server_timeout):\n            return_code = self.server_process.poll()\n    \n            if return_code is not None:\n                self.log_file.flush()\n    \n                with open('vllm_server.log', 'r') as log_file:\n                    logs = log_file.read()\n    \n                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n    \n            try:\n                self.client.models.list()\n                elapsed = time.time() - start_time\n                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n    \n                return\n    \n            except Exception:\n                time.sleep(1)\n    \n        raise RuntimeError('Server failed to start (timeout).\\n')\n    \n    def _initialize_kernels(self) -> None:\n        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n        start_time = time.time()\n    \n        self.sandbox_pool = queue.Queue()\n    \n        def _create_sandbox():\n            \n            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n    \n        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n    \n            for future in as_completed(futures):\n                self.sandbox_pool.put(future.result())\n    \n        elapsed = time.time() - start_time\n        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n\n    def _scan_for_answer(self, text: str) -> int | None:\n        pattern = r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}'\n        matches = re.findall(pattern, text)\n    \n        if matches:\n            try:\n                clean_value = matches[-1].replace(',', '')\n                value = int(clean_value)\n    \n                if 0 <= value <= 99999:\n                    return value\n    \n            except ValueError:\n                pass\n                \n        pattern = r'final\\s+answer\\s+is\\s*([0-9,]+)'\n        matches = re.findall(pattern, text, re.IGNORECASE)\n    \n        if matches:\n            try:\n                clean_value = matches[-1].replace(',', '')\n                value = int(clean_value)\n    \n                if 0 <= value <= 99999:\n                    return value\n    \n            except ValueError:\n                pass\n    \n        return None\n    \n    def _compute_mean_entropy(self, logprobs_buffer: list) -> float:\n        if not logprobs_buffer:\n            return float('inf')\n    \n        total_entropy = 0.0\n        token_count = 0\n    \n        for top_logprobs_dict in logprobs_buffer:\n            \n            if not isinstance(top_logprobs_dict, dict):\n                continue\n            \n            if not top_logprobs_dict:\n                continue\n            \n            token_entropy = 0.0\n            \n            for token_str, log_prob in top_logprobs_dict.items():\n                prob = math.exp(log_prob)\n                \n                if prob > 0:\n                    token_entropy -= prob * math.log2(prob)\n            \n            total_entropy += token_entropy\n            token_count += 1\n    \n        if token_count == 0:\n            return float('inf')\n    \n        return total_entropy / token_count\n\n    def _fmt_time(self, seconds: float) -> str:\n        s = int(round(max(0.0, seconds)))\n        m, s = divmod(s, 60)\n        return f\"{m}:{s:02d}\"\n\n    def _process_attempt(\n        self, \n        problem: str, \n        system_prompt: str, \n        attempt_index: int, \n        stop_event: threading.Event, \n        deadline: float,\n        problem_id: str,\n        plan: str = \"\",\n        plan_digest: str = \"\",\n        plan_raw: str = \"\",\n        plan_sanitized: str = \"\",\n    ) -> dict:\n        ## DEBUG\n        attempt_log = deque([])\n        attempt_start = time.time()\n\n        if stop_event.is_set() or time.time() > deadline:\n            print(f\"Problem: {problem_id} TIMEOUT!\")\n            return {\n                'Attempt': attempt_index + 1, \n                'Answer': None, \n                'Python Calls': 0, \n                'Python Errors': 0, \n                'Response Length': 0, \n                'Entropy': float('inf'),\n                'Log': \"\\n\".join(attempt_log)\n            }\n    \n        local_tool = None\n        sandbox = None\n        python_calls = 0\n        python_errors = 0\n        total_tokens = 0\n        final_answer = None\n        \n        logprobs_buffer = []\n    \n        attempt_seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n    \n        try:\n            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n            local_tool = AIMO3Tool(\n                local_jupyter_timeout=self.cfg.jupyter_timeout, \n                tool_prompt=self.cfg.tool_prompt, \n                sandbox=sandbox\n            )\n            encoding = self.encoding\n\n            aug = \"\"\n            if plan:\n                aug += f\"\\n\\n=== CURRENT ATTEMPT PLAN ===\\n{plan}\\n\"\n            if aug:\n                aug += \"\\nFollow the plan.\\n\"\n            full_problem = problem + aug\n            if self.cfg.debug and self.logger:\n                attempt_log.append(self.logger.log_planner_block(plan_raw, plan_sanitized, plan_digest))\n                attempt_log.append(\"### Planner Augmentation\\n\")\n                attempt_log.append(f\"{self.logger.format_markdown(aug, mode='text')}\\n\")\n\n            messages = self.template.apply_chat_template(\n                system_prompt,\n                full_problem, # problem,\n                local_tool.tool_config\n            )\n\n            conversation = Conversation.from_messages(messages)\n    \n            for turn_i in range(self.cfg.turns):\n                if stop_event.is_set() or time.time() > deadline:\n                    break\n    \n                prompt_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n    \n                if max_tokens < self.cfg.buffer_tokens:\n                    break\n\n                ## DEBUG\n                if self.cfg.debug and self.cfg.debug_req:\n                    # convert prompt_ids (tensors) back to readable text\n                    # which includes LLM special symbols like <|im_start|>\n                    full_request_text = encoding.decode(prompt_ids)\n                    \n                    snippet = self.logger.get_debug_snippet(full_request_text)\n                    formatted_req = self.logger.format_markdown(snippet)\n                    attempt_log.append(f\"### Turn {turn_i} - Raw Request to Model:\")\n                    attempt_log.append(formatted_req)\n\n                stream = self.client.completions.create(\n                    model=self.cfg.served_model_name, \n                    temperature=self.cfg.temperature, \n                    logprobs=self.cfg.top_logprobs, \n                    max_tokens=max_tokens, \n                    prompt=prompt_ids, \n                    seed=attempt_seed, \n                    stream=True, \n                    extra_body={\n                        'min_p': self.cfg.min_p, \n                        'stop_token_ids': self.stop_token_ids, \n                        'return_token_ids': True\n                    }\n                )\n\n                ## DEBUG\n                full_response_text = \"\"\n\n                try:\n                    token_buffer = []\n                    text_chunks = []\n    \n                    for chunk in stream:\n                        if stop_event.is_set() or time.time() > deadline:\n                            break\n    \n                        new_tokens = chunk.choices[0].token_ids\n                        new_text = chunk.choices[0].text\n    \n                        if new_tokens:\n                            token_buffer.extend(new_tokens)\n                            total_tokens += len(new_tokens)\n                            text_chunks.append(new_text)\n                            ## DEBUG\n                            if self.cfg.debug and self.cfg.debug_resp:\n                                full_response_text += new_text\n\n                            chunk_logprobs = chunk.choices[0].logprobs\n                            \n                            if chunk_logprobs is not None:\n                                if chunk_logprobs.top_logprobs:\n                                    logprobs_buffer.extend(chunk_logprobs.top_logprobs)\n    \n                        if '}' in new_text:\n                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n                            answer = self._scan_for_answer(search_text)\n    \n                            if answer is not None:\n                                final_answer = answer\n                                break\n    \n                finally:\n                    stream.close()\n\n                ## DEBUG\n                if self.cfg.debug and full_response_text:\n                    attempt_log.append(f\"### Turn {turn_i} - Model Response:\")\n                    formatted_resp = self.logger.format_markdown(full_response_text)\n                    attempt_log.append(formatted_resp)\n\n                if final_answer is not None:\n                    break\n    \n                if not token_buffer:\n                    break\n    \n                new_messages = encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n                conversation.messages.extend(new_messages)\n                last_message = new_messages[-1]\n    \n                if last_message.channel == 'final':\n                    answer_text = last_message.content[0].text\n                    final_answer = self._scan_for_answer(answer_text)\n                    break\n    \n                if last_message.recipient == 'python':\n                    python_calls += 1\n                    tool_responses = local_tool.process_sync_plus(last_message)\n                    response_text = tool_responses[0].content[0].text\n\n                    ## DEBUG\n                    if self.cfg.debug:\n                        code_content = last_message.content[0].text\n                        attempt_log.append(f\"### Turn {turn_i} - Python Call:\")\n                        attempt_log.append(f\"```python\\n{code_content}\\n```\\n\")\n\n                        attempt_log.append(f\"### Turn {turn_i} - Python Output:\")\n                        snippet_out = self.logger.get_debug_snippet(response_text)\n                        formatted_out = self.logger.format_markdown(snippet_out, mode=\"text\")\n                        attempt_log.append(f\"{formatted_out}\\n\")\n\n                    if response_text.startswith('[ERROR]') or 'Traceback' in response_text or 'Error:' in response_text:\n                        python_errors += 1\n    \n                    conversation.messages.extend(tool_responses)\n    \n        except Exception as exc:\n            python_errors += 1\n            ## DEBUG\n            if self.cfg.debug:\n                attempt_log.append(f\"\\n**EXCEPTION:** {str(exc)}\\n\")\n\n        finally:\n            if sandbox is not None:\n                sandbox.reset()\n                self.sandbox_pool.put(sandbox)\n    \n        mean_entropy = self._compute_mean_entropy(logprobs_buffer)\n        attempt_elapsed = time.time() - attempt_start\n        attempt_time = self._fmt_time(attempt_elapsed)\n        if self.cfg.debug:\n            attempt_log.appendleft(f\"Attempt spent time: **{attempt_time}**\\n\")\n            attempt_log.appendleft(f\"## Attempt {attempt_index + 1}\\n\")\n    \n        return {\n            'Attempt': attempt_index + 1, \n            'Response Length': total_tokens, \n            'Python Calls': python_calls, \n            'Python Errors': python_errors, \n            'Entropy': mean_entropy, \n            'Answer': final_answer,\n            'Plan': plan,\n            'PlanDigest': plan_digest,\n            'PlanRaw': plan_raw,\n            'PlanSanitized': plan_sanitized,\n            'Log': \"\\n\".join(attempt_log),\n            'Time': attempt_time\n        }\n    \n    def _select_answer(self, detailed_results: list) -> int:\n        answer_weights = defaultdict(float)\n        answer_votes = defaultdict(int)\n\n        for result in detailed_results:\n            answer = result['Answer']\n            entropy = result['Entropy']\n            py_err = int(result.get('Python Errors', 0) or 0)\n            \n            if answer is not None:\n                weight = 1.0 / max(entropy, 1e-9)\n                if self.cfg.enable_error_penalty:\n                    weight *= 1.0 / (1.0 + self.cfg.error_penalty_lambda * py_err)\n                \n                answer_weights[answer] += weight\n                answer_votes[answer] += 1\n\n        scored_answers = []\n\n        for answer, total_weight in answer_weights.items():\n            scored_answers.append({\n                'answer': answer, \n                'votes': answer_votes[answer], \n                'score': total_weight\n            })\n\n        scored_answers.sort(key=lambda x: x['score'], reverse=True)\n\n        vote_data = []\n\n        for item in scored_answers:\n            vote_data.append((\n                item['answer'], \n                item['votes'], \n                item['score']\n            ))\n\n        vote_dataframe = pd.DataFrame(\n            vote_data, \n            columns=['Answer', 'Votes', 'Score']\n        )\n\n        vote_dataframe = vote_dataframe.round({'Score': 3})\n        display(vote_dataframe)\n        \n        if not scored_answers:\n            print('\\nFinal Answer: 0\\n')\n            return vote_dataframe, 0\n\n        final_answer = scored_answers[0]['answer']    \n        print(f'\\nFinal Answer: {final_answer}\\n')\n        return vote_dataframe, final_answer\n\n    def solve_problem(self, problem: str, problem_id: str = \"UNK\") -> int:\n        print(f'\\nProblem: {problem}\\n')\n        problem_start = time.time()\n        \n        user_input = f'{problem} {self.cfg.preference_prompt}'\n    \n        elapsed_global = time.time() - self.notebook_start_time\n        time_left = self.cfg.notebook_limit - elapsed_global\n        problems_left_others = max(0, self.problems_remaining - 1)\n        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n    \n        budget = time_left - reserved_time\n        budget = min(budget, self.cfg.high_problem_timeout)\n        budget = max(budget, self.cfg.base_problem_timeout)\n    \n        deadline = time.time() + budget\n    \n        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n    \n        tasks = []\n    \n        for attempt_index in range(self.cfg.attempts):\n            tasks.append((self.cfg.system_prompt, attempt_index))\n    \n        detailed_results = []\n        valid_answers = []\n        stop_event = threading.Event()\n        \n        if self.cfg.attempts_mode == \"serial\":\n            # ===== SERIAL MODE =====\n\n            # program-side history ONLY for planner (stable, structured)\n            planner_history = []  # list of dicts with Attempt/PlanDigest/Answer/PyCalls/PyErr/Entropy\n\n            for attempt_index in range(self.cfg.attempts):\n                if time.time() > deadline:\n                    break\n\n                plan, plan_digest, plan_raw, plan_san = self.planner.gen_plan(problem_text=problem, history=planner_history, attempt_index=attempt_index)\n\n                result = self._process_attempt(\n                    problem=user_input,\n                    system_prompt=self.cfg.system_prompt,\n                    attempt_index=attempt_index,\n                    stop_event=stop_event,\n                    deadline=deadline,\n                    problem_id=problem_id,\n                    plan=plan,\n                    plan_digest=plan_digest,\n                    plan_raw=plan_raw,\n                    plan_sanitized=plan_san,\n                )\n\n                detailed_results.append(result)\n                if result.get(\"Answer\") is not None:\n                    valid_answers.append(result[\"Answer\"])\n\n                # update planner history (structured)\n                planner_history.append({\n                    \"Attempt\": result.get(\"Attempt\", attempt_index + 1),\n                    \"Answer\": result.get(\"Answer\"),\n                    \"Python Calls\": result.get(\"Python Calls\", 0),\n                    \"Python Errors\": result.get(\"Python Errors\", 0),\n                    \"Entropy\": result.get(\"Entropy\", float(\"inf\")),\n                    \"PlanDigest\": plan_digest,\n                })\n\n                # early stop (same logic you already use)\n                counts = Counter(valid_answers).most_common(1)\n                if counts and counts[0][1] >= self.cfg.early_stop:\n                    break\n                # if attempt_index % 2 == 1: gc.collect()\n        else:\n            # ===== PARALLEL MODE (retained) =====\n            executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n        \n            try:\n                futures = []\n        \n                for (system_prompt, attempt_index) in tasks:\n                    future = executor.submit(\n                        self._process_attempt, \n                        user_input, \n                        system_prompt, \n                        attempt_index, \n                        stop_event, \n                        deadline,\n                        problem_id\n                    )\n        \n                    futures.append(future)\n        \n                for future in as_completed(futures):\n                    try:\n                        result = future.result()\n                        detailed_results.append(result)\n        \n                        if result['Answer'] is not None:\n                            valid_answers.append(result['Answer'])\n        \n                        counts = Counter(valid_answers).most_common(1)\n        \n                        if counts and counts[0][1] >= self.cfg.early_stop:\n                            stop_event.set()\n        \n                            for f in futures:\n                                f.cancel()\n        \n                            break\n        \n                    except Exception as exc:\n                        print(f'Future failed: {exc}')\n                        continue\n        \n            finally:\n                stop_event.set()\n                executor.shutdown(wait=True, cancel_futures=True)\n                \n                self.problems_remaining = max(0, self.problems_remaining - 1)\n\n        if detailed_results:\n            results_dataframe = pd.DataFrame(detailed_results)\n            results_dataframe['Entropy'] = results_dataframe['Entropy'].round(3)\n            results_dataframe['Answer'] = results_dataframe['Answer'].astype('Int64')\n            \n            cols = [c for c in results_dataframe.columns if not c in ['Log', 'Plan', 'PlanRaw', 'PlanSanitized', 'PlanDigest']]\n            display(results_dataframe[cols])\n\n        problem_elapsed = time.time() - problem_start\n        problem_time = self._fmt_time(problem_elapsed)\n        if not valid_answers:\n            print('\\nResult: 0\\n')\n            vote_data, final_answer = pd.DataFrame(columns=['Answer', 'Votes', 'Score']), 0\n        else:\n            vote_data, final_answer = self._select_answer(detailed_results)\n        \n        self.logger.write_debug_logs(detailed_results, vote_data, problem, problem_id, problem_time)\n        return final_answer\n\n    def __del__(self):\n        if hasattr(self, 'server_process'):\n            self.server_process.terminate()\n            self.server_process.wait()\n    \n        if hasattr(self, 'log_file'):\n            self.log_file.close()\n    \n        if hasattr(self, 'sandbox_pool'):\n            while not self.sandbox_pool.empty():\n                try:\n                    sb = self.sandbox_pool.get_nowait()\n                    sb.close()\n    \n                except Exception:\n                    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:25:19.329838Z","iopub.execute_input":"2026-01-17T08:25:19.330056Z","iopub.status.idle":"2026-01-17T08:25:19.363787Z","shell.execute_reply.started":"2026-01-17T08:25:19.330042Z","shell.execute_reply":"2026-01-17T08:25:19.363408Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if \"solver\" in globals(): del solver\nsolver = AIMO3Solver(CFG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:25:19.364728Z","iopub.execute_input":"2026-01-17T08:25:19.364862Z","iopub.status.idle":"2026-01-17T08:28:42.078710Z","shell.execute_reply.started":"2026-01-17T08:25:19.364850Z","shell.execute_reply":"2026-01-17T08:28:42.078247Z"}},"outputs":[{"name":"stdout","text":"Loading model weights from /kaggle/input/gpt-oss-120b/transformers/default/1 into OS Page Cache...\nProcessed 26 files (65.28 GB) in 77.37 seconds.\n\nWaiting for vLLM server...\nServer is ready (took 122.09 seconds).\n\nInitializing 16 persistent Jupyter kernels...\nKernels initialized in 2.75 seconds.\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n    id_value = id_.item(0)\n    question_text = question.item(0)\n    gc.disable()\n    final_answer = solver.solve_problem(question_text, problem_id=str(id_value))\n    gc.enable()\n    gc.collect()\n    return pl.DataFrame({'id': id_value, 'answer': final_answer})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:28:42.079339Z","iopub.execute_input":"2026-01-17T08:28:42.079483Z","iopub.status.idle":"2026-01-17T08:28:42.082804Z","shell.execute_reply.started":"2026-01-17T08:28:42.079468Z","shell.execute_reply":"2026-01-17T08:28:42.082438Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    CFG.debug = False\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        # ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n        # ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv',)\n        # ('/kaggle/input/aimo-p3-hard/test2.csv',)\n        # ('/kaggle/input/aimo-p3-hard/test3.csv',)\n        # ('/kaggle/input/aimo-p3-hard/p5.csv',)\n        # ('/kaggle/input/aimo-p3-hard/p9.csv',)\n        ('/kaggle/input/aimo-p3-hard/p10.csv',)\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T08:28:42.083342Z","iopub.execute_input":"2026-01-17T08:28:42.083477Z","iopub.status.idle":"2026-01-17T08:43:43.661120Z","shell.execute_reply.started":"2026-01-17T08:28:42.083464Z","shell.execute_reply":"2026-01-17T08:43:43.660511Z"}},"outputs":[{"name":"stdout","text":"\nProblem: Let $n \\geq 6$ be a positive integer. We call a positive integer $n$-Norwegian if it has three distinct positive divisors whose sum is equal to $n$. Let $f(n)$ denote the smallest $n$-Norwegian positive integer. Let $M=3^{2025!}$ and for a non-negative integer $c$ define \n\\begin{equation*}\n    g(c)=\\frac{1}{2025!}\\left\\lfloor \\frac{2025! f(M+c)}{M}\\right\\rfloor.\n\\end{equation*}\nWe can write \n\\begin{equation*}\n    g(0)+g(4M)+g(1848374)+g(10162574)+g(265710644)+g(44636594)=\\frac{p}{q}\n\\end{equation*}\nwhere $p$ and $q$ are coprime positive integers. What is the remainder when $p+q$ is divided by $99991$?\n\nBudget: 900.00 seconds | Deadline: 1768639422.50\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer  \\\n0        1            33929            50             10    0.634    <NA>   \n1        2            58922            77              8    0.704    <NA>   \n2        3            41148            50              3    0.700    2404   \n3        4             7745             2              0    0.715    <NA>   \n\n   Time  \n0  3:46  \n1  6:11  \n2  4:18  \n3  0:41  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Attempt</th>\n      <th>Response Length</th>\n      <th>Python Calls</th>\n      <th>Python Errors</th>\n      <th>Entropy</th>\n      <th>Answer</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>33929</td>\n      <td>50</td>\n      <td>10</td>\n      <td>0.634</td>\n      <td>&lt;NA&gt;</td>\n      <td>3:46</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>58922</td>\n      <td>77</td>\n      <td>8</td>\n      <td>0.704</td>\n      <td>&lt;NA&gt;</td>\n      <td>6:11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>41148</td>\n      <td>50</td>\n      <td>3</td>\n      <td>0.700</td>\n      <td>2404</td>\n      <td>4:18</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>7745</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.715</td>\n      <td>&lt;NA&gt;</td>\n      <td>0:41</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   Answer  Votes  Score\n0    2404      1  0.461","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Answer</th>\n      <th>Votes</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2404</td>\n      <td>1</td>\n      <td>0.461</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nFinal Answer: 2404\n\nDebug log written to P10.md\n","output_type":"stream"}],"execution_count":12}]}