by [ChatGPT 5.2 Thinking](https://chatgpt.com/g/g-p-68b60d5555308191a7a4c6bb91e1270a/c/6979c36b-88e0-8323-a970-ddfdcf427d39)

PB 从 41/50 掉到 36/50，即使你“肉眼看代码几乎没变”，在 AIMO3 这种 **随机采样 + 并发推理 + 工具执行闭环** 的系统里，其实挺常见，通常不是“程序坏了”，而是下面这些更隐蔽的因素在动。

---

## 1) 你的系统本质是随机算法：一次提交就是一次“抽样”

你现在用 `temperature=1.0`，而且还做多 attempts / 投票，本质上每次提交得到的是一个随机变量的样本。
**同一套代码，41→36 这种波动完全可能**（尤其当你离满分还差得远时，方差会更大）。

Kaggle 这类讨论里也有人直接说过 public LB 很“luck based / oracle BoN”味道很重（大家在比“抽到最好的一次”）。([kaggle.com][1])
所以你感到“不合理”的点，往往来自把它当成“确定性程序”在看，但实际上它更像“带随机性的搜索器”。

---

## 2) 即便你固定了 `seed`，vLLM **默认也不保证可复现**

这是最容易被忽略、但在你这种“并发 + server mode”里影响最大的点。

vLLM 官方文档明确写了：**默认不保证 reproducibility**；在线（server）模式想要可复现，基本只能靠 **batch invariance**。([docs.vllm.ai][2])
另外，vLLM 也说明：即使做了这些设置，也只保证在**相同硬件、相同 vLLM 版本**上可复现。([docs.vllm.ai][2])

而且 vLLM 还有专门的“Batch invariance”功能，说白了就是让输出不再依赖“当时服务器怎么 batching/怎么排队/同一时刻有谁一起跑”。它的开关是：

```bash
export VLLM_BATCH_INVARIANT=1
```

并且文档也说明它是为了让输出**对 batch size 和请求顺序不敏感**。([docs.vllm.ai][3])

> 你的 Kaggle 提交每次运行时，GPU 上的排队、batch 组合、并发顺序都可能不一样；如果没开 batch invariance，你会看到“同代码同 seed 也漂移”的现象。

---

## 3) “只加了 debug 代码”也可能改变结果：不是语义变了，是**预算/时序/并发**变了

哪怕 debug 逻辑只在 test 下跑，你仍然可能踩到这些坑：

* **时间预算被吃掉**：多了 IO（dump yaml）、多了 import、更多日志，可能导致某些题少跑了 1～2 轮 / 少跑了几个 attempts，然后分数直接掉。
* **并发调度改变**：多几毫秒的差异就会改变 vLLM 的 batching 组成；在非 batch-invariant 模式下，这会反馈到采样结果（输出漂移）。
* **随机种子生成方式被“间接改变”**：比如 attempt_seed 用了全局 RNG、时间戳、或依赖迭代顺序（dict/set 的遍历、任务完成顺序），你“代码逻辑没改”，但执行时序变了，seed 流就不同了。

---

## 4) 依赖/模型版本可能“悄悄变了”

常见于这些情况：

* `pip install` 没 pin 版本（尤其是 `openai_harmony`、vLLM、transformers 等），Kaggle 镜像/缓存更新后，你以为代码一样，实际运行的是不同版本。
* 模型权重如果没固定 revision（HF commit hash），理论上也可能更新（哪怕很少发生）。gpt-oss-120b 的生态近期变化比较快。([Hugging Face][4])

---

## 5) 快速排查清单（建议你直接照做）

**优先级从高到低：**

1. **在 server mode 启用 batch invariance**：`VLLM_BATCH_INVARIANT=1`（你用 H100 的话满足硬件条件）。([docs.vllm.ai][3])
2. **把 attempt_seed 固定成纯函数**（不依赖时间、不依赖执行顺序）

   * 例如：`attempt_seed = hash64(global_seed, problem_id, attempt_idx)`
3. **记录“每题实际跑了多少轮/多少 attempts/是否超时”**到 submission 文件里（或者 stdout），用来确认掉分是不是因为预算少了。
4. **pin 依赖版本 + pin 模型 revision**（至少 vLLM / openai_harmony / transformers / torch）。
5. 如果你用了 joblib cache：先做一次实验把 cache 全关掉跑一遍，确认掉分是否来自缓存/并发 IO。

---

### 你这次最可能的真实原因（按概率排序）

1. **vLLM 的 batching/调度导致输出不稳定**（没开 batch invariance）
2. **随机性本身（temperature=1.0）导致样本波动**
3. **轻微的时间/并发差异 → 少跑几轮 attempts**
4. **依赖版本漂移**

如果你愿意贴两份提交的关键运行日志（每题用了多少轮、多少 attempts 命中、总耗时、是否触发 early stop），我可以帮你判断到底是“输出漂移”还是“预算减少”导致的 41→36。

[1]: https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-3/discussion/662643?utm_source=chatgpt.com "AI Mathematical Olympiad - Progress Prize 3 - Kaggle"
[2]: https://docs.vllm.ai/en/stable/usage/reproducibility/ "Reproducibility - vLLM"
[3]: https://docs.vllm.ai/en/latest/features/batch_invariance/ "Batch Invariance - vLLM"
[4]: https://huggingface.co/openai/gpt-oss-120b?utm_source=chatgpt.com "openai/gpt-oss-120b · Hugging Face"


---

by [Gemini Pro](https://gemini.google.com/app/10afa3f2f57e4370?hl=en-AU)

在 Kaggle 这种涉及 **LLM 推理**和**复杂计算沙箱**的比赛中，代码逻辑“基本一致”但成绩从 41 暴跌到 36，确实让人心碎。除了你已经确认的参数微调外，这种情况通常是由以下几个隐藏的“非确定性变量”导致的：

### 1. 累计耗时导致的“隐形超时” (Time Budget Variance)

这是最常见的原因。Kaggle 的提交有总时间限制（通常是 9 小时跑完 50 道题）。

* **Debug 代码的开销：** 即使你只在 `if test:` 下跑辅助代码，但如果这些代码涉及文件 IO（写 YAML）、复杂的 AST 解析或额外的 `print`，它们会消耗宝贵的 CPU 时间。
* **级联效应：** 比如你在前 40 题多花了 5 分钟写日志，导致最后 10 题（通常是最难的题）因为剩余时间不足，触发了程序的“提前终止”或“减少尝试次数”逻辑。
* **建议：** 检查你的日志，对比两次提交中**每道题的平均 Attempt 次数**。如果后一次提交因为时间紧迫导致高难度题目的尝试次数变少，分数自然会掉。

### 2. 采样随机性与“幸存者偏差”

在  的高随机性下，41 和 36 的差距可能就在**运气**之间。

* **Seed 的效力：** 即使你固定了 `seed`，由于 GPU 的并行计算（特别是 vLLM 这种高度并发的推理引擎）存在非确定性（Non-deterministic algorithms），模型生成的 Token 序列在不同运行环境下可能存在微小差异。
* **数学题的脆弱性：** IMO 级别的题目，推理链条极长。中间一个 Token 的偏移（比如一个正负号或一个变量名）就会导致整个 Python 代码逻辑崩盘。
* **统计学波动：** 50 道题的样本量其实并不大。在 $T=1.0$ 时，单次提交的波动范围可能高达 。你之前的 41 分里可能包含了一部分“运气球”，而 36 分可能才是该策略的真实均值。

### 3. 沙箱环境的“状态污染” (Sandbox Persistence)

如果你在本地 Jupyter Sandbox 中运行，需要极其小心**全局变量**或**文件状态**的残留。

* **问题：** 如果你的 Debug 代码（dump yaml 等）意外地改变了工作目录、修改了某些全局配置，或者在多次尝试之间没有彻底清空 `sys.modules`。
* **后果：** 第 10 题的代码可能会意外调用到第 9 题生成的临时文件或缓存，导致逻辑混乱。
* **@memory.cache 的副作用：** 你提到的缓存装饰器如果清理不彻底，可能会让模型在处理相似但不相同的题目时，错误地复用了旧的执行结果。

### 4. 停止符与截断 (Stop Tokens & Max Tokens)

* 如果你修改了 System Prompt（即使只是一点点），或者增加了辅助代码改变了上下文长度。
* **Context Window 压力：** 辅助代码占用了额外的 Token。如果某道题推理极其复杂，多出来的这几十个 Token 可能正好让模型触及了 `max_tokens` 限制，导致 Python 代码没写完就断掉了。
* **截断错误：** 检查你的 `stop_token_ids`。如果模型因为某些新增的 log 触发了非预期的停止条件，生成会被强行中断。

### 5. vLLM 的 KV Cache 压力

* 在并发模式下，如果 `max_model_len` 或 `gpu_memory_utilization` 设置得比较激进，增加的辅助代码或 YAML 生成会占用更多的 KV Cache。
* 当 Cache 满载时，vLLM 会进行抢占（Preemption），这会导致推理延迟增加，甚至在极端情况下导致请求失败。

---

### 排查建议

为了确认这 5 分是怎么掉的，你可以尝试：

1. **完全回滚：** 哪怕是辅助代码也全部删掉，看能否回到 41 分。如果能，说明是资源消耗或上下文污染。
2. **对比 Failed Cases：** 找出那 5 道原本对现在错的题，看模型生成的 Traceback。是因为生成的代码变烂了？还是代码运行超时了？还是投票没投出来？
3. **计算 Confidence：** 在你的投票系统中记录“胜出票数/总票数”。如果 36 分那次的平均胜出票数明显低于 41 分那次，说明模型的输出质量整体下降了（可能是由于 Prompt 微调）。

**你想让我帮你写一个脚本来对比两次运行生成的 Traceback 日志，找出具体的失败模式吗？**