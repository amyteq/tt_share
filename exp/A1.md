by [ChatGPT 5.2 Thinking](https://chatgpt.com/g/g-p-68b60d5555308191a7a4c6bb91e1270a/c/6979c36b-88e0-8323-a970-ddfdcf427d39)

下面按你的 5 个点逐个答（我会尽量把“确保可落地”的工程视角写清楚）。

---

## 1) `min_p` 和常见的 `top_p` 是同一个吗？区别是什么？

不是同一个。

* **top_p（nucleus）**：从概率最高的 token 开始累加，直到累计概率 ≥ top_p，只在这“累计概率覆盖的集合”里采样。`top_p=1` 等于不截断。 ([docs.vllm.ai][1])
* **min_p**：按“相对阈值”截断：只保留那些满足
  [
  p(token) \ge \text{min_p} \times p(\text{top token})
  ]
  的 token。`min_p=0` 等于不截断。 ([docs.vllm.ai][1])

直觉对比：

* top_p 控制“总概率质量的覆盖范围”（更像“保留多少质量”）。
* min_p 控制“和最优 token 相比差太多的都扔掉”（更像“剔除长尾噪声”）。

所以你现在用 `min_p=0.02` 的含义是：**只要某 token 概率 < 0.02×最高概率，就永远不考虑它**。这通常会比 top_p 更直接地砍掉长尾。

---

## 2) 数学题一般用 `temperature=0.0 & top_p=0.95`，为什么你这里用 `temperature=1.0`？

关键点：你现在的系统是“**多轮工具调用 + 多 attempt 并发 + 投票/打分**”，它天然是“探索型系统”。

* 在 vLLM 里，`temperature=0` 基本就是 **greedy**（确定性选最大概率 token），这会让每次 attempt 的轨迹高度一致，**多并发就变成重复采样**，收益很小。 ([docs.vllm.ai][1])
* `temperature=1.0` 配合 `seed` 和你的并发 attempts，能带来**解题路径多样性**：不同 attempt 可能走不同的思路/不同的计算策略，然后你再用 verifier/投票把正确的留下来。

但也解释了你观察到的现象：你把参数往“更贪心/更保守”方向调（或调得不合适）时，PB 反而下降，因为系统从“探索 + 选择”变成“单一路径 + 更容易卡死/走偏”。

---

## 3) 为什么 AST 改错/加缓存等工程改进，反而可能掉分？

这类现象在 AIMO 这种“LLM 代理 + 工具执行闭环”里非常常见，通常是下面几类原因叠加：

### A. 你“修复”的错误，其实是模型的有效反馈信号

很多时候，模型生成的代码出错（slice 用错、print 缺失、参数不合法）会触发**明确的异常**，异常会成为下一轮 prompt 的强信号，迫使模型纠错。
你用 AST 把它“兜底修好”，可能让代码**悄悄跑出一个错误结果**（没有异常），模型就把错结果当真继续推，最后输出错误答案——这在 leaderboard 上比“报错后重试”更伤。

### B. AST 改写可能改变语义（尤其是你提到的 slice 规则）

比如你把 `d[:n]` 统统修成别的写法，如果模型本意其实不是“dict slice”，而是 `list/dict-like` 的特定类型，或者它就是想让它报错以提示自己（有些模型会这样“自导自演”），你改写后会产生不可预期的偏差。

### C. 缓存容易引入“错误复用”

* joblib 的 cache key 由参数决定；数学题里常见的函数参数包含 `sympy` 对象、lambda、闭包、随机态、包含全局状态的对象等，**要么不可序列化，要么序列化后语义不稳定**。vLLM/Joblib 对这类场景并不“保证安全”。 ([docs.vllm.ai][1])
* 即便你做了 safe_cache gate（你前面提到的），也会出现另一类问题：某些对象“能 pickle”，但它的等价性不等于数学等价性，导致错缓存命中。

### D. 时间预算/Token 预算被你“工程层”吃掉了

AST parse/transform、额外打印、缓存落盘 IO，都可能在 Kaggle 的时间/资源限制下挤占模型的思考轮数，造成总体成功率下降（尤其是 50 道题要平均分配预算的设定）。

---

## 4) 并发多 attempts + score/votes：确实是很强的工程方向

你现在这套更接近“self-consistency + reranking”的范式，本质上是用算力换鲁棒性，通常在这种比赛里是最稳定的提升手段之一。

不过注意一个细节：**要让并发真正“多样”，光并发不够**，需要让每个 attempt 在“思路/工具/搜索空间”上有差异（否则就是重复）。

---

## 5) 除了分阶段（plan + solve）提示词，还有什么改进方法？

给你一些在这类“数学 + 工具执行”闭环里更常见、也更可能带来提升的改法（不依赖大改框架）：

### A) 做“强 verifier”，把投票从“答案一致”升级成“可验证”

不要只 vote 最多的答案，尽量让每个 attempt 输出**可检验的证据**，然后你用程序验：

* 模运算/同余条件检查
* 代回原题验证
* 构造性题：验证构造对象满足性质
* 计数题：用小规模 brute force 验证 pattern，再外推（外推要谨慎，但能筛掉大量错解）

这会显著降低“错误答案一致性投票”的风险。

### B) 多样化 attempt 的“策略分配”（而不是同策略多次）

例如把 attempts 分成不同“persona/策略”：

1. **纯符号推导**（sympy 优先）
2. **枚举/DP/搜索**（先小规模，找规律）
3. **数论专用套路**（CRT/ord/群论）
4. **几何/组合结构化**（先抽象成图/不变量）

每组策略用不同 system side “tool preference” 或不同 stop/长度预算，能比单纯换 seed 更有效。

### C) 让模型输出“中间状态”更结构化（减少跑偏）

要求模型每轮必须给：

* 当前已知量/目标量
* 计划中的下一步（必须是可执行的 Python 操作或可验证的数学断言）
* 预计输出格式（比如 `ANSWER = ...`）

结构化不是为了好看，是为了让你在程序端做更强的 parse + verifier。

### D) 把“代码修复”从 AST 强改，换成“运行时策略”

相比 AST “硬改代码”，更稳的方式是：

* 捕获异常 → 把异常摘要喂回模型（并附带“最小修复建议”）
* 对常见错误做**提示级修复**（prompt patch），而不是语义级改写
  因为语义级改写最容易造成“悄悄错”。

### E) 用“短输出 + 多轮”而不是“一次长输出”

IMO 级题很容易一口气写长推导后在某个环节错掉。你现在是多轮 stream，本来就适合：

* 每轮限制 max_tokens 更小
* 每轮强制只做一个可验证步骤
* 验证通过再继续
  这会提升整体稳定性（代价是轮数增多，但通常比一次跑偏强）。

---

## 参数上给你一个实用的建议（不需要大改）

既然你看到 `temperature=1.0, min_p=0.02` 比附近一些点更好，那可以把“调参目标”明确成：

1. **保证多样性**：temperature 不要太低（否则并发无效）
2. **控制长尾噪声**：min_p / top_p 负责截断
3. **让每个 attempt 有差异**：seed + 策略分组 + 预算差异

在 vLLM 语义里，min_p 就是“相对 top-token 的概率阈值” ([docs.vllm.ai][1])，所以你可以优先做一个小网格：

* `temperature ∈ {0.7, 1.0, 1.2}`
* `min_p ∈ {0.01, 0.02, 0.05}`（vLLM issue 里有人经验说 0.05 常见，但要看模型/任务） ([GitHub][2])
  并且**每个点都配合你的投票/verifier**评估，不要只看单次正确率。

---

如果你愿意贴一下你当前的 **score/votes** 规则（怎么给一个 attempt 打分、怎么判“可验证/不可验证”、怎么选最终答案），我可以更具体地建议：哪些 verifier 最值得加、以及如何设计“策略分组 attempts”，让并发带来的多样性真的转化为分数。

[1]: https://docs.vllm.ai/en/v0.6.0/dev/sampling_params.html?utm_source=chatgpt.com "Sampling Parameters — vLLM"
[2]: https://github.com/vllm-project/vllm/issues/2287?utm_source=chatgpt.com "OpenAI server with min_p · Issue #2287 · vllm-project/vllm - GitHub"

---

by [Gemini Pro](https://gemini.google.com/app/10afa3f2f57e4370?hl=en-AU)

这是一个非常硬核且高水平的 Kaggle AIMO 竞赛场景。既然你已经拿到了 41/50 的高分，说明基础架构（vLLM + Sandbox + Loop）已经非常扎实。

针对你提出的关于采样参数、工程优化失败原因以及后续改进方向的问题，我结合目前 LLM 解决数学竞赛（Math Reasoning）的前沿研究和实战经验来解答。

### 1. `min_p` 和 `top_p` 是同一个参数吗？有何区别？

**它们完全不同。**

* **`top_p` (Nucleus Sampling):**
* **原理:** 它截取概率分布中**累积概率**达到  (例如 0.95) 的前  个 token。
* **问题:** 它是“静态”的累积。如果模型对下一个词非常不确定（分布扁平），`top_p` 会保留大量低质量的 token；如果模型非常确定（分布尖锐），`top_p` 也可能切掉稍微次优但合理的 token。


* **`min_p`:**
* **原理:** 这是一个相对阈值。只有当某个 token 的概率  时，它才会被保留。
* **优势:** 它是**动态**的。
* 如果模型非常确信（首选 token 概率 0.9），`min_p=0.02` 意味着只有概率大于  的 token 才会入选。这会极大地砍掉长尾垃圾。
* 如果模型很不确信（首选 token 概率 0.1），阈值变为 ，此时它会保留更多可能性的词，允许模型“探索”。




* **结论:** 在数学推理任务中，`min_p` 通常比 `top_p` 表现更好，因为它在保证逻辑连贯性（高置信度时）和允许思维跳跃（低置信度时）之间取得了更好的平衡。

### 2. 一般用 `temperature=0.0`，为何这里用 `temperature=1.0`？

这取决于你的**推理策略（Inference Strategy）**。

* **Greedy Decode (Temp=0):** 适用于你只做**一次**尝试（Pass@1）。你希望模型输出它认为概率最高的那条路径。
* **Majority Voting / Self-Consistency (Temp > 0):** 你现在的策略是“并发多次 attempts, 收集结果 score”。
* 如果 `Temp=0`，你并发跑 100 次，得到的 100 个代码和答案几乎是一模一样的。如果模型错了，你就 100% 错了，无法通过投票修正。
* 设置 `Temp=1.0` (配合 `min_p` 压制幻觉) 是为了**引入多样性 (Diversity)**。IMO 题目很难，模型大概率第一反应是错的，或者并在不同路径上游走。高温度允许模型探索不同的解题路径（不同的算法、不同的库、不同的切入点）。
* **核心逻辑:** 在 Self-Consistency (SC) 策略下，**Diversity is King**。只要你的 `min_p` 设置得当防止完全胡言乱语，高温度能让你覆盖到包含正确答案的解空间。



### 3. 为什么 AST 分析改错、加 print 等“工程改进”反而掉分？

这是一个非常典型的“过度工程化”陷阱，原因可能如下：

* **Logic vs. Syntax (因果倒置):**
* 当模型生成的代码出现 Syntax Error 或者 Slice 越界时，通常**不仅仅是代码写错了，而是前面的思维链（CoT）逻辑已经崩了**。
* 强行修正代码（例如自动补全 print，自动修正 slice），是在试图让一段“逻辑错误”的代码强行跑通。跑通后生成的错误答案（False Positive）会混入你的投票池，干扰 Majority Voting 的准确性。
* **不如报错:** 让它 Crash，不仅节省了沙箱运行时间，而且在投票系统中，一个 Error (无答案) 通常比一个 Wrong Answer (错误答案) 对最终决策的危害要小。


* **Token 分布偏移 (Out-of-Distribution):**
* 开源模型（如 Qwen/DeepSeek/Llama 系列）主要是在干净的代码库上训练的。它们习惯自己写 print，或者在特定 context 下输出。
* 如果你人为地在 Prompt 后面追加 `print(answer)` 或者强行修改它的代码结构再塞回 context，这破坏了模型原本预测的 token 序列流畅度。模型可能会对这种“被篡改”的上下文感到困惑，导致下一轮生成的质量下降。


* **Context 污染:** 自动纠错产生的冗余信息会占用 Context Window，且可能包含误导性信息。

### 4. 还有什么改进方法？(除了 Plan + Solve 和 投票)

既然你已经在用 Python Sandbox 和 Voting，下一步提升通常来自更高级的搜索策略和反馈机制：

#### A. 引入“代码执行反馈” (Reflexion / Iterative Refinement)

目前你似乎是追加结果到 user message。如果代码报错（Runtime Error），**千万不要自己在 Python 侧修**，而是把 Traceback 原封不动地喂回给模型。

* **Prompt:** "The code failed with the following error: {traceback}. Please fix the code and try again."
* LLM 极其擅长根据 Error Message 修代码。这比 AST 静态修复有效得多。

#### B. 蒙特卡洛树搜索 (MCTS) 或 Best-of-N (Verifier)

目前的 Voting 是“平权的”，但这不一定最优。

* **Verifier (判别器):** 训练一个小的 Reward Model，或者让 LLM 自己充当 Verifier（"Review the above solution..."），给生成的答案打分。在投票时，使用加权投票（Weighted Voting）。
* **Tree Search:** 不要只在最后看答案。在生成的每一步（例如 Plan 阶段，或者每生成一段代码后），让模型评估当前的路径是否有希望。如果没希望，尽早剪枝（Pruning），回退重试。这比单纯的一条路跑到黑更有效，但工程量很大。

#### C. 动态 Few-Shot (RAG)

* IMO 题目涵盖代数、几何、数论、组合。
* 构建一个高质量的题库（可以用 AIME 或往年 AMC 题）。
* 根据当前的 User Problem，检索 3-5 个**最相似**的题目及其优秀的 Python 解题代码作为 Context 中的 Examples。
* **针对性:** 几何题给几何的库用法（如 `sympy.geometry`），数论题给数论的技巧。

#### D. 答案过滤器 (Sanity Check)

这是一个低成本高收益的工程 Trick。

* IMO 问题的答案通常是 0-999 (或者是特定的整数格式)。
* **过滤:** 如果模型输出负数、小数、无理数或者格式极其离谱的答案，直接在投票阶段丢弃，或者让模型重做。不要让显然错误的答案稀释了正确答案的票数。

#### E. 资源管理 (Time Management)

* **Early Stopping:** 如果你有 50 次尝试机会，前 10 次跑完如果有 8 次都是同一个答案，就直接提交，不要跑满 50 次。省下的时间（GPU算力）留给那些方差大、这就做不出来的难题。

### 总结建议

1. **参数:** 保持 `Temperature=1.0` (甚至可以尝试 0.7-1.2 的动态范围)，`min_p` 优于 `top_p`，可以微调 `min_p` (0.05 ~ 0.1) 看看效果。
2. **回滚:** 去掉 AST 自动纠错和强制 Print。代码错就让它错，把 Error 喂回给模型让它自己修（Self-Correction）。
3. **下一步重心:** 实现 **Runtime Error Feedback loop**（让模型修bug）和 **Answer Sanity Check**（过滤非法答案）。如果算力允许，尝试基于检索的 **Dynamic Few-Shot**。