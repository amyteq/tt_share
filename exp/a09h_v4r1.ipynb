{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":14632585,"sourceType":"datasetVersion","datasetId":9245165},{"sourceId":289055161,"sourceType":"kernelVersion"},{"sourceId":510391,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":404485,"modelId":422384}],"dockerImageVersionId":31260,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":452.400962,"end_time":"2026-01-12T17:31:26.837673","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-12T17:23:54.436711","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2026-01-29T11:39:24.361844Z","iopub.execute_input":"2026-01-29T11:39:24.362058Z","iopub.status.idle":"2026-01-29T11:40:35.987688Z","shell.execute_reply.started":"2026-01-29T11:39:24.362043Z","shell.execute_reply":"2026-01-29T11:40:35.987223Z"},"papermill":{"duration":23.948239,"end_time":"2026-01-12T17:24:20.350211","exception":false,"start_time":"2026-01-12T17:23:56.401972","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Found existing installation: keras 3.10.0\nUninstalling keras-3.10.0:\n  Successfully uninstalled keras-3.10.0\nFound existing installation: matplotlib 3.10.0\nUninstalling matplotlib-3.10.0:\n  Successfully uninstalled matplotlib-3.10.0\nFound existing installation: scikit-learn 1.6.1\nUninstalling scikit-learn-1.6.1:\n  Successfully uninstalled scikit-learn-1.6.1\nFound existing installation: tensorflow 2.19.0\nUninstalling tensorflow-2.19.0:\n  Successfully uninstalled tensorflow-2.19.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\nimport subprocess\n\nimport warnings\nwarnings.simplefilter('ignore')\n\ndef set_env(input_archive, temp_dir):\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir, exist_ok=True)\n        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n    subprocess.run([\n        sys.executable, \n        '-m', \n        'pip', \n        'install', \n        '--no-index', \n        '--find-links', \n        f'{temp_dir}/wheels', \n        'unsloth', \n        'trl', \n        'vllm', \n        'openai_harmony',\n    ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n\nset_env(\n    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz', \n    temp_dir='/kaggle/tmp/setup'\n)\n\nsubprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:40:35.988621Z","iopub.execute_input":"2026-01-29T11:40:35.988788Z","iopub.status.idle":"2026-01-29T11:44:12.673230Z","shell.execute_reply.started":"2026-01-29T11:40:35.988771Z","shell.execute_reply":"2026-01-29T11:44:12.672845Z"},"papermill":{"duration":0.006461,"end_time":"2026-01-12T17:24:20.359095","exception":false,"start_time":"2026-01-12T17:24:20.352634","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"cl100k_base.tiktoken\no200k_base.tiktoken\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['ls', '/kaggle/tmp/setup/tiktoken_encodings'], returncode=0)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"os.environ['TRANSFORMERS_NO_TF'] = '1'\nos.environ['TRANSFORMERS_NO_FLAX'] = '1'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\nos.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'\n\nimport gc\nimport re\nimport math\nimport time\nimport queue\nimport threading\nimport contextlib\nfrom collections import deque\nfrom typing import Iterable, Optional\nfrom jupyter_client import KernelManager\nfrom collections import Counter, defaultdict\nfrom concurrent.futures import as_completed, ThreadPoolExecutor\n\nimport pandas as pd\nimport polars as pl\n\nfrom openai import OpenAI\n\nfrom openai_harmony import (\n    HarmonyEncodingName, \n    load_harmony_encoding, \n    SystemContent, \n    ReasoningEffort, \n    ToolNamespaceConfig, \n    Author, \n    Message, \n    Role, \n    TextContent, \n    Conversation\n)\n\nfrom transformers import set_seed\nimport kaggle_evaluation.aimo_3_inference_server\n\nclass CFG:\n    \n    # prompts sometimes seems useless, such as to restrict d[:n], keep it simple?\n    # use AST instead to validate/tweak generated codes?\n    system_prompt = (\n        'You are an elite mathematical problem solver with expertise at the International '\n        'Mathematical Olympiad (IMO) level. Your goal is to find the correct answer through '\n        'rigorous mathematical reasoning.\\n\\n'\n        \n        '# Problem-Solving Approach:\\n'\n        '1. UNDERSTAND: Carefully read and rephrase the problem in your own words. '\n        'Identify what is given, what needs to be found, and any constraints.\\n'\n        '2. EXPLORE: Consider multiple solution strategies. Think about relevant theorems, '\n        'techniques, patterns, or analogous problems. Don\\'t commit to one approach immediately.\\n'\n        '3. PLAN: Select the most promising approach and outline key steps before executing.\\n'\n        '4. EXECUTE: Work through your solution methodically. Show all reasoning steps clearly.\\n'\n        '5. VERIFY: Check your answer by substituting back, testing edge cases, or using '\n        'alternative methods. Ensure logical consistency throughout.\\n\\n'\n        \n        '# Mathematical Reasoning Principles:\\n'\n        '- Break complex problems into smaller, manageable sub-problems\\n'\n        '- Look for patterns, symmetries, and special cases that provide insight\\n'\n        '- Use concrete examples to build intuition before generalizing\\n'\n        '- Consider extreme cases and boundary conditions\\n'\n        '- If stuck, try working backwards from the desired result\\n'\n        '- Be willing to restart with a different approach if needed\\n\\n'\n        \n        '# Verification Requirements:\\n'\n        '- Cross-check arithmetic and algebraic manipulations\\n'\n        '- Verify that your solution satisfies all problem constraints\\n'\n        '- Test your answer with simple cases or special values when possible\\n'\n        '- Ensure dimensional consistency and reasonableness of the result\\n\\n'\n        \n        '# Output Format:\\n'\n        'The final answer must be a non-negative integer between 0 and 99999.\\n'\n        'Place your final numerical answer inside \\\\boxed{}, e.g., \\\\boxed{42}\\n\\n'\n        \n        'Think step-by-step and show your complete reasoning process. Quality of reasoning '\n        'is as important as the final answer.'\n    )\n    \n    tool_prompt = (\n        'Use this tool to execute Python code for:\\n'\n        '- Complex calculations that would be error-prone by hand\\n'\n        '- Numerical verification of analytical results\\n'\n        '- Generating examples or testing conjectures\\n'\n        '- Visualizing problem structure when helpful\\n'\n        '- Brute-force verification for small cases\\n\\n'\n        \n        'The environment is a stateful Jupyter notebook. Code persists between executions.\\n'\n        'Always use print() to display results. Write clear, well-commented code.\\n\\n'\n        \n        'Remember: Code should support your mathematical reasoning, not replace it. '\n        'Explain what you\\'re computing and why before running code.'\n    )\n    \n    preference_prompt = (\n        'You have access to `math`, `numpy`, and `sympy` for:\\n\\n'\n        \n        '# Symbolic Computation (sympy):\\n'\n        '- Algebraic manipulation and simplification\\n'\n        '- Solving equations and systems of equations\\n'\n        '- Symbolic differentiation and integration\\n'\n        '- Number theory functions (primes, divisors, modular arithmetic)\\n'\n        '- Polynomial operations and factorization\\n'\n        '- Working with mathematical expressions symbolically\\n\\n'\n        \n        '# Numerical Computation (numpy):\\n'\n        '- Array operations and linear algebra\\n'\n        '- Efficient numerical calculations for large datasets\\n'\n        '- Matrix operations and eigenvalue problems\\n'\n        '- Statistical computations\\n\\n'\n        \n        '# Mathematical Functions (math):\\n'\n        '- Standard mathematical functions (trig, log, exp)\\n'\n        '- Constants like pi and e\\n'\n        '- Basic operations for single values\\n\\n'\n        \n        'Best Practices:\\n'\n        '- Use sympy for exact symbolic answers when possible\\n'\n        '- Use numpy for numerical verification and large-scale computation\\n'\n        '- Combine symbolic and numerical approaches: derive symbolically, verify numerically\\n'\n        '- Document your computational strategy clearly\\n'\n        '- Validate computational results against known cases or theoretical bounds'\n    )\n    \n    served_model_name = 'gpt-oss'\n    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n    \n    kv_cache_dtype = 'fp8_e4m3'\n    dtype = 'auto'\n\n    high_problem_timeout = 900\n    base_problem_timeout = 300\n\n    notebook_limit = 17400\n    server_timeout = 180\n\n    session_timeout = 960\n    jupyter_timeout = 6\n    sandbox_timeout = 3\n\n    stream_interval = 200\n    context_tokens = 65536\n    buffer_tokens = 512\n    search_tokens = 32\n    top_logprobs = 5\n    batch_size = 256\n    early_stop = 4\n    attempts = 8\n    workers = 16\n    turns = 128\n    seed = 42\n\n    gpu_memory_utilization = 0.96\n    # TODO: why this combination? not temperature <= 0.1 + min_p = 0.95? to EXP\n    temperature = 1.0\n    min_p = 0.02\n\n    # AST validate & fix\n    ast_fix_slice = True\n    ast_add_cache = False\n    ast_fix_print = True\n\n    # debug\n    debug = True\n    debug_req = True\n    debug_req_full = 20\n    debug_resp = True\n    debug_limit = 3000\n    debug_cols = ['Log', 'Plan', 'PlanRaw', 'PlanSanitized', 'PlanDigest']\n\nset_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:44:12.673868Z","iopub.execute_input":"2026-01-29T11:44:12.674017Z","iopub.status.idle":"2026-01-29T11:44:20.352719Z","shell.execute_reply.started":"2026-01-29T11:44:12.674004Z","shell.execute_reply":"2026-01-29T11:44:20.352307Z"},"papermill":{"duration":0.008442,"end_time":"2026-01-12T17:27:09.697869","exception":false,"start_time":"2026-01-12T17:27:09.689427","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import ast\n\ndef _ann_contains_callable(ann: ast.AST) -> bool:\n    # covers: Callable, typing.Callable, collections.abc.Callable, etc.\n    if ann is None:\n        return False\n    # Name(\"Callable\")\n    if isinstance(ann, ast.Name) and ann.id.lower() == \"callable\":\n        return True\n    # Attribute(..., attr=\"Callable\")\n    if isinstance(ann, ast.Attribute) and ann.attr.lower() == \"callable\":\n        return True\n    # Subscript(Callable[[...], ...])\n    if isinstance(ann, ast.Subscript):\n        return _ann_contains_callable(ann.value) or _ann_contains_callable(ann.slice)\n    # Union / | : just scan children\n    for child in ast.iter_child_nodes(ann):\n        if _ann_contains_callable(child):\n            return True\n    return False\n\ndef _has_uncacheable_params(fn: ast.FunctionDef) -> bool:\n    bad_name_tokens = (\"func\", \"callable\", \"callback\", \"fn\", \"lambda\")\n    args = fn.args\n\n    # varargs/kwargs => unstable signature; skip\n    if args.vararg is not None or args.kwarg is not None:\n        return True\n\n    # positional + kwonly args\n    all_args = list(args.posonlyargs) + list(args.args) + list(args.kwonlyargs)\n\n    for a in all_args:\n        # name heuristic\n        if any(tok in a.arg.lower() for tok in bad_name_tokens):\n            return True\n        # annotation heuristic\n        if _ann_contains_callable(a.annotation):\n            return True\n\n    return False\n\nclass _Rewriter(ast.NodeTransformer):\n    def __init__(self, cfg):\n        self.cfg = cfg\n\n    def visit_FunctionDef(self, node: ast.FunctionDef):\n        # recurse first\n        self.generic_visit(node)\n\n        if not self.cfg.ast_add_cache or _has_uncacheable_params(node):\n            return node\n\n        # if already has @memory.cache, do nothing\n        for dec in node.decorator_list:\n            if (isinstance(dec, ast.Attribute)\n                and isinstance(dec.value, ast.Name)\n                and dec.value.id == \"memory\"\n                and dec.attr == \"cache\"):\n                return node\n\n        # prepend @memory.cache\n        node.decorator_list.insert(\n            0,\n            ast.Attribute(value=ast.Name(id=\"memory\", ctx=ast.Load()), attr=\"cache\", ctx=ast.Load())\n        )\n        return node\n\n    def visit_Subscript(self, node: ast.Subscript):\n        # rewrite inside first\n        self.generic_visit(node)\n\n        if not self.cfg.ast_fix_slice:\n            return node\n\n        s = node.slice\n        if isinstance(s, ast.Slice) and s.upper is not None:\n            # x[:n] / x[a:n] => head(x, n)\n            return ast.copy_location(\n                ast.Call(\n                    func=ast.Name(id=\"head\", ctx=ast.Load()),\n                    args=[node.value, s.upper],\n                    keywords=[]\n                ),\n                node,\n            )\n        return node\n\ndef _to_load(t: ast.AST) -> ast.AST:\n    # minimal conversion for assignment targets\n    if isinstance(t, ast.Name):\n        return ast.Name(id=t.id, ctx=ast.Load())\n    if isinstance(t, ast.Tuple):\n        return ast.Tuple(elts=[_to_load(e) for e in t.elts], ctx=ast.Load())\n    if isinstance(t, ast.List):\n        return ast.List(elts=[_to_load(e) for e in t.elts], ctx=ast.Load())\n    if isinstance(t, ast.Attribute):\n        return ast.Attribute(value=_to_load(t.value), attr=t.attr, ctx=ast.Load())\n    if isinstance(t, ast.Subscript):\n        return ast.Subscript(value=_to_load(t.value), slice=t.slice, ctx=ast.Load())\n    return t  # fallback\n\ndef _rewrite_code(code: str, cfg = CFG) -> str:\n    tree = ast.parse(code)\n    if not tree.body:\n        return code\n\n    # 1) rewrite function decorators + slices\n    tree = _Rewriter(cfg).visit(tree)\n    ast.fix_missing_locations(tree)\n\n    if not cfg.ast_fix_print:\n        return ast.unparse(tree)\n\n    # 2) auto print last line if it is assignment\n    last = tree.body[-1]\n    # a = 1 => print(a)\n    if isinstance(last, ast.Assign):\n        tree.body[-1] = ast.Expr(\n            value=ast.Call(\n                func=ast.Name(id=\"print\", ctx=ast.Load()),\n                args=[_to_load(t) for t in last.targets],\n                keywords=[]\n            )\n        )\n    # 2+3 => print(2+3)\n    if isinstance(last, ast.Expr):\n        # Avoid double-print if it's already print(...)\n        v = last.value\n        if not (isinstance(v, ast.Call) and isinstance(v.func, ast.Name) and v.func.id == \"print\"):\n            tree.body.append(ast.Expr(\n                value=ast.Call(\n                    func=ast.Name(id=\"print\", ctx=ast.Load()),\n                    args=[v],\n                    keywords=[]\n                )\n            ))\n\n    ast.fix_missing_locations(tree)\n    return ast.unparse(tree)\n\n### util methods\ndef _fmt_time(seconds: float) -> str:\n    s = int(round(max(0.0, seconds)))\n    m, s = divmod(s, 60)\n    return f\"{m}:{s:02d}\"\n\ndef _format_markdown(text: str, mode: str = \"quote\") -> str:\n    if not text:\n        return \"\"\n    lines = text.split('\\n')\n    escaped_lines = [f\"\\\\{line}\" if line.startswith('#') else line for line in lines]\n    processed_text = '\\n'.join(escaped_lines)\n    if mode in [\"markdown\", \"text\", \"python\"]:\n        return f\"```{mode}\\n{processed_text}\\n```\\n\"\n    if mode == \"quote\":\n        return '\\n'.join([f\"> {line}\" for line in escaped_lines]) + \"\\n\"\n    if mode == \"\":\n        return processed_text + \"\\n\"\n    return f\"```\\n{processed_text}\\n```\\n\"\n\ndef _delete(name: str):\n    if name is not None and name != \"\" and name in globals(): \n        del globals()[name]","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:44:20.353615Z","iopub.execute_input":"2026-01-29T11:44:20.353854Z","iopub.status.idle":"2026-01-29T11:44:20.366039Z","shell.execute_reply.started":"2026-01-29T11:44:20.353840Z","shell.execute_reply":"2026-01-29T11:44:20.365701Z"},"papermill":{"duration":0.009149,"end_time":"2026-01-12T17:27:14.758651","exception":false,"start_time":"2026-01-12T17:27:14.749502","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class AIMO3Logger:\n    def __init__(self, cfg):\n        self.cfg = cfg\n\n    def get_debug_snippet(self, text: str) -> str:\n        limit = self.cfg.debug_limit\n        if not text or len(text) <= limit:\n            return text or \"\"\n        head = text[:100]\n        tail_len = limit - 100\n        tail = text[-tail_len:]\n        return f\"{head}\\n ... \\n{tail}\"\n\n    def log_planner_block(self, plan_raw: str, plan_sanitized: str, plan_digest: str) -> str:\n        raw_snip = self.get_debug_snippet(plan_raw)\n        san_snip = self.get_debug_snippet(plan_sanitized)\n        digest = plan_digest.strip()\n\n        out = []\n        out.append(\"### Planner Output (raw)\\n\")\n        out.append(_format_markdown(raw_snip, mode='text'))\n        out.append(\"### Planner Output (sanitized)\\n\")\n        out.append(_format_markdown(san_snip, mode='text'))\n        out.append(\"### Plan Digest\\n\")\n        out.append(_format_markdown(digest, mode='text'))\n        return \"\".join(out)\n\n    def write_debug_logs(self, detailed_results, vote_dataframe, problem, problem_id=\"UNK\", problem_time=\"\"):\n        if not self.cfg.debug:\n            return\n        try:\n            summary_lines = [\"\\n## Summary Stats\\n\"]\n            if detailed_results:\n                df = pd.DataFrame(detailed_results)\n                cols = [c for c in df.columns if c not in self.cfg.debug_cols]\n                summary_lines.append(df[cols].to_markdown(index=False))\n                summary_lines.append(\"\\n\\n\")\n\n            if not vote_dataframe.empty:\n                summary_lines.append(\"## Vote Counts\\n\")\n                summary_lines.append(vote_dataframe.to_markdown(index=False))\n                summary_lines.append(\"\\n\")\n\n            final_log_content = [f\"# Problem ID: {problem_id}\\n\"]\n            final_log_content.append(f\"Problem spent time: **{problem_time}**\\n\\n\")\n            final_log_content.append(f\"**Problem:**\\n{_format_markdown(problem)}\\n\")\n            final_log_content.append(f\"**system_prompt:**\\n{_format_markdown(self.cfg.system_prompt)}\\n\")\n            final_log_content.append(f\"**tool_prompt:**\\n{_format_markdown(self.cfg.tool_prompt)}\\n\")\n            final_log_content.append(f\"**preference_prompt:**\\n{_format_markdown(self.cfg.preference_prompt)}\\n\")\n            final_log_content.append(f\"**CFG** > temperature: **{self.cfg.temperature}**, \"\n                                     f\"min_p: **{self.cfg.min_p}**, \"\n                                     f\"served_model_name: **{self.cfg.served_model_name}**\\n\")\n            final_log_content.extend(summary_lines)\n            final_log_content.append(\"\\n===\\n\")\n\n            sorted_results = sorted(detailed_results, key=lambda x: x['Attempt'])\n            for res in sorted_results:\n                log_content = res.get('Log', '')\n                if log_content:\n                    final_log_content.append(log_content)\n                    final_log_content.append(\"\\n===\\n\")\n\n            output_path = f\"{problem_id}.md\"\n            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\".join(final_log_content))\n            print(f\"Debug log written to {output_path}\")\n        except Exception as e:\n            print(f\"Failed to write debug log: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T11:44:20.366533Z","iopub.execute_input":"2026-01-29T11:44:20.366667Z","iopub.status.idle":"2026-01-29T11:44:20.387397Z","shell.execute_reply.started":"2026-01-29T11:44:20.366645Z","shell.execute_reply":"2026-01-29T11:44:20.387061Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class AIMO3Sandbox:\n\n    _port_lock = threading.Lock()\n    _next_port = 50000\n\n    @classmethod\n    def _get_next_ports(cls, count: int = 5) -> list[int]:\n        with cls._port_lock:\n            ports = list(range(cls._next_port, cls._next_port + count))\n            cls._next_port += count\n            return ports\n\n    def __init__(self, timeout: float):\n\n        self._default_timeout = timeout\n        self._owns_kernel = False\n        self._client = None\n        self._km = None\n        \n        ports = self._get_next_ports(5)\n\n        env = os.environ.copy()\n        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n        env['JUPYTER_PLATFORM_DIRS'] = '1'\n        env['PYTHONWARNINGS'] = 'ignore'\n        env['MPLBACKEND'] = 'Agg'\n\n        self._km = KernelManager()\n        self._km.shell_port = ports[0]\n        self._km.iopub_port = ports[1]\n        self._km.stdin_port = ports[2]\n        self._km.hb_port = ports[3]\n        self._km.control_port = ports[4]\n\n        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n\n        self._client = self._km.blocking_client()\n        self._client.start_channels()\n        self._client.wait_for_ready(timeout=self._default_timeout)\n        self._owns_kernel = True\n\n        self._helper_imports = \"\"\"\nimport math\nimport numpy\nimport sympy\nimport sympy as sp\nimport itertools\nimport collections\nimport mpmath\nimport sys\nimport re\n\nfrom sympy.ntheory.modular import crt\nfrom sympy import cyclotomic_poly\nfrom sympy.ntheory.residue_ntheory import primitive_root, nthroot_mod\nfrom sympy.ntheory import n_order\nfrom itertools import product\n\nmpmath.mp.dps = 64\nmath.isprime = sympy.isprime\nsympy.crt = crt\nsympy.npolycyclotomic = cyclotomic_poly\nsympy.n_order = n_order\nsympy.multiplicative_order = n_order\n\ndef is_power_of_prime(n, p):\n    if n <= 0 or p <= 1 or not sympy.isprime(p):\n        return False\n    # n = p^k ?\n    while n % p == 0:\n        n //= p\n    return n == 1\n\nsympy.isprimepower = is_power_of_prime\n\ndef discrete_root(a: int, n: int, m: int, *, all_roots: bool = False):\n    \\\"\\\"\\\"\n    Solve x**n ≡ a (mod m).\n    Returns:\n      - smallest solution if all_roots=False\n      - sorted list of all solutions modulo m if all_roots=True\n\n    Notes:\n      - Uses sympy.ntheory.residue_ntheory.nthroot_mod for prime/prime-power moduli.\n      - For composite m, solves each prime-power modulus then combines via CRT.\n    \\\"\\\"\\\"\n    a = int(a); n = int(n); m = int(m)\n    if m <= 0 or n <= 0:\n        raise ValueError(\"Require m>0 and n>0\")\n\n    a %= m\n    if m == 1:\n        return [0] if all_roots else 0\n    if a == 0:\n        # For prime modulus p, only root is 0; for prime powers/composites there can be more,\n        # but we keep it simple and return 0 (a valid root).\n        return [0] if all_roots else 0\n\n    fac = sp.factorint(m)  # {p: e}\n    mod_list = []\n    roots_list = []\n\n    for p, e in fac.items():\n        pe = p**e\n        try:\n            r = nthroot_mod(a % pe, n, pe, all_roots=True)  # may return list or None\n        except Exception as ex:\n            raise NotImplementedError(f\"nthroot_mod failed for modulus {pe}: {ex}\") from ex\n        if not r:\n            return [] if all_roots else None\n        if not isinstance(r, (list, tuple)):\n            r = [r]\n        mod_list.append(pe)\n        roots_list.append(list(map(int, r)))\n\n    # If only one modulus component, return directly\n    if len(mod_list) == 1:\n        sols = sorted(set([x % mod_list[0] for x in roots_list[0]]))\n        return sols if all_roots else sols[0]\n\n    # Combine all combinations via CRT\n    sols = set()\n    for combo in product(*roots_list):\n        x, mod = crt(mod_list, combo)  # returns (x, lcm_mod) or (None, None)\n        if x is not None:\n            sols.add(int(x % m))\n\n    sols = sorted(sols)\n    return sols if all_roots else (sols[0] if sols else None)\n\nsympy.discrete_root = discrete_root\nsympy.ntheory.residue_ntheory.discrete_root = discrete_root\n\n# prevent huge number print error!\nsys.set_int_max_str_digits(0)  # unlimited (best-effort)\n# avoid recursion stack overflow!\nsys.setrecursionlimit(20000)\n\nimport pickle\nfrom joblib import Memory\nfrom functools import lru_cache, wraps\n\nmemory = Memory(location='aimo3_cache', verbose=0)\n_real_cache = memory.cache\n\ndef _safe_cache_decorator(func):\n    cached_func = _real_cache(func)\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            pickle.dumps((args, kwargs), protocol=pickle.HIGHEST_PROTOCOL)\n        except Exception:\n            return func(*args, **kwargs)\n        return cached_func(*args, **kwargs)\n    return wrapper\n\nmemory.cache = _safe_cache_decorator\n\"\"\"\n\n        self._helper_methods = \"\"\"\ndef head(obj, n=5):\n    if isinstance(obj, dict):\n        return dict(list(obj.items())[:n])\n    try:\n        return obj[:n]\n    except:\n        return obj\n\"\"\"\n\n        self.execute([self._helper_imports, self._helper_methods])\n\n    def _format_error(self, traceback: list[str]) -> str:\n        clean_lines = []\n        for frame in traceback:\n            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n                continue\n            clean_lines.append(clean_frame)\n        return ''.join(clean_lines)\n\n    def execute(self, code: str | Iterable[str], timeout: float | None = None) -> str:\n        client = self._client\n        effective_timeout = timeout or self._default_timeout\n\n        if isinstance(code, (list, tuple)):\n            code = \"\\n\\n\".join(c.rstrip() for c in code if isinstance(c, str) and c.strip())\n\n        msg_id = client.execute(\n            code, \n            store_history=True, \n            allow_stdin=False, \n            stop_on_error=False\n        )\n\n        stdout_parts = []\n        stderr_parts = []\n        start_time = time.time()\n        while True:\n            elapsed = time.time() - start_time\n            if elapsed > effective_timeout:\n                self._km.interrupt_kernel()\n                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n\n            try:\n                msg = client.get_iopub_msg(timeout=1.0)\n            except queue.Empty:\n                continue\n\n            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n                continue\n\n            msg_type = msg.get('msg_type')\n            content = msg.get('content', {})\n\n            if msg_type == 'stream':\n                text = content.get('text', '')\n                if content.get('name') == 'stdout':\n                    stdout_parts.append(text)\n                else:\n                    stderr_parts.append(text)\n\n            elif msg_type == 'error':\n                traceback_list = content.get('traceback', [])\n                stderr_parts.append(self._format_error(traceback_list))\n\n            elif msg_type in {'execute_result', 'display_data'}:\n                data = content.get('data', {})\n                text = data.get('text/plain')\n                if text:\n                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n\n            elif msg_type == 'status':\n                if content.get('execution_state') == 'idle':\n                    break\n\n        stdout = ''.join(stdout_parts)\n        stderr = ''.join(stderr_parts)\n\n        if stderr:\n            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n\n        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n\n    def close(self):\n        with contextlib.suppress(Exception):\n            if self._client:\n                self._client.stop_channels()\n        if self._owns_kernel and self._km is not None:\n            with contextlib.suppress(Exception):\n                self._km.shutdown_kernel(now=True)\n            with contextlib.suppress(Exception):\n                self._km.cleanup_resources()\n\n    def reset(self):\n        self.execute(['%reset -f\\n', self._helper_imports, self._helper_methods])\n\n    def __del__(self):\n        self.close()","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:44:20.387865Z","iopub.execute_input":"2026-01-29T11:44:20.387983Z","iopub.status.idle":"2026-01-29T11:44:20.400510Z","shell.execute_reply.started":"2026-01-29T11:44:20.387973Z","shell.execute_reply":"2026-01-29T11:44:20.400188Z"},"papermill":{"duration":0.016525,"end_time":"2026-01-12T17:27:14.779818","exception":false,"start_time":"2026-01-12T17:27:14.763293","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class AIMO3Tool:\n\n    def __init__(self, cfg, sandbox=None):\n        self.cfg = cfg\n        self._local_jupyter_timeout = cfg.jupyter_timeout\n        self._tool_prompt = cfg.tool_prompt\n        self._jupyter_session = sandbox\n        self._owns_session = sandbox is None\n        self._execution_lock = threading.Lock()\n        self._init_lock = threading.Lock()\n\n    def _ensure_session(self):\n        if self._jupyter_session is None:\n            with self._init_lock:\n                if self._jupyter_session is None:\n                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n\n    def _ensure_last_print(self, code: str) -> str:\n        lines = code.strip().split('\\n')\n        if not lines:\n            return code\n\n        last_line = lines[-1].strip()\n\n        if 'print' in last_line or 'import' in last_line:\n            return code\n        if not last_line:\n            return code\n        if last_line.startswith('#'):\n            return code\n        lines[-1] = 'print(' + last_line + ')'\n        return '\\n'.join(lines)\n\n    @property\n    def instruction(self) -> str:\n        return self._tool_prompt\n\n    @property\n    def tool_config(self) -> ToolNamespaceConfig:\n        return ToolNamespaceConfig(\n            name='python', \n            description=self.instruction, \n            tools=[]\n        )\n\n    def _make_response(self, output: str, channel: str | None = None) -> Message:\n        content = TextContent(text=output)\n        author = Author(role=Role.TOOL, name='python')\n        message = Message(author=author, content=[content]).with_recipient('assistant')\n\n        if channel:\n            message = message.with_channel(channel)\n        return message\n\n    def process_sync_plus(self, code: str, channel: str) -> list[Message]:\n        self._ensure_session()\n        # final_script = self._ensure_last_print(code)\n        with self._execution_lock:\n            try:\n                output = self._jupyter_session.execute(code)\n            except TimeoutError as exc:\n                output = f'[ERROR] {exc}'\n        return [self._make_response(output, channel=channel)]","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:44:20.401062Z","iopub.execute_input":"2026-01-29T11:44:20.401178Z","iopub.status.idle":"2026-01-29T11:44:20.415601Z","shell.execute_reply.started":"2026-01-29T11:44:20.401167Z","shell.execute_reply":"2026-01-29T11:44:20.415306Z"},"papermill":{"duration":0.010858,"end_time":"2026-01-12T17:27:14.795379","exception":false,"start_time":"2026-01-12T17:27:14.784521","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class AIMO3Server:\n\n    def __init__(self, cfg, port: int = 8000):\n        self.cfg = cfg\n        self.port = port\n        self.base_url = f'http://localhost:{port}/v1'\n        self.api_key = 'sk-local'\n        self.client = OpenAI(\n            base_url=self.base_url, \n            api_key=self.api_key, \n            timeout=self.cfg.session_timeout\n        )\n        self._preload_model_weights()\n        self.server_process = self._start_server()\n        self._wait_for_server()\n    \n    def _preload_model_weights(self) -> None:\n        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n        start_time = time.time()\n        files_to_load = []\n        total_size = 0\n        for root, _, files in os.walk(self.cfg.model_path):\n            for file_name in files:\n                file_path = os.path.join(root, file_name)\n                if os.path.isfile(file_path):\n                    files_to_load.append(file_path)\n                    total_size += os.path.getsize(file_path)\n    \n        def _read_file(path: str) -> None:\n            with open(path, 'rb') as file_object:\n                while file_object.read(1024 * 1024 * 1024):\n                    pass\n    \n        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n            list(executor.map(_read_file, files_to_load))\n    \n        elapsed = time.time() - start_time\n        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n    \n    def _start_server(self) -> subprocess.Popen:\n        cmd = [\n            sys.executable, '-m', 'vllm.entrypoints.openai.api_server', \n            '--seed', str(self.cfg.seed), \n            '--model', self.cfg.model_path, \n            '--served-model-name', self.cfg.served_model_name, \n            '--tensor-parallel-size', '1', \n            '--max-num-seqs', str(self.cfg.batch_size), \n            '--gpu-memory-utilization', str(self.cfg.gpu_memory_utilization), \n            '--host', '0.0.0.0', \n            '--port', str(self.port), \n            '--dtype', self.cfg.dtype, \n            '--kv-cache-dtype', self.cfg.kv_cache_dtype, \n            '--max-model-len', str(self.cfg.context_tokens), \n            '--stream-interval', str(self.cfg.stream_interval), \n            '--async-scheduling', \n            '--disable-log-stats', \n            '--enable-prefix-caching'\n        ]\n        self.log_file = open('vllm_server.log', 'w')\n        return subprocess.Popen(\n            cmd, \n            stdout=self.log_file, \n            stderr=subprocess.STDOUT, \n            start_new_session=True\n        )\n    \n    def _wait_for_server(self):\n        print('Waiting for vLLM server...')\n        start_time = time.time()\n        for _ in range(self.cfg.server_timeout):\n            return_code = self.server_process.poll()\n            if return_code is not None:\n                self.log_file.flush()\n                with open('vllm_server.log', 'r') as log_file:\n                    logs = log_file.read()\n                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n    \n            try:\n                self.client.models.list()\n                elapsed = time.time() - start_time\n                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n                return\n            except Exception:\n                time.sleep(1)\n        raise RuntimeError('Server failed to start (timeout).\\n')\n\n    def __del__(self):\n        if hasattr(self, 'server_process'):\n            self.server_process.terminate()\n            self.server_process.wait()\n        if hasattr(self, 'log_file'):\n            self.log_file.close()","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:44:20.416039Z","iopub.execute_input":"2026-01-29T11:44:20.416153Z","iopub.status.idle":"2026-01-29T11:44:20.432833Z","shell.execute_reply.started":"2026-01-29T11:44:20.416142Z","shell.execute_reply":"2026-01-29T11:44:20.432485Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class AIMO3Solver:\n\n    def __init__(self, cfg, port: int = 8000):\n        self.cfg = cfg\n        self.port = port\n        self.base_url = f'http://localhost:{port}/v1'\n        self.api_key = 'sk-local'\n        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key, timeout=self.cfg.session_timeout)\n\n        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n        \n        self.notebook_start_time = time.time()\n        self.problems_remaining = 50\n        self.logger = AIMO3Logger(cfg)\n        self._initialize_kernels()\n\n    def _initialize_kernels(self) -> None:\n        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n        start_time = time.time()\n        self.sandbox_pool = queue.Queue()\n\n        def _create_sandbox():\n            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n    \n        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n            for future in as_completed(futures):\n                self.sandbox_pool.put(future.result())\n\n        elapsed = time.time() - start_time\n        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n\n    def _scan_for_answer(self, text: str) -> int | None:\n        pattern = r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}'\n        matches = re.findall(pattern, text)\n    \n        if matches:\n            try:\n                clean_value = matches[-1].replace(',', '')\n                value = int(clean_value)\n                if 0 <= value <= 99999:\n                    return value\n    \n            except ValueError:\n                pass\n        \n        pattern = r'final\\s+answer\\s+is\\s*([0-9,]+)'\n        matches = re.findall(pattern, text, re.IGNORECASE)\n    \n        if matches:\n            try:\n                clean_value = matches[-1].replace(',', '')\n                value = int(clean_value)\n                if 0 <= value <= 99999:\n                    return value\n    \n            except ValueError:\n                pass\n    \n        return None\n    \n    def _compute_mean_entropy(self, logprobs_buffer: list) -> float:\n        if not logprobs_buffer:\n            return float('inf')\n    \n        total_entropy = 0.0\n        token_count = 0\n    \n        for top_logprobs_dict in logprobs_buffer:            \n            if not isinstance(top_logprobs_dict, dict):\n                continue\n            \n            if not top_logprobs_dict:\n                continue\n            \n            token_entropy = 0.0\n            \n            for token_str, log_prob in top_logprobs_dict.items():\n                prob = math.exp(log_prob)\n                \n                if prob > 0:\n                    token_entropy -= prob * math.log2(prob)\n            \n            total_entropy += token_entropy\n            token_count += 1\n    \n        if token_count == 0:\n            return float('inf')\n    \n        return total_entropy / token_count\n    \n    def _process_attempt(\n        self, \n        problem: str, \n        system_prompt: str, \n        attempt_index: int, \n        stop_event: threading.Event, \n        deadline: float,\n        problem_id: str,\n        plan: str = \"\",\n    ) -> dict:\n        attempt_log = deque([])\n        attempt_start = time.time()\n\n        if stop_event.is_set() or time.time() > deadline:\n            return {\n                'Attempt': attempt_index + 1, \n                'Answer': None, \n                'Python Calls': 0, \n                'Python Errors': 0, \n                'Response Length': 0, \n                'Entropy': float('inf'),\n                'Log': \"\\n\".join(attempt_log)\n            }\n    \n        local_tool = None\n        sandbox = None\n        python_calls = 0\n        python_errors = 0\n        total_tokens = 0\n        final_answer = None\n        \n        logprobs_buffer = []\n        attempt_seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n    \n        try:\n            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)    \n            local_tool = AIMO3Tool(self.cfg, sandbox=sandbox)\n    \n            encoding = self.encoding\n            system_content = (\n                SystemContent.new()\n                .with_model_identity(system_prompt)\n                .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n                .with_tools(local_tool.tool_config)\n            )\n            messages = [\n                Message.from_role_and_content(Role.SYSTEM, system_content),\n                Message.from_role_and_content(Role.USER, problem), # problem\n            ]\n            conversation = Conversation.from_messages(messages)\n    \n            for turn_i in range(self.cfg.turns):\n                if stop_event.is_set() or time.time() > deadline:\n                    break\n    \n                prompt_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n    \n                if max_tokens < self.cfg.buffer_tokens:\n                    break\n    \n                if self.cfg.debug and self.cfg.debug_req:\n                    full_request_text = encoding.decode(prompt_ids)\n                    snippet = self.logger.get_debug_snippet(full_request_text)\n                    # for Turn 0, 20,... log full request\n                    formatted_req = _format_markdown(full_request_text if turn_i % self.cfg.debug_req_full == 0 else snippet)\n                    attempt_log.append(f\"### Turn {turn_i} - Raw Request to Model:\")\n                    attempt_log.append(formatted_req)\n\n                # didn't find any timeout, ignore for now\n                # req_timeout = max(1.0, deadline - time.time())\n                stream = self.client.completions.create(\n                    model=self.cfg.served_model_name, \n                    temperature=self.cfg.temperature, \n                    logprobs=self.cfg.top_logprobs, \n                    max_tokens=max_tokens, \n                    prompt=prompt_ids, \n                    seed=attempt_seed, \n                    stream=True, \n                    extra_body={\n                        'min_p': self.cfg.min_p, \n                        'stop_token_ids': self.stop_token_ids, \n                        'return_token_ids': True\n                    }\n                )\n    \n                full_response_text = \"\"\n                try:\n                    token_buffer = []\n                    text_chunks = []\n    \n                    for chunk in stream:\n                        if stop_event.is_set() or time.time() > deadline:\n                            break\n    \n                        new_tokens = chunk.choices[0].token_ids\n                        new_text = chunk.choices[0].text\n    \n                        if new_tokens:\n                            token_buffer.extend(new_tokens)\n                            total_tokens += len(new_tokens)\n                            text_chunks.append(new_text)\n                            if self.cfg.debug and self.cfg.debug_resp:\n                                full_response_text += new_text\n\n                            chunk_logprobs = chunk.choices[0].logprobs\n                            if chunk_logprobs is not None:\n                                if chunk_logprobs.top_logprobs:\n                                    logprobs_buffer.extend(chunk_logprobs.top_logprobs)\n    \n                        if '}' in new_text:\n                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n                            answer = self._scan_for_answer(search_text)\n    \n                            if answer is not None:\n                                final_answer = answer\n                                break\n    \n                finally:\n                    stream.close()\n\n                if self.cfg.debug and full_response_text:\n                    attempt_log.append(f\"### Turn {turn_i} - Model Response:\")\n                    formatted_resp = _format_markdown(full_response_text)\n                    attempt_log.append(formatted_resp)\n\n                if final_answer is not None:\n                    break\n    \n                if not token_buffer:\n                    break\n    \n                new_messages = encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n                conversation.messages.extend(new_messages)\n                last_message = new_messages[-1]\n    \n                if last_message.channel == 'final':\n                    answer_text = last_message.content[0].text\n                    final_answer = self._scan_for_answer(answer_text)\n                    break\n    \n                if last_message.recipient == 'python':\n                    python_calls += 1\n                    raw_script = last_message.content[0].text\n                    final_script = _rewrite_code(raw_script)\n                    tool_responses = local_tool.process_sync_plus(final_script, last_message.channel)\n                    response_text = tool_responses[0].content[0].text\n    \n                    has_error = response_text.startswith('[ERROR]') or 'Traceback' in response_text or 'Error:' in response_text\n                    if self.cfg.debug:\n                        emoji_error = '❌' if has_error else ''\n                        attempt_log.append(f\"### Turn {turn_i} - Python Raw:\")\n                        attempt_log.append(_format_markdown(raw_script, mode='python'))\n                        attempt_log.append(f\"### Turn {turn_i} - Python Call:\")\n                        attempt_log.append(_format_markdown(final_script, mode='python'))\n                        attempt_log.append(f\"### Turn {turn_i} {emoji_error} - Python Output:\")\n                        snippet_out = self.logger.get_debug_snippet(response_text)\n                        formatted_out = _format_markdown(snippet_out, mode='text')\n                        attempt_log.append(f\"{formatted_out}\\n\")\n                    \n                    if has_error:\n                        python_errors += 1\n    \n                    conversation.messages.extend(tool_responses)\n    \n        except Exception as exc:\n            python_errors += 1\n            if self.cfg.debug:\n                attempt_log.append(f\"\\n**EXCEPTION:** {str(exc)}\\n\")\n            print(f\"EXCEPTION: {str(exc)}\")\n    \n        finally:\n            if sandbox is not None:\n                sandbox.reset()\n                self.sandbox_pool.put(sandbox)\n    \n        mean_entropy = self._compute_mean_entropy(logprobs_buffer)\n        attempt_elapsed = time.time() - attempt_start\n        attempt_time = _fmt_time(attempt_elapsed)\n        if self.cfg.debug:\n            attempt_log.appendleft(f\"Attempt spent time: **{attempt_time}**\\n\")\n            attempt_log.appendleft(f\"## Attempt {attempt_index + 1}\\n\")\n\n        return {\n            'Attempt': attempt_index + 1, \n            'Response Length': total_tokens, \n            'Python Calls': python_calls, \n            'Python Errors': python_errors, \n            'Entropy': mean_entropy, \n            'Answer': final_answer,\n            'Log': \"\\n\".join(attempt_log),\n            'Time': attempt_time\n        }\n    \n    def _select_answer(self, detailed_results: list) -> int:\n        answer_weights = defaultdict(float)\n        answer_votes = defaultdict(int)\n\n        for result in detailed_results:\n            answer = result['Answer']\n            entropy = result['Entropy']\n            \n            if answer is not None:\n                weight = 1.0 / max(entropy, 1e-9)\n                answer_weights[answer] += weight\n                answer_votes[answer] += 1\n\n        scored_answers = []\n        for answer, total_weight in answer_weights.items():\n            scored_answers.append({\n                'answer': answer, \n                'votes': answer_votes[answer], \n                'score': total_weight\n            })\n\n        scored_answers.sort(key=lambda x: x['score'], reverse=True)\n        vote_data = []\n\n        for item in scored_answers:\n            vote_data.append((\n                item['answer'], \n                item['votes'], \n                item['score']\n            ))\n\n        vote_dataframe = pd.DataFrame(\n            vote_data, \n            columns=['Answer', 'Votes', 'Score']\n        )\n\n        vote_dataframe = vote_dataframe.round({'Score': 3})\n        display(vote_dataframe)\n        \n        if not scored_answers:\n            print('\\nFinal Answer: 0\\n')\n            return 0\n\n        final_answer = scored_answers[0]['answer']    \n        print(f'\\nFinal Answer: {final_answer}\\n')\n\n        return vote_dataframe, final_answer\n    \n    def solve_problem(self, problem: str, problem_id: str = 'UNK') -> int:\n        print(f'\\nProblem: {problem}\\n')\n        p_start = time.time()\n        user_input = f'{problem} {self.cfg.preference_prompt}'\n    \n        elapsed_global = time.time() - self.notebook_start_time\n        time_left = self.cfg.notebook_limit - elapsed_global\n        problems_left_others = max(0, self.problems_remaining - 1)\n        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n    \n        budget = time_left - reserved_time\n        budget = min(budget, self.cfg.high_problem_timeout)\n        budget = max(budget, self.cfg.base_problem_timeout)\n    \n        deadline = time.time() + budget\n    \n        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n    \n        tasks = []\n    \n        for attempt_index in range(self.cfg.attempts):\n            tasks.append((self.cfg.system_prompt, attempt_index))\n    \n        detailed_results = []\n        valid_answers = []\n    \n        stop_event = threading.Event()\n    \n        executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n    \n        try:\n            futures = []\n    \n            for (system_prompt, attempt_index) in tasks:\n                future = executor.submit(\n                    self._process_attempt, \n                    user_input, \n                    system_prompt, \n                    attempt_index, \n                    stop_event, \n                    deadline,\n                    problem_id,\n                )\n    \n                futures.append(future)\n    \n            for future in as_completed(futures):\n                try:\n                    result = future.result()\n                    detailed_results.append(result)\n                    if result['Answer'] is not None:\n                        valid_answers.append(result['Answer'])\n    \n                    counts = Counter(valid_answers).most_common(1)\n                    if counts and counts[0][1] >= self.cfg.early_stop:\n                        stop_event.set()\n                        for f in futures:\n                            f.cancel()\n                        break\n    \n                except Exception as exc:\n                    print(f'Future failed: {exc}')\n                    continue\n    \n        finally:\n            stop_event.set()\n            executor.shutdown(wait=True, cancel_futures=True)\n            self.problems_remaining = max(0, self.problems_remaining - 1)\n    \n        if detailed_results:\n            results_dataframe = pd.DataFrame(detailed_results)\n            results_dataframe['Entropy'] = results_dataframe['Entropy'].round(3)\n            results_dataframe['Answer'] = results_dataframe['Answer'].astype('Int64')\n            \n            cols = [c for c in results_dataframe.columns if not c in self.cfg.debug_cols]\n            display(results_dataframe[cols])\n    \n        if not valid_answers:\n            print('\\nResult: 0\\n')\n            vote_data, final_answer = pd.DataFrame(columns=['Answer', 'Votes', 'Score']), 0\n        else:\n            vote_data, final_answer = self._select_answer(detailed_results)\n        \n        p_end = time.time()\n        self.logger.write_debug_logs(detailed_results, vote_data, problem, problem_id, _fmt_time(p_end - p_start))\n        return final_answer\n\n    def __del__(self):\n        if hasattr(self, 'sandbox_pool'):\n            while not self.sandbox_pool.empty():\n                try:\n                    sb = self.sandbox_pool.get_nowait()\n                    sb.close()\n                except Exception:\n                    pass","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:44:20.433362Z","iopub.execute_input":"2026-01-29T11:44:20.433479Z","iopub.status.idle":"2026-01-29T11:44:20.455835Z","shell.execute_reply.started":"2026-01-29T11:44:20.433468Z","shell.execute_reply":"2026-01-29T11:44:20.455493Z"},"papermill":{"duration":0.030513,"end_time":"2026-01-12T17:27:14.830218","exception":false,"start_time":"2026-01-12T17:27:14.799705","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"_delete(\"server\")\nserver = AIMO3Server(CFG)\n\n_delete(\"solver\")\nsolver = AIMO3Solver(CFG)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2026-01-29T11:44:20.456829Z","iopub.execute_input":"2026-01-29T11:44:20.456963Z","iopub.status.idle":"2026-01-29T11:47:38.032872Z","shell.execute_reply.started":"2026-01-29T11:44:20.456950Z","shell.execute_reply":"2026-01-29T11:47:38.032454Z"},"papermill":{"duration":229.073583,"end_time":"2026-01-12T17:31:03.908118","exception":false,"start_time":"2026-01-12T17:27:14.834535","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Loading model weights from /kaggle/input/gpt-oss-120b/transformers/default/1 into OS Page Cache...\nProcessed 26 files (65.28 GB) in 72.87 seconds.\n\nWaiting for vLLM server...\nServer is ready (took 121.63 seconds).\n\nInitializing 16 persistent Jupyter kernels...\nKernels initialized in 2.68 seconds.\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"predict_answers = {}\n\ndef predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n    global predict_answers\n    id_value = id_.item(0)\n    question_text = question.item(0)\n    gc.disable()\n    p_start = time.time()\n    final_answer = solver.solve_problem(question_text, problem_id=str(id_value))\n    p_end = time.time()\n    p_time = p_end - p_start\n    predict_answers[id_value] = {'id': id_value, 'answer': final_answer, 'time': p_time, 'time_str': _fmt_time(p_time)}\n    gc.enable()\n    gc.collect()\n    return pl.DataFrame({'id': id_value, 'answer': final_answer})\n\ninference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n\ndef test():\n    global predict_answers\n\n    # test_csv = '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv'\n    # test_csv = '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv'\n    # test_csv = '/kaggle/input/aimo-p3-hard/easy2.csv'\n    # test_csv = '/kaggle/input/aimo-p3-hard/test2.csv'\n    # test_csv = '/kaggle/input/aimo-p3-hard/test3.csv'\n    # test_csv = '/kaggle/input/aimo-p3-hard/p5.csv'\n    test_csv = '/kaggle/input/aimo-p3-hard/p10.csv'\n\n    t_start = time.time()\n    inference_server.run_local_gateway((test_csv,))\n    t_end = time.time()\n    t_time = t_end - t_start\n\n    df = pd.read_csv(test_csv)\n    real_answers = dict(zip(df[\"id\"], df[\"answer\"])) if \"answer\" in df.columns else {}\n    correct_count = 0\n    total_count = 0\n    # Check accuracy if ground truth available\n    for id in predict_answers:\n        pa = predict_answers[id]\n        if id in real_answers:\n            total_count += 1\n            real_answer = real_answers[id]\n            is_correct = (pa['answer'] == real_answer)\n            if is_correct:\n                correct_count += 1\n            status = \"✅\" if is_correct else \"❌\"\n            print(f\"Problem {id}: ({pa['time_str']}) -- Predict Answer: {pa['answer']} | Ground Truth: {real_answer} | {status}\")\n        else:\n            print(f\"Problem {id}: ({pa['time_str']}) -- Predict Answer: {pa['answer']}\")\n    \n    df2 = pl.DataFrame(list(predict_answers.values()))\n    stats = df2.select([\n        pl.col(\"time\").max().alias(\"max_time\"),\n        pl.col(\"time\").min().alias(\"min_time\"),\n        pl.col(\"time\").mean().alias(\"avg_time\"),\n    ])\n\n    max_id = df2.filter(pl.col(\"time\") == stats[\"max_time\"][0]).select(\"id\").item()\n    min_id = df2.filter(pl.col(\"time\") == stats[\"min_time\"][0]).select(\"id\").item()\n\n    print(f\"📊 Total {total_count} problems, ⏱️ total time {_fmt_time(t_time)}; \"\n          f\"Running Accuracy 🎯: {correct_count}/{total_count} ({100*correct_count/total_count:.1f}%)\\n\"\n          f\"Max time: {_fmt_time(stats['max_time'][0])} (id={max_id}); \"\n          f\"Min time: {_fmt_time(stats['min_time'][0])} (id={min_id}); \"\n          f\"Avg time: {_fmt_time(stats['avg_time'][0])}\\n\")","metadata":{"execution":{"iopub.status.busy":"2026-01-29T11:47:38.033422Z","iopub.execute_input":"2026-01-29T11:47:38.033561Z","iopub.status.idle":"2026-01-29T11:47:38.100252Z","shell.execute_reply.started":"2026-01-29T11:47:38.033547Z","shell.execute_reply":"2026-01-29T11:47:38.099874Z"},"papermill":{"duration":0.009843,"end_time":"2026-01-12T17:31:03.922914","exception":false,"start_time":"2026-01-12T17:31:03.913071","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"CFG.ast_add_cache = True\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    CFG.debug = False\n    inference_server.serve()\nelse:\n    test()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2026-01-29T11:47:38.100881Z","iopub.execute_input":"2026-01-29T11:47:38.101008Z","iopub.status.idle":"2026-01-29T11:58:51.360044Z","shell.execute_reply.started":"2026-01-29T11:47:38.100995Z","shell.execute_reply":"2026-01-29T11:58:51.359598Z"},"papermill":{"duration":21.184855,"end_time":"2026-01-12T17:31:25.112805","exception":false,"start_time":"2026-01-12T17:31:03.927950","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\nProblem: Let $n \\geq 6$ be a positive integer. We call a positive integer $n$-Norwegian if it has three distinct positive divisors whose sum is equal to $n$. Let $f(n)$ denote the smallest $n$-Norwegian positive integer. Let $M=3^{2025!}$ and for a non-negative integer $c$ define \n$$\n    g(c)=\\frac{1}{2025!}\\left\\lfloor \\frac{2025! f(M+c)}{M}\\right\\rfloor.\n$$\nWe can write \n$$\n    g(0)+g(4M)+g(1848374)+g(10162574)+g(265710644)+g(44636594)=\\frac{p}{q}\n$$\nwhere $p$ and $q$ are coprime positive integers. What is the remainder when $p+q$ is divided by $99991$?\n\nBudget: 900.00 seconds | Deadline: 1769688158.45\n\nEXCEPTION: invalid syntax (<unknown>, line 9)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer  \\\n0        6            30371            38              3    0.700    6825   \n1        3            33539            25              0    0.710   78742   \n2        5            32011            26              1    0.713    <NA>   \n3        7            29375            86              7    0.665    8687   \n4        2            34288            53              2    0.729   40958   \n5        4            56643            60              3    0.696    <NA>   \n6        8            59817            55              6    0.711    <NA>   \n7        1            57205           114              3    0.676    8687   \n\n    Time  \n0   6:33  \n1   6:46  \n2   6:54  \n3   7:12  \n4   7:27  \n5  10:32  \n6  10:49  \n7  11:12  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Attempt</th>\n      <th>Response Length</th>\n      <th>Python Calls</th>\n      <th>Python Errors</th>\n      <th>Entropy</th>\n      <th>Answer</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>30371</td>\n      <td>38</td>\n      <td>3</td>\n      <td>0.700</td>\n      <td>6825</td>\n      <td>6:33</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>33539</td>\n      <td>25</td>\n      <td>0</td>\n      <td>0.710</td>\n      <td>78742</td>\n      <td>6:46</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>32011</td>\n      <td>26</td>\n      <td>1</td>\n      <td>0.713</td>\n      <td>&lt;NA&gt;</td>\n      <td>6:54</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>29375</td>\n      <td>86</td>\n      <td>7</td>\n      <td>0.665</td>\n      <td>8687</td>\n      <td>7:12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>34288</td>\n      <td>53</td>\n      <td>2</td>\n      <td>0.729</td>\n      <td>40958</td>\n      <td>7:27</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>56643</td>\n      <td>60</td>\n      <td>3</td>\n      <td>0.696</td>\n      <td>&lt;NA&gt;</td>\n      <td>10:32</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8</td>\n      <td>59817</td>\n      <td>55</td>\n      <td>6</td>\n      <td>0.711</td>\n      <td>&lt;NA&gt;</td>\n      <td>10:49</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>57205</td>\n      <td>114</td>\n      <td>3</td>\n      <td>0.676</td>\n      <td>8687</td>\n      <td>11:12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   Answer  Votes  Score\n0    8687      2  2.983\n1    6825      1  1.428\n2   78742      1  1.408\n3   40958      1  1.371","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Answer</th>\n      <th>Votes</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8687</td>\n      <td>2</td>\n      <td>2.983</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6825</td>\n      <td>1</td>\n      <td>1.428</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>78742</td>\n      <td>1</td>\n      <td>1.408</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>40958</td>\n      <td>1</td>\n      <td>1.371</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nFinal Answer: 8687\n\nDebug log written to P10.md\nProblem P10: (11:12) -- Predict Answer: 8687 | Ground Truth: 8687 | ✅\n📊 Total 1 problems, ⏱️ total time 11:13; Running Accuracy 🎯: 1/1 (100.0%)\nMax time: 11:12 (id=P10); Min time: 11:12 (id=P10); Avg time: 11:12\n\n","output_type":"stream"}],"execution_count":12}]}