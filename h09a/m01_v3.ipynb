{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d2a2e5",
   "metadata": {
    "papermill": {
     "duration": 0.003292,
     "end_time": "2026-01-25T23:19:51.556414",
     "exception": false,
     "start_time": "2026-01-25T23:19:51.553122",
     "status": "completed"
    },
    "tags": [
     "ignore"
    ]
   },
   "source": [
    "### prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680f2e9b",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-01-25T23:19:51.562322Z",
     "iopub.status.busy": "2026-01-25T23:19:51.562159Z",
     "iopub.status.idle": "2026-01-25T23:21:04.954652Z",
     "shell.execute_reply": "2026-01-25T23:21:04.954208Z"
    },
    "papermill": {
     "duration": 73.396586,
     "end_time": "2026-01-25T23:21:04.955484",
     "exception": false,
     "start_time": "2026-01-25T23:19:51.558898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: keras 3.10.0\r\n",
      "Uninstalling keras-3.10.0:\r\n",
      "  Successfully uninstalled keras-3.10.0\r\n",
      "Found existing installation: matplotlib 3.10.0\r\n",
      "Uninstalling matplotlib-3.10.0:\r\n",
      "  Successfully uninstalled matplotlib-3.10.0\r\n",
      "Found existing installation: scikit-learn 1.6.1\r\n",
      "Uninstalling scikit-learn-1.6.1:\r\n",
      "  Successfully uninstalled scikit-learn-1.6.1\r\n",
      "Found existing installation: tensorflow 2.19.0\r\n",
      "Uninstalling tensorflow-2.19.0:\r\n",
      "  Successfully uninstalled tensorflow-2.19.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ff8818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T23:21:04.962275Z",
     "iopub.status.busy": "2026-01-25T23:21:04.962105Z",
     "iopub.status.idle": "2026-01-25T23:24:17.375387Z",
     "shell.execute_reply": "2026-01-25T23:24:17.375013Z"
    },
    "papermill": {
     "duration": 192.420627,
     "end_time": "2026-01-25T23:24:17.379055",
     "exception": false,
     "start_time": "2026-01-25T23:21:04.958428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl100k_base.tiktoken\n",
      "o200k_base.tiktoken\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ls', '/kaggle/tmp/setup/tiktoken_encodings'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def set_env(input_archive, temp_dir):\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n",
    "    \n",
    "    subprocess.run([\n",
    "        sys.executable, \n",
    "        '-m', \n",
    "        'pip', \n",
    "        'install', \n",
    "        '--no-index', \n",
    "        '--find-links', \n",
    "        f'{temp_dir}/wheels', \n",
    "        'unsloth', \n",
    "        'trl', \n",
    "        'vllm', \n",
    "        'openai_harmony'\n",
    "    ], \n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    "    check=True)\n",
    "\n",
    "set_env(\n",
    "    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz', \n",
    "    temp_dir='/kaggle/tmp/setup'\n",
    ")\n",
    "\n",
    "subprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896a5d4",
   "metadata": {
    "papermill": {
     "duration": 0.002688,
     "end_time": "2026-01-25T23:24:17.384726",
     "exception": false,
     "start_time": "2026-01-25T23:24:17.382038",
     "status": "completed"
    },
    "tags": [
     "ignore"
    ]
   },
   "source": [
    "### CFG including prompts & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c383d12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T23:24:17.391273Z",
     "iopub.status.busy": "2026-01-25T23:24:17.391117Z",
     "iopub.status.idle": "2026-01-25T23:24:24.835890Z",
     "shell.execute_reply": "2026-01-25T23:24:24.835442Z"
    },
    "papermill": {
     "duration": 7.449649,
     "end_time": "2026-01-25T23:24:24.837041",
     "exception": false,
     "start_time": "2026-01-25T23:24:17.387392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
    "os.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import math\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import contextlib\n",
    "from collections import deque\n",
    "from typing import Iterable, Optional\n",
    "from jupyter_client import KernelManager\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import shutil\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from openai_harmony import (\n",
    "    HarmonyEncodingName, \n",
    "    load_harmony_encoding, \n",
    "    SystemContent, \n",
    "    ReasoningEffort, \n",
    "    ToolNamespaceConfig, \n",
    "    Author, \n",
    "    Message, \n",
    "    Role, \n",
    "    TextContent, \n",
    "    Conversation\n",
    ")\n",
    "\n",
    "from transformers import set_seed\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "\n",
    "class CFG:\n",
    "\n",
    "    system_prompt = (\n",
    "        'You are an elite mathematical problem solver with expertise at the International '\n",
    "        'Mathematical Olympiad (IMO) level. Your goal is to find the correct answer through '\n",
    "        'rigorous mathematical reasoning.\\n\\n'\n",
    "        # 'Use \\\\boxed{} exactly once at the very end (never for intermediate results).'\n",
    "        # 'If you cannot finish within time, output your best verified result anyway as \\\\boxed{N}.'\n",
    "\n",
    "        '# Problem-Solving Approach:\\n'\n",
    "        '1. UNDERSTAND: Carefully read and rephrase the problem in your own words. '\n",
    "        'Identify what is given, what needs to be found, and any constraints.\\n'\n",
    "        '2. EXPLORE: Consider multiple solution strategies. Think about relevant theorems, '\n",
    "        'techniques, patterns, or analogous problems. Don\\'t commit to one approach immediately.\\n'\n",
    "        '3. PLAN: Select the most promising approach and outline key steps before executing.\\n'\n",
    "        '4. EXECUTE: Work through your solution methodically. Show all reasoning steps clearly.\\n'\n",
    "        '5. VERIFY: Check your answer by substituting back, testing edge cases, or using '\n",
    "        'alternative methods. Ensure logical consistency throughout.\\n\\n'\n",
    "\n",
    "        '# Mathematical Reasoning Principles:\\n'\n",
    "        '- Break complex problems into smaller, manageable sub-problems\\n'\n",
    "        '- Look for patterns, symmetries, and special cases that provide insight\\n'\n",
    "        '- Use concrete examples to build intuition before generalizing\\n'\n",
    "        '- Consider extreme cases and boundary conditions\\n'\n",
    "        '- If stuck, try working backwards from the desired result\\n'\n",
    "        '- Be willing to restart with a different approach if needed\\n\\n'\n",
    "\n",
    "        '# Verification Requirements:\\n'\n",
    "        '- Cross-check arithmetic and algebraic manipulations\\n'\n",
    "        '- Verify that your solution satisfies all problem constraints\\n'\n",
    "        '- Test your answer with simple cases or special values when possible\\n'\n",
    "        '- Ensure dimensional consistency and reasonableness of the result\\n\\n'\n",
    "\n",
    "        '# Output Format:\\n'\n",
    "        'The final answer must be a non-negative integer between 0 and 99999.\\n'\n",
    "        'Place your final numerical answer inside \\\\boxed{}, e.g., \\\\boxed{42}\\n\\n'\n",
    "\n",
    "        'Think step-by-step and show your complete reasoning process. Quality of reasoning '\n",
    "        'is as important as the final answer.'\n",
    "    )\n",
    "    \n",
    "    tool_prompt = (\n",
    "        'Use this tool to execute Python code for:\\n'\n",
    "        '- error-prone arithmetic, modular computations, factorization, and small-case scans\\n'\n",
    "        '- numerical verification of analytical results\\n'\n",
    "        '- verifying intermediate claims (sanity checks), not for replacing reasoning\\n\\n'\n",
    "        'Environment: stateful Jupyter. ALWAYS print() final results you need.\\n'\n",
    "        'Batch work: prefer ONE Python call that computes all required quantities.\\n\\n'\n",
    "\n",
    "        '**ENFORCED RULE: NO SLICING**\\n'\n",
    "        '* **Slicing is forbidden for all objects.** Do not use `x[:n]`, `x[a:b]`, `x[::k]`, `x[-n:]`, or any bracket slicing form that contains `:`.\\n'\n",
    "        '* **Always replace any slice with `head(x, n)`** (the `head(obj, n)` function is pre-defined and works for all supported types).\\n'\n",
    "        '* Examples (must follow exactly):\\n'\n",
    "        '  * `x[:10]` → `head(x, 10)`\\n'\n",
    "        '  * `x[:k]` → `head(x, k)`\\n'\n",
    "        '  * `x[-10:]` → `head(x, 10)`  *(take last-n is not supported; use head as an approximation if needed)*\\n'\n",
    "        '  * `x[1:10]` → `head(x, 10)` *(ignore start index; use head)*\\n'\n",
    "        '* Any code containing a slice (a `:` inside `[...]`) will be rejected or auto-rewritten to `head(x, n)`.\\n'\n",
    "\n",
    "        # '**Hard rules (enforced):**\\n'\n",
    "        # --- preview / slicing safety ---\n",
    "        # '- Any code using `X[:n]` where `X` may be a dict will be rejected. Use `head(X, n)`.\\n'\n",
    "        '- Any helper function that is called more than once, is recursive, or is used inside a loop **must** be decorated with `@memory.cache`, otherwise the tool execution will be rejected.\\n'\n",
    "        '- `head`, `memory` is pre-defined; do not redefine it.\\n'\n",
    "        # --- huge integer safety ---\n",
    "        '- NEVER do str(x), len(str(x)), f\\'{x}\\' when x might be huge.\\n'\n",
    "        '- Use x.bit_length(), x % mod, math.gcd(a,b), or modular checks.\\n'\n",
    "        '- For big exponents ALWAYS use pow(a, e, mod). Never pow(a, huge_e) without mod.\\n'\n",
    "        '- If you need a safe summary: print(bit_length, mod residues, gcds), not the integer itself.\\n\\n'\n",
    "        # --- number theory / correctness ---\n",
    "        '- If divisors/primes matter: use sympy.factorint(n).\\n'\n",
    "        '- Use explicit namespaces: math.gcd / math.lcm / sympy.factorint. Avoid bare gcd/lcm names.\\n\\n'\n",
    "        # --- performance & safety rules ---\n",
    "        '- No `while True`. All loops must have explicit bounds.\\n'\n",
    "        '- Start small range, time it, then scale by x2/x3/.. only if needed and fast.\\n'\n",
    "        '- Avoid large nested loops; if slow, switch to math (mod/factorization/caching/early break).\\n'\n",
    "        '- If a call times out: DO NOT rerun the same code; reduce bounds or change method.\\n'\n",
    "    )\n",
    "\n",
    "    preference_prompt = (\n",
    "        'You have access to math / numpy / sympy.\\n'\n",
    "        'General strategy:\\n'\n",
    "        '- Prefer verifiable approaches: modular arithmetic, factorization, bounds + small scans.\\n'\n",
    "        '- If selecting the best among many integers: define a candidate set + coverage plan\\n'\n",
    "        '  (prove a bound, or do bounded scan + verification).\\n\\n'\n",
    "        'Practical defaults:\\n'\n",
    "        '- For expensive recursive functions or repeated calculations, always decorate them with `@memory.cache`.'\n",
    "        '  Example: ```@memory.cache\\ndef complex_calc(n): ...```\\n'\n",
    "        '- Modular arithmetic: use pow(a,e,mod) for huge exponents.\\n'\n",
    "        '- Factorization/divisors: sympy.factorint.\\n'\n",
    "        '- Exact rationals: sympy.Rational or fractions.Fraction when p/q matters.\\n'\n",
    "        '- For problems involving large $M$, investigate if $f(M+c)$ follows a periodic pattern or depends only on $M \\pmod d$. '\n",
    "        '  Always check divisors of $M$ and surrounding integers.\\n'\n",
    "        '- For brute force: start small, add pruning/caching, and only expand if necessary.\\n\\n'\n",
    "    )\n",
    "\n",
    "    # --- NEW: planner (separate session) ---\n",
    "    planner_system_prompt = (\n",
    "        \"You are a mathematical competition strategist. Given a problem and history, \"\n",
    "        \"generate {num_plans} distinct, executable strategies. \"\n",
    "        \"Each strategy must be separated by '---PLAN SEPARATOR---'.\\n\"\n",
    "        \"Constraints:\\n\"\n",
    "        \"1. Diversity: Each plan must use a different focus (e.g., Number Theory, Brute Force, Pattern Search, Symbolic Algebra).\\n\"\n",
    "        \"2. Executability: Focus on high-level Python logic. DO NOT perform manual calculations.\\n\"\n",
    "        \"3. Anti-Boundary: Do not attempt to compute huge numbers directly; use modular arithmetic.\\n\"\n",
    "        \"Format: PLAN 1: [Short Title] \\n[Steps] ... ---PLAN SEPARATOR--- PLAN 2: ...\"\n",
    "    )\n",
    "\n",
    "    planner_prompt = (\n",
    "        'Output EXACTLY this template, no extra text:\\n'\n",
    "        'PLAN:\\n'\n",
    "        '- ...\\n'\n",
    "        '- ...\\n'\n",
    "        '- ...\\n'\n",
    "        '- ...\\n'\n",
    "        '- ...\\n'\n",
    "        'DIGEST:\\n'\n",
    "        '- ...\\n'\n",
    "        '<<<END>>>\\n'\n",
    "        'Rules:\\n'\n",
    "        '- PLAN has 5-8 bullets, each <=120 chars.\\n '\n",
    "        '- Each PLAN bullet must be an action (verb-first), not a conclusion.\\n '\n",
    "        '- DIGEST is exactly 1 bullet, <=256 chars.\\n '\n",
    "        '- The line \"<<<END>>>\" must be on its own line (no other text on that line).\\n '\n",
    "        '- \"PLAN:\" and \"DIGEST:\" must NOT be prefixed by \"-\" or \"*\". They must be standalone headers.\\n '\n",
    "        '- Plan only: do NOT compute the final answer, do NOT include any final numeric result, do NOT write \\\\boxed{}.\\n '\n",
    "        '- No explanations, no meta talk, no code.\\n '\n",
    "        '- Do NOT include the words: analysis, final, assistant, user, rewrite, template.\\n '    \n",
    "        '- Must include at least ONE new verification/search tactic not used in the immediately previous attempt '\n",
    "        '(e.g., widen a bound, change enumeration order, add an independent check, prove a missing lemma).\\n'\n",
    "        '- Must explicitly list the most recent failure mode (from history) AND one concrete prevention rule '\n",
    "        '(imperative form, e.g., \"Do not print huge ints; use bit_length/mod\", \"Do not assume X without a small-case check\").'\n",
    "        '- DIGEST must NOT restate PLAN bullets; it should compress the approach into ONE actionable sentence.\\n'\n",
    "        '- DIGEST must NOT contain the marker \"<<<END>>>\". The marker line is standalone.\\n'\n",
    "    )\n",
    "\n",
    "    plan_enabled = False\n",
    "    digest_enabled = False\n",
    "    attempt_tags = True         # 是否启用结构化标签\n",
    "    plan_sanitize = True\n",
    "    plan_context_limit = 1536\n",
    "    plan_digest_limit = 256\n",
    "    plan_max_tokens = 384\n",
    "    plan_temperature = 0.2\n",
    "    plan_dedup_ratio = 0.88         # digest 相似度阈值（越高越严格）\n",
    "    plan_focus_rotate = True        # 是否给每个 plan 一个不同“侧重点提示”\n",
    "    plan_focus_list = [\n",
    "        \"algebraic-derivation\",     # 偏证明/推导\n",
    "        \"small-scan\",               # 偏小范围打表验证（<=200/2000）\n",
    "        \"number-theory\",            # 偏因子分解/整除结构\n",
    "        \"mod-arithmetic\",           # 偏模运算/同余验证\n",
    "    ]\n",
    "\n",
    "    warmup_compile_cache = False\n",
    "    compile_cache_src = \"/kaggle/input/gpt-oss-120b-cache-compile/torch_compile_cache\"\n",
    "    compile_cache_dst = \"/root/.cache/vllm/torch_compile_cache\"\n",
    "\n",
    "    vllm_port = 8000\n",
    "    served_model_name = 'gpt-oss'\n",
    "    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
    "\n",
    "    # not working!! P5 => 62140 ❌\n",
    "    # served_model_name = 'gpt-oss-sft'\n",
    "    # model_path = '/kaggle/input/gpt-oss-sft-aimo3/transformers/default/1'\n",
    "    \n",
    "    kv_cache_dtype = 'fp8_e4m3'\n",
    "    dtype = 'auto'\n",
    "\n",
    "    high_problem_timeout = 900\n",
    "    base_problem_timeout = 300\n",
    "\n",
    "    notebook_limit = 17400\n",
    "    server_timeout = 180\n",
    "\n",
    "    session_timeout = 960\n",
    "    jupyter_timeout = 7    # 6\n",
    "    sandbox_timeout = 3\n",
    "\n",
    "    stream_interval = 200\n",
    "    context_tokens = 65536\n",
    "    buffer_tokens = 512\n",
    "    search_tokens = 256\n",
    "    top_logprobs = 5\n",
    "    batch_size = 256\n",
    "    early_stop = 4\n",
    "    attempts = 8\n",
    "    max_waves = 2\n",
    "    workers = 16\n",
    "    turns = 128\n",
    "    seed = 42\n",
    "\n",
    "    gpu_memory_utilization = 0.96\n",
    "    temperature = 1.0\n",
    "    min_p = 0.02\n",
    "\n",
    "    vote_policy = 'score'  # 'calls', 'score'\n",
    "    penalty_err_enabled = False\n",
    "    penalty_err_alpha = 0.1\n",
    "\n",
    "    # --- NEW: vote bonus for \"verification signal\" ---\n",
    "    # none | calls | ok_calls\n",
    "    bonus_py_call = 'none'\n",
    "    bonus_py_alpha = 0.05           # 每个(有效)py call 的乘法奖励系数\n",
    "    bonus_py_cap = 4                # 最多计入多少个 call（避免奖励刷太多）\n",
    "\n",
    "    debug = True\n",
    "    debug_req = True\n",
    "    debug_resp = True\n",
    "    debug_limit = 3000\n",
    "    debug_cols = ['Log', 'Plan', 'PlanRaw', 'PlanSanitized', 'PlanDigest']\n",
    "\n",
    "def _delete(name: str):\n",
    "    if name is not None and name != \"\" and name in globals(): del globals()[name]\n",
    "\n",
    "def _fmt_time(seconds: float) -> str:\n",
    "    s = int(round(max(0.0, seconds)))\n",
    "    m, s = divmod(s, 60)\n",
    "    return f\"{m}:{s:02d}\"\n",
    "\n",
    "def _format_markdown(text: str, mode: str = \"quote\") -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    lines = text.split('\\n')\n",
    "    escaped_lines = [f\"\\\\{line}\" if line.startswith('#') else line for line in lines]\n",
    "    processed_text = '\\n'.join(escaped_lines)\n",
    "    if mode in [\"markdown\", \"text\", \"python\"]:\n",
    "        return f\"```{mode}\\n{processed_text}\\n```\\n\"\n",
    "    if mode == \"quote\":\n",
    "        return '\\n'.join([f\"> {line}\" for line in escaped_lines]) + \"\\n\"\n",
    "    if mode == \"\":\n",
    "        return processed_text + \"\\n\"\n",
    "    return f\"```\\n{processed_text}\\n```\\n\"\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ebe421",
   "metadata": {
    "papermill": {
     "duration": 0.003016,
     "end_time": "2026-01-25T23:24:24.843253",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.840237",
     "status": "completed"
    },
    "tags": [
     "ignore"
    ]
   },
   "source": [
    "### Sandbox & Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8684a05d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T23:24:24.850049Z",
     "iopub.status.busy": "2026-01-25T23:24:24.849814Z",
     "iopub.status.idle": "2026-01-25T23:24:24.860259Z",
     "shell.execute_reply": "2026-01-25T23:24:24.859933Z"
    },
    "papermill": {
     "duration": 0.014711,
     "end_time": "2026-01-25T23:24:24.860935",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.846224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Sandbox:\n",
    "\n",
    "    _port_lock = threading.Lock()\n",
    "    _next_port = 50000\n",
    "\n",
    "    @classmethod\n",
    "    def _get_next_ports(cls, count: int = 5) -> list[int]:\n",
    "        with cls._port_lock:\n",
    "            ports = list(range(cls._next_port, cls._next_port + count))\n",
    "            cls._next_port += count\n",
    "            return ports\n",
    "\n",
    "    def __init__(self, timeout: float):\n",
    "        self._default_timeout = timeout\n",
    "        self._owns_kernel = False\n",
    "        self._client = None\n",
    "        self._km = None\n",
    "        \n",
    "        ports = self._get_next_ports(5)\n",
    "\n",
    "        env = os.environ.copy()\n",
    "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n",
    "        env['JUPYTER_PLATFORM_DIRS'] = '1'\n",
    "        env['PYTHONWARNINGS'] = 'ignore'\n",
    "        env['MPLBACKEND'] = 'Agg'\n",
    "\n",
    "        self._km = KernelManager()\n",
    "        self._km.shell_port = ports[0]\n",
    "        self._km.iopub_port = ports[1]\n",
    "        self._km.stdin_port = ports[2]\n",
    "        self._km.hb_port = ports[3]\n",
    "        self._km.control_port = ports[4]\n",
    "        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n",
    "\n",
    "        self._client = self._km.blocking_client()\n",
    "        self._client.start_channels()\n",
    "        self._client.wait_for_ready(timeout=self._default_timeout)\n",
    "        self._owns_kernel = True\n",
    "\n",
    "        self._helper_imports = \"\"\"\n",
    "import math\n",
    "import numpy\n",
    "import sympy\n",
    "import itertools\n",
    "import collections\n",
    "import mpmath\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from sympy.ntheory.modular import crt\n",
    "from sympy import cyclotomic_poly\n",
    "\n",
    "sympy.crt = crt\n",
    "sympy.npolycyclotomic = cyclotomic_poly\n",
    "math.isprime = sympy.isprime\n",
    "\n",
    "mpmath.mp.dps = 64\n",
    "\n",
    "# prevent huge number print error!\n",
    "sys.set_int_max_str_digits(0)  # unlimited (best-effort)\n",
    "# avoid recursion stack overflow!\n",
    "sys.setrecursionlimit(20000)\n",
    "\n",
    "from joblib import Memory\n",
    "from functools import lru_cache\n",
    "\n",
    "memory = Memory(location='aimo3_cache', verbose=0)\n",
    "\"\"\"\n",
    "\n",
    "        self._helper_methods = \"\"\"\n",
    "# ---- safe formatting helpers ----\n",
    "def _safe_int_str(n):\n",
    "    try:\n",
    "        return str(n)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return f\"<int bit_length={int(n.bit_length())}>\"\n",
    "        except Exception:\n",
    "            return \"<int>\"\n",
    "\n",
    "def _safe_atom(x):\n",
    "    if isinstance(x, int):\n",
    "        # Avoid huge-int str; represent very large ints compactly\n",
    "        bl = x.bit_length()\n",
    "        if bl >= 4096:\n",
    "            return f\"<int bit_length={bl}>\"\n",
    "        return x\n",
    "    return x\n",
    "\n",
    "def head(x, k=10):\n",
    "    \\\"\\\"\\\"Safe preview: works for dict/list/tuple/set/pandas/polars objects.\\\"\\\"\\\"\n",
    "    try:\n",
    "        if isinstance(x, dict):\n",
    "            items = list(x.items())[:k]\n",
    "            return [(_safe_atom(a), _safe_atom(b)) for a, b in items]\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            return [_safe_atom(v) for v in x[:k]]\n",
    "        if isinstance(x, set):\n",
    "            return [_safe_atom(v) for v in list(x)[:k]]\n",
    "        # pandas / polars DataFrame-like\n",
    "        if hasattr(x, \"head\"):\n",
    "            return x.head(k)\n",
    "    except Exception as _e:\n",
    "        return f\"<head error: {_e}>\"\n",
    "    # fallback: string (safe)\n",
    "    try:\n",
    "        s = repr(x)\n",
    "        return s[:2000] + (\"...\" if len(s) > 2000 else \"\")\n",
    "    except Exception:\n",
    "        return f\"<{type(x).__name__}>\"\n",
    "\n",
    "def head2(obj, n=5):\n",
    "    if isinstance(obj, dict):\n",
    "        return dict(list(obj.items())[:n])\n",
    "    try:\n",
    "        return obj[:n]\n",
    "    except:\n",
    "        return obj\n",
    "\n",
    "def pint(x, mod=None, name=None):\n",
    "    \\\"\\\"\\\"Safe summary for ints: avoids huge-int str.\\\"\\\"\\\"\n",
    "    if not isinstance(x, int):\n",
    "        print(f\"{name + ': ' if name else ''}{type(x).__name__}\")\n",
    "        return\n",
    "    bl = x.bit_length()\n",
    "    if mod is None:\n",
    "        msg = f\"int(bit_length={bl})\"\n",
    "    else:\n",
    "        try:\n",
    "            msg = f\"int(bit_length={bl}, mod={mod} => {x % mod})\"\n",
    "        except Exception:\n",
    "            msg = f\"int(bit_length={bl}, mod={mod} => <error>)\"\n",
    "    print(f\"{name + ': ' if name else ''}{msg}\")\n",
    "\n",
    "def p(x, name=None, k=10):\n",
    "    \\\"\\\"\\\"Safe print: dict/list previews via head(); ints via pint(); otherwise repr-trunc.\\\"\\\"\\\"\n",
    "    if isinstance(x, int):\n",
    "        pint(x, name=name)\n",
    "        return\n",
    "    if isinstance(x, dict):\n",
    "        try:\n",
    "            h = head(x, k)\n",
    "            print(f\"{name + ': ' if name else ''}dict(len={len(x)}), head={h}\")\n",
    "        except Exception as _e:\n",
    "            print(f\"{name + ': ' if name else ''}dict(<error: {_e}>)\")\n",
    "        return\n",
    "    if isinstance(x, (list, tuple, set)):\n",
    "        try:\n",
    "            h = head(x, k)\n",
    "            print(f\"{name + ': ' if name else ''}{type(x).__name__}(len={len(x)}), head={h}\")\n",
    "        except Exception as _e:\n",
    "            print(f\"{name + ': ' if name else ''}{type(x).__name__}(<error: {_e}>)\")\n",
    "        return\n",
    "    try:\n",
    "        s = repr(x)\n",
    "        if len(s) > 2000:\n",
    "            s = s[:2000] + \"...\"\n",
    "        print(f\"{name + ': ' if name else ''}{s}\")\n",
    "    except Exception:\n",
    "        print(f\"{name + ': ' if name else ''}<{type(x).__name__}>\")\n",
    "\"\"\"\n",
    "\n",
    "        self.execute([self._helper_imports, self._helper_methods])\n",
    "\n",
    "    def _format_error(self, traceback: list[str]) -> str:\n",
    "        clean_lines = []\n",
    "        for frame in traceback:\n",
    "            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n",
    "            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n",
    "                continue\n",
    "            clean_lines.append(clean_frame)\n",
    "        return ''.join(clean_lines)\n",
    "\n",
    "    def execute(self, code: str | Iterable[str], timeout: float | None = None) -> str:\n",
    "        client = self._client\n",
    "        effective_timeout = timeout or self._default_timeout\n",
    "\n",
    "        if isinstance(code, (list, tuple)):\n",
    "            code = \"\\n\\n\".join(\n",
    "                c.rstrip() for c in code if isinstance(c, str) and c.strip()\n",
    "            )\n",
    "\n",
    "        msg_id = client.execute(\n",
    "            code, \n",
    "            store_history=True, \n",
    "            allow_stdin=False, \n",
    "            stop_on_error=False\n",
    "        )\n",
    "\n",
    "        stdout_parts = []\n",
    "        stderr_parts = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            if elapsed > effective_timeout:\n",
    "                self._km.interrupt_kernel()\n",
    "\n",
    "                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n",
    "\n",
    "            try:\n",
    "                msg = client.get_iopub_msg(timeout=1.0)\n",
    "\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "\n",
    "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
    "                continue\n",
    "\n",
    "            msg_type = msg.get('msg_type')\n",
    "            content = msg.get('content', {})\n",
    "\n",
    "            if msg_type == 'stream':\n",
    "                text = content.get('text', '')\n",
    "\n",
    "                if content.get('name') == 'stdout':\n",
    "                    stdout_parts.append(text)\n",
    "\n",
    "                else:\n",
    "                    stderr_parts.append(text)\n",
    "\n",
    "            elif msg_type == 'error':\n",
    "                traceback_list = content.get('traceback', [])\n",
    "\n",
    "                stderr_parts.append(self._format_error(traceback_list))\n",
    "\n",
    "            elif msg_type in {'execute_result', 'display_data'}:\n",
    "                data = content.get('data', {})\n",
    "                text = data.get('text/plain')\n",
    "\n",
    "                if text:\n",
    "                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n",
    "\n",
    "            elif msg_type == 'status':\n",
    "                if content.get('execution_state') == 'idle':\n",
    "                    break\n",
    "\n",
    "        stdout = ''.join(stdout_parts)\n",
    "        stderr = ''.join(stderr_parts)\n",
    "\n",
    "        if stderr:\n",
    "            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n",
    "\n",
    "        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n",
    "\n",
    "    def close(self):\n",
    "        with contextlib.suppress(Exception):\n",
    "            if self._client:\n",
    "                self._client.stop_channels()\n",
    "\n",
    "        if self._owns_kernel and self._km is not None:\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.shutdown_kernel(now=True)\n",
    "\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.cleanup_resources()\n",
    "\n",
    "    def reset(self):\n",
    "        self.execute(['%reset -f\\n', self._helper_imports, self._helper_methods])\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9e2259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T23:24:24.867102Z",
     "iopub.status.busy": "2026-01-25T23:24:24.866961Z",
     "iopub.status.idle": "2026-01-25T23:24:24.873829Z",
     "shell.execute_reply": "2026-01-25T23:24:24.873478Z"
    },
    "papermill": {
     "duration": 0.010736,
     "end_time": "2026-01-25T23:24:24.874528",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.863792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Tool:\n",
    "\n",
    "    def __init__(self, cfg, sandbox=None):\n",
    "        self.cfg = cfg\n",
    "        self._local_jupyter_timeout = cfg.jupyter_timeout\n",
    "        self._tool_prompt = cfg.tool_prompt\n",
    "        self._jupyter_session = sandbox\n",
    "        self._owns_session = sandbox is None\n",
    "        self._execution_lock = threading.Lock()\n",
    "        self._init_lock = threading.Lock()\n",
    "\n",
    "    def _ensure_session(self):\n",
    "        if self._jupyter_session is None:\n",
    "            with self._init_lock:\n",
    "                if self._jupyter_session is None:\n",
    "                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n",
    "\n",
    "    def _auto_print_last_line(self, code: str):\n",
    "        tree = ast.parse(code)\n",
    "        if not tree.body: return code\n",
    "        last_node = tree.body[-1]\n",
    "        \n",
    "        # if it's a line like: a = 1\n",
    "        if isinstance(last_node, ast.Assign):\n",
    "            # construct print(targets)\n",
    "            print_node = ast.Expr(\n",
    "                value=ast.Call(\n",
    "                    func=ast.Name(id='print', ctx=ast.Load()),\n",
    "                    args=[t for t in last_node.targets],\n",
    "                    keywords=[]\n",
    "                )\n",
    "            )\n",
    "            # append to the code block\n",
    "            tree.body.append(print_node)\n",
    "            \n",
    "        return ast.unparse(tree)\n",
    "\n",
    "    def _ensure_last_print(self, code: str) -> str:\n",
    "        lines = code.strip().split('\\n')\n",
    "        if not lines:\n",
    "            return code\n",
    "        last_line = lines[-1].strip()\n",
    "\n",
    "        if 'print' in last_line or 'import' in last_line:\n",
    "            return code\n",
    "\n",
    "        if not last_line:\n",
    "            return code\n",
    "\n",
    "        if last_line.startswith('#'):\n",
    "            return code\n",
    "\n",
    "        # this might modify the code to print(a = 5) which is wrong\n",
    "        # lines[-1] = 'print(' + last_line + ')'\n",
    "        return self._auto_print_last_line(code)\n",
    "\n",
    "    @property\n",
    "    def instruction(self) -> str:\n",
    "        return self._tool_prompt\n",
    "\n",
    "    @property\n",
    "    def tool_config(self) -> ToolNamespaceConfig:\n",
    "        return ToolNamespaceConfig(\n",
    "            name='python', \n",
    "            description=self.instruction, \n",
    "            tools=[]\n",
    "        )\n",
    "\n",
    "    def _make_response(self, output: str, channel: str | None = None) -> Message:\n",
    "        content = TextContent(text=output)\n",
    "        author = Author(role=Role.TOOL, name='python')\n",
    "        message = Message(author=author, content=[content]).with_recipient('assistant')\n",
    "\n",
    "        if channel:\n",
    "            message = message.with_channel(channel)\n",
    "\n",
    "        return message\n",
    "\n",
    "    def process_sync_plus(self, message: Message, turn: int = 0, logs: list = None) -> list[Message]:\n",
    "        self._ensure_session()\n",
    "        raw_script = message.content[0].text\n",
    "        final_script = self._ensure_last_print(raw_script)\n",
    "\n",
    "        if self.cfg.debug and logs is not None:\n",
    "            logs.append(f\"### Turn {turn} - python raw_script & final_script:\")\n",
    "            logs.append(_format_markdown(raw_script))\n",
    "            logs.append(_format_markdown(final_script))\n",
    "        \n",
    "        with self._execution_lock:\n",
    "            try:\n",
    "                output = self._jupyter_session.execute(final_script)\n",
    "\n",
    "            except TimeoutError as exc:\n",
    "                output = f'[ERROR] {exc}'\n",
    "\n",
    "        return [self._make_response(output, channel=message.channel)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd853e7",
   "metadata": {
    "papermill": {
     "duration": 0.003112,
     "end_time": "2026-01-25T23:24:24.880561",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.877449",
     "status": "completed"
    },
    "tags": [
     "ignore"
    ]
   },
   "source": [
    "### Logger & Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8e94b",
   "metadata": {
    "papermill": {
     "duration": 0.002654,
     "end_time": "2026-01-25T23:24:24.886104",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.883450",
     "status": "completed"
    },
    "tags": [
     "ignore"
    ]
   },
   "source": [
    "#### AIMO3Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ad6c2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T23:24:24.891934Z",
     "iopub.status.busy": "2026-01-25T23:24:24.891817Z",
     "iopub.status.idle": "2026-01-25T23:24:24.897873Z",
     "shell.execute_reply": "2026-01-25T23:24:24.897538Z"
    },
    "papermill": {
     "duration": 0.00986,
     "end_time": "2026-01-25T23:24:24.898594",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.888734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Logger:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def get_debug_snippet(self, text: str) -> str:\n",
    "        limit = self.cfg.debug_limit\n",
    "        if not text or len(text) <= limit:\n",
    "            return text or \"\"\n",
    "        head = text[:100]\n",
    "        tail_len = limit - 100\n",
    "        tail = text[-tail_len:]\n",
    "        return f\"{head}\\n ... \\n{tail}\"\n",
    "\n",
    "    def log_planner_block(self, plan_raw: str, plan_sanitized: str, plan_digest: str) -> str:\n",
    "        raw_snip = self.get_debug_snippet(plan_raw)\n",
    "        san_snip = self.get_debug_snippet(plan_sanitized)\n",
    "        digest = plan_digest.strip()\n",
    "\n",
    "        out = []\n",
    "        out.append(\"### Planner Output (raw)\\n\")\n",
    "        out.append(_format_markdown(raw_snip, mode=\"text\"))\n",
    "        out.append(\"### Planner Output (sanitized)\\n\")\n",
    "        out.append(_format_markdown(san_snip, mode=\"text\"))\n",
    "        out.append(\"### Plan Digest\\n\")\n",
    "        out.append(_format_markdown(digest, mode=\"text\"))\n",
    "        return \"\".join(out)\n",
    "\n",
    "    def write_debug_logs(self, detailed_results, vote_dataframe, problem, problem_id=\"UNK\", problem_time=\"\"):\n",
    "        if not self.cfg.debug:\n",
    "            return\n",
    "        try:\n",
    "            summary_lines = [\"\\n## Summary Stats\\n\"]\n",
    "            if detailed_results:\n",
    "                df = pd.DataFrame(detailed_results)\n",
    "                cols = [c for c in df.columns if c not in self.cfg.debug_cols]\n",
    "                summary_lines.append(df[cols].to_markdown(index=False))\n",
    "                summary_lines.append(\"\\n\\n\")\n",
    "\n",
    "            if not vote_dataframe.empty:\n",
    "                summary_lines.append(\"## Vote Counts\\n\")\n",
    "                summary_lines.append(vote_dataframe.to_markdown(index=False))\n",
    "                summary_lines.append(\"\\n\")\n",
    "\n",
    "            final_log_content = [f\"# Problem ID: {problem_id}\\n\"]\n",
    "            final_log_content.append(f\"Problem spent time: **{problem_time}**\\n\\n\")\n",
    "            final_log_content.append(f\"**Problem:**\\n{_format_markdown(problem)}\\n\")\n",
    "            final_log_content.append(f\"**system_prompt:**\\n{_format_markdown(self.cfg.system_prompt)}\\n\")\n",
    "            final_log_content.append(f\"**tool_prompt:**\\n{_format_markdown(self.cfg.tool_prompt)}\\n\")\n",
    "            final_log_content.append(f\"**preference_prompt:**\\n{_format_markdown(self.cfg.preference_prompt)}\\n\")\n",
    "            final_log_content.append(f\"**planner_system_prompt:**\\n{_format_markdown(self.cfg.planner_system_prompt)}\\n\")\n",
    "            final_log_content.append(f\"**planner_prompt:**\\n{_format_markdown(self.cfg.planner_prompt)}\\n\")\n",
    "            final_log_content.append(f\"**CFG** > plan_enabled: **{self.cfg.plan_enabled}**, \"\n",
    "                                     f\"digest_enabled: **{self.cfg.digest_enabled}**, \"\n",
    "                                     f\"penalty_err_enabled: **{self.cfg.penalty_err_enabled}**, \"\n",
    "                                     f\"penalty_err_alpha: **{self.cfg.penalty_err_alpha}**, \"\n",
    "                                     f\"bonus_py_call: **{self.cfg.bonus_py_call}**, \"\n",
    "                                     f\"bonus_py_alpha: **{self.cfg.bonus_py_alpha}**, \"\n",
    "                                     f\"served_model_name: **{self.cfg.served_model_name}**\\n\")\n",
    "            final_log_content.extend(summary_lines)\n",
    "            final_log_content.append(\"\\n===\\n\")\n",
    "\n",
    "            sorted_results = sorted(detailed_results, key=lambda x: x['Attempt'])\n",
    "            for res in sorted_results:\n",
    "                log_content = res.get('Log', '')\n",
    "                if log_content:\n",
    "                    final_log_content.append(log_content)\n",
    "                    final_log_content.append(\"\\n===\\n\")\n",
    "\n",
    "            output_path = f\"{problem_id}.md\"\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\".join(final_log_content))\n",
    "            print(f\"Debug log written to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to write debug log: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5af86",
   "metadata": {
    "papermill": {
     "duration": 0.003584,
     "end_time": "2026-01-25T23:24:24.905151",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.901567",
     "status": "completed"
    },
    "tags": [
     "ignore"
    ]
   },
   "source": [
    "#### AIMO3Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6414509f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T23:24:24.911872Z",
     "iopub.status.busy": "2026-01-25T23:24:24.911728Z",
     "iopub.status.idle": "2026-01-25T23:24:24.920411Z",
     "shell.execute_reply": "2026-01-25T23:24:24.920063Z"
    },
    "papermill": {
     "duration": 0.012831,
     "end_time": "2026-01-25T23:24:24.921109",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.908278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Server:\n",
    "    def __init__(self, cfg, port: int = 8000):\n",
    "        self.cfg = cfg\n",
    "        self.port = getattr(self.cfg, \"vllm_port\", port)\n",
    "        self.base_url = f\"http://localhost:{self.port}/v1\"\n",
    "        self.api_key = \"sk-local\"\n",
    "        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key, timeout=self.cfg.session_timeout)\n",
    "        self.log_file = None\n",
    "        \n",
    "        self._preload_model_weights()\n",
    "        self.process = self._start()\n",
    "        self._wait_ready()\n",
    "\n",
    "    # copy vLLM compile cache if available\n",
    "    def _warmup_compile_cache(self):\n",
    "        if not getattr(self.cfg, \"warmup_compile_cache\", False):\n",
    "            return\n",
    "        src = getattr(self.cfg, \"compile_cache_src\", \"\")\n",
    "        dst = getattr(self.cfg, \"compile_cache_dst\", \"\")\n",
    "        if not src or not dst:\n",
    "            return\n",
    "        if os.path.exists(src):\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "            try:\n",
    "                shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def _preload_model_weights(self) -> None:\n",
    "        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        files_to_load = []\n",
    "        total_size = 0\n",
    "    \n",
    "        for root, _, files in os.walk(self.cfg.model_path):\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "    \n",
    "                if os.path.isfile(file_path):\n",
    "                    files_to_load.append(file_path)\n",
    "                    total_size += os.path.getsize(file_path)\n",
    "    \n",
    "        def _read_file(path: str) -> None:\n",
    "            with open(path, 'rb') as file_object:\n",
    "                while file_object.read(1024 * 1024 * 1024):\n",
    "                    pass\n",
    "    \n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
    "            list(executor.map(_read_file, files_to_load))\n",
    "    \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n",
    "    \n",
    "    def _start(self) -> subprocess.Popen:\n",
    "        if self.cfg.warmup_compile_cache:\n",
    "            self._warmup_compile_cache()\n",
    "\n",
    "        cmd = [\n",
    "            sys.executable, '-m', 'vllm.entrypoints.openai.api_server', \n",
    "            '--seed', str(self.cfg.seed), \n",
    "            '--model', self.cfg.model_path, \n",
    "            '--served-model-name', self.cfg.served_model_name, \n",
    "            '--tensor-parallel-size', '1', \n",
    "            '--max-num-seqs', str(self.cfg.batch_size), \n",
    "            '--gpu-memory-utilization', str(self.cfg.gpu_memory_utilization), \n",
    "            '--host', '0.0.0.0', \n",
    "            '--port', str(self.port), \n",
    "            '--dtype', self.cfg.dtype, \n",
    "            '--kv-cache-dtype', self.cfg.kv_cache_dtype, \n",
    "            '--max-model-len', str(self.cfg.context_tokens), \n",
    "            '--stream-interval', str(self.cfg.stream_interval), \n",
    "            '--async-scheduling', \n",
    "            '--disable-log-stats', \n",
    "            '--enable-prefix-caching'\n",
    "        ]\n",
    "    \n",
    "        self.log_file = open('vllm_server.log', 'w')\n",
    "        return subprocess.Popen(\n",
    "            cmd, stdout=self.log_file, stderr=subprocess.STDOUT, start_new_session=True\n",
    "        )\n",
    "    \n",
    "    def _wait_ready(self):\n",
    "        print('Waiting for vLLM server...')\n",
    "        start_time = time.time()\n",
    "    \n",
    "        for _ in range(self.cfg.server_timeout):\n",
    "            return_code = self.process.poll()\n",
    "    \n",
    "            if return_code is not None:\n",
    "                self.log_file.flush()\n",
    "                with open('vllm_server.log', 'r') as log_file:\n",
    "                    logs = log_file.read()\n",
    "                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n",
    "    \n",
    "            try:\n",
    "                self.client.models.list()\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n",
    "                return\n",
    "            except Exception:\n",
    "                time.sleep(1)\n",
    "        raise RuntimeError('Server failed to start (timeout).\\n')\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.process is not None:\n",
    "            with contextlib.suppress(Exception):\n",
    "                self.process.terminate()\n",
    "                self.process.wait()\n",
    "        if self.log_file is not None:\n",
    "            with contextlib.suppress(Exception):\n",
    "                self.log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6818e",
   "metadata": {
    "papermill": {
     "duration": 0.003007,
     "end_time": "2026-01-25T23:24:24.927102",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.924095",
     "status": "completed"
    },
    "tags": [
     "ignore"
    ]
   },
   "source": [
    "### Planner & Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce0a7e",
   "metadata": {
    "papermill": {
     "duration": 0.002639,
     "end_time": "2026-01-25T23:24:24.932525",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.929886",
     "status": "completed"
    },
    "tags": [
     "ignore"
    ]
   },
   "source": [
    "#### AIMO3Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ff2b7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T23:24:24.939144Z",
     "iopub.status.busy": "2026-01-25T23:24:24.939007Z",
     "iopub.status.idle": "2026-01-25T23:24:24.954762Z",
     "shell.execute_reply": "2026-01-25T23:24:24.954426Z"
    },
    "papermill": {
     "duration": 0.020019,
     "end_time": "2026-01-25T23:24:24.955416",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.935397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Planner:\n",
    "    \"\"\"\n",
    "    Robust planner for Harmony-format models:\n",
    "    - Collect token_ids and parse via harmony encoding to avoid 'assistantfinal' artifacts.\n",
    "    - Enforce PLAN/DIGEST template with at most one repair.\n",
    "    - Never return empty digest; never return non-bullet long paragraphs as plan.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, port: int = 8000):\n",
    "        self.cfg = cfg\n",
    "        self.port = getattr(self.cfg, \"vllm_port\", port)\n",
    "        self.base_url = f\"http://localhost:{self.port}/v1\"\n",
    "        self.api_key = \"sk-local\"\n",
    "        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key, timeout=self.cfg.session_timeout)\n",
    "\n",
    "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
    "\n",
    "    # ----------------- helpers -----------------\n",
    "\n",
    "    def _build_history_block(self, history: list[dict]) -> str:\n",
    "        if not history:\n",
    "            return \"ATTEMPT HISTORY: (none)\\n\"\n",
    "        keep = getattr(self.cfg, \"planner_history_keep\", 8)\n",
    "        maxc = getattr(self.cfg, \"planner_digest_max_chars\", 256)\n",
    "        lines = [\"ATTEMPT HISTORY (structured):\"]\n",
    "        for r in history[-keep:]:\n",
    "            dig = (r.get(\"PlanDigest\") or \"\").replace(\"\\n\", \" \").strip()\n",
    "            if len(dig) > maxc:\n",
    "                dig = dig[:maxc] + \"...\"\n",
    "            lines.append(\n",
    "                f\"- Attempt {r.get('Attempt')}: \"\n",
    "                f\"Answer={r.get('Answer')}, Entropy={float(r.get('Entropy', 1e9)):.3f}, \"\n",
    "                f\"PyCalls={int(r.get('Python Calls', 0) or 0)}, PyErr={int(r.get('Python Errors', 0) or 0)}; \"\n",
    "                f\"PlanDigest={dig}\"\n",
    "            )\n",
    "        return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "    def _render_prompt_ids(self, system_prompt: str, user_text: str):\n",
    "        sys_content = (\n",
    "            SystemContent.new()\n",
    "            .with_model_identity(system_prompt)\n",
    "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.LOW)\n",
    "            .with_tools(ToolNamespaceConfig(name=\"none\", description=\"\", tools=[]))\n",
    "        )\n",
    "        messages = [\n",
    "            Message.from_role_and_content(Role.SYSTEM, sys_content),\n",
    "            Message.from_role_and_content(Role.USER, user_text),\n",
    "        ]\n",
    "        conv = Conversation.from_messages(messages)\n",
    "        return self.encoding.render_conversation_for_completion(conv, Role.ASSISTANT)\n",
    "\n",
    "    def _decode_from_token_ids(self, token_ids: list[int]) -> str:\n",
    "        if not token_ids:\n",
    "            return \"\"\n",
    "        msgs = self.encoding.parse_messages_from_completion_tokens(token_ids, Role.ASSISTANT)\n",
    "        # concatenate all assistant text contents\n",
    "        parts = []\n",
    "        for m in msgs:\n",
    "            if not m.content:\n",
    "                continue\n",
    "            for c in m.content:\n",
    "                if hasattr(c, \"text\") and c.text:\n",
    "                    parts.append(c.text)\n",
    "        return \"\".join(parts).strip()\n",
    "\n",
    "    def _stream_completion(self, system_prompt: str, user_text: str, seed: int, max_tokens: int, temperature: float):\n",
    "        prompt_ids = self._render_prompt_ids(system_prompt, user_text)\n",
    "        req_timeout = min(30.0, float(self.cfg.session_timeout))\n",
    "        try:\n",
    "            stream = self.client.completions.create(\n",
    "                model=self.cfg.served_model_name,\n",
    "                temperature=float(temperature),\n",
    "                max_tokens=int(max_tokens),\n",
    "                prompt=prompt_ids,\n",
    "                seed=int(seed),\n",
    "                stream=True,\n",
    "                timeout=req_timeout,\n",
    "                extra_body={\n",
    "                    \"min_p\": self.cfg.min_p,\n",
    "                    \"stop_token_ids\": self.stop_token_ids,\n",
    "                    \"return_token_ids\": True,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # 返回空 planner，让 solver 走 fallback/无 plan 路径\n",
    "            return \"\", f\"[PLANNER_STREAM_CREATE_ERROR] {e}\"\n",
    "\n",
    "        token_buf = []\n",
    "        text_buf = []\n",
    "        try:\n",
    "            for chunk in stream:\n",
    "                # token_ids is the reliable path for harmony parsing\n",
    "                tids = getattr(chunk.choices[0], \"token_ids\", None)\n",
    "                if tids:\n",
    "                    token_buf.extend(tids)\n",
    "                t = chunk.choices[0].text\n",
    "                if t:\n",
    "                    text_buf.append(t)\n",
    "        finally:\n",
    "            stream.close()\n",
    "\n",
    "        # raw text is only for debugging\n",
    "        raw_text = \"\".join(text_buf).strip()\n",
    "        parsed_text = self._decode_from_token_ids(token_buf)\n",
    "        return parsed_text, raw_text\n",
    "\n",
    "    def _bulletize(self, text: str, max_lines: int = 8) -> str:\n",
    "        t = (text or \"\").strip()\n",
    "        if not t:\n",
    "            return \"\"\n",
    "        lines = [ln.strip() for ln in t.splitlines() if ln.strip()]\n",
    "        out = []\n",
    "        for ln in lines[:max_lines]:\n",
    "            ln = re.sub(r\"^\\s*(analysis|final|commentary)\\s*\", \"\", ln, flags=re.IGNORECASE).strip()\n",
    "            if not ln:\n",
    "                continue\n",
    "            if not ln.startswith((\"-\", \"*\", \"•\")):\n",
    "                ln = \"- \" + ln\n",
    "            out.append(ln)\n",
    "        return \"\\n\".join(out).strip()\n",
    "\n",
    "    def _strip_answerish_lines(self, plan_part: str) -> str:\n",
    "        lines = plan_part.splitlines()\n",
    "        bad_kw = (\"\\\\boxed\", \"remainder\", \"mod\", \"final answer\", \"=\", \"so each\", \"therefore\", \"template\", \"assistantfinal\", \"assistant\")\n",
    "        out = []\n",
    "        for ln in lines:\n",
    "            low = ln.lower()\n",
    "            # 只过滤“答案型总结句”，保留正常数学等式的可能性会有误伤，但在 planner 场景利大于弊\n",
    "            if any(k in low for k in bad_kw) and any(ch.isdigit() for ch in ln):\n",
    "                continue\n",
    "            out.append(ln)\n",
    "        return \"\\n\".join(out).strip()\n",
    "\n",
    "    def _extract_plan_and_digest(self, text: str) -> tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Accept headers with optional bullet prefixes:\n",
    "          PLAN: or - PLAN:\n",
    "          DIGEST: or - DIGEST:\n",
    "        \"\"\"\n",
    "        t = (text or \"\").strip()\n",
    "        if not t:\n",
    "            return \"\", \"\"\n",
    "\n",
    "        # normalize <<<END>>> cut\n",
    "        t = re.split(r\"(?m)^\\s*<<<END>>>\\s*$\", t)[0].strip()\n",
    "\n",
    "        # find headers (allow optional bullet prefix)\n",
    "        plan_hdr = re.search(r\"(?im)^\\s*(?:[-*•]\\s*)?PLAN\\s*:\\s*$\", t)\n",
    "        dig_hdr = re.search(r\"(?im)^\\s*(?:[-*•]\\s*)?DIGEST\\s*:\\s*$\", t)\n",
    "\n",
    "        if plan_hdr and dig_hdr and dig_hdr.start() > plan_hdr.end():\n",
    "            plan_block = t[plan_hdr.end():dig_hdr.start()].strip()\n",
    "            dig_block = t[dig_hdr.end():].strip()\n",
    "        else:\n",
    "            # fallback: try split by substring if headers are inline\n",
    "            if \"DIGEST:\" in t:\n",
    "                left, right = t.split(\"DIGEST:\", 1)\n",
    "                plan_block = left\n",
    "                dig_block = right\n",
    "                plan_block = re.sub(r\"(?im)^\\s*(?:[-*•]\\s*)?PLAN\\s*:\\s*\", \"\", plan_block).strip()\n",
    "            else:\n",
    "                plan_block, dig_block = t, \"\"\n",
    "\n",
    "        # keep only bullet lines for plan\n",
    "        plan_lines = []\n",
    "        for ln in plan_block.splitlines():\n",
    "            ln = ln.strip()\n",
    "            if ln.startswith((\"-\", \"*\", \"•\")):\n",
    "                plan_lines.append(ln)\n",
    "        plan_part = \"\\n\".join(plan_lines).strip()\n",
    "\n",
    "        # digest: first bullet line\n",
    "        digest_line = \"\"\n",
    "        for ln in dig_block.splitlines():\n",
    "            ln = ln.strip()\n",
    "            if ln.startswith((\"-\", \"*\", \"•\")):\n",
    "                digest_line = ln.lstrip(\"-*• \").strip()\n",
    "                break\n",
    "            \n",
    "        plan_part = re.sub(r\"(?i)\\b<<<END>>>\\b\", \"\", plan_part).strip()\n",
    "        digest_line = re.sub(r\"(?i)\\s*\\b<<<END>>>\\b\\s*$\", \"\", digest_line).strip()\n",
    "\n",
    "        return plan_part, digest_line\n",
    "\n",
    "    def _make_digest_fallback(self, plan_text: str) -> str:\n",
    "        maxc = getattr(self.cfg, \"planner_digest_max_chars\", 256)\n",
    "        lines = [ln.strip() for ln in (plan_text or \"\").splitlines() if ln.strip()]\n",
    "        bullets = [ln.lstrip(\"-*• \").strip() for ln in lines if ln.startswith((\"-\", \"*\", \"•\"))]\n",
    "        s = (bullets[0] if bullets else \"\").strip()\n",
    "        if not s:\n",
    "            s = \"Try a different approach; enforce small scans + modular checks + caching.\"\n",
    "        return s[:maxc].strip()\n",
    "\n",
    "    def _is_good(self, plan_part: str, digest: str) -> bool:\n",
    "        if not plan_part:\n",
    "            return False\n",
    "        nbul = sum(1 for ln in plan_part.splitlines() if ln.strip().startswith((\"-\", \"*\", \"•\")))\n",
    "        if nbul < 5:\n",
    "            return False\n",
    "        return bool(digest.strip())\n",
    "\n",
    "    # ----------------- main API -----------------\n",
    "\n",
    "    def gen_plan(self, problem_text: str, history: list[dict], attempt_index: int):\n",
    "        # NOTE: planner should be short; do NOT set serial_plan_max_tokens too large.\n",
    "        max_tokens = min(int(self.cfg.serial_plan_max_tokens), 512)\n",
    "        temp = float(getattr(self.cfg, \"serial_aux_temperature\", 0.7))\n",
    "\n",
    "        user_prompt = (\n",
    "            f\"{self.cfg.planner_prompt}\\n\\n\"\n",
    "            f\"PROBLEM:\\n{problem_text}\\n\\n\"\n",
    "            f\"{self._build_history_block(history)}\\n\"\n",
    "            f\"Output now.\"\n",
    "        )\n",
    "\n",
    "        seed = int((self.cfg.seed + 777) * (attempt_index + 1) ** 2)\n",
    "        parsed_text, raw_text = self._stream_completion(\n",
    "            system_prompt=self.cfg.planner_system_prompt,\n",
    "            user_text=user_prompt,\n",
    "            seed=seed,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temp,\n",
    "        )\n",
    "\n",
    "        plan_part, digest = self._extract_plan_and_digest(parsed_text)\n",
    "\n",
    "        # one repair if bad\n",
    "        if not self._is_good(plan_part, digest):\n",
    "            repair_user = (\n",
    "                \"FORMATTER TASK. Output ONLY the required template.\\n\"\n",
    "                \"First line: PLAN:\\n\"\n",
    "                \"Then 5-8 lines starting with '- '\\n\"\n",
    "                \"Then: DIGEST:\\n\"\n",
    "                \"Then exactly one '- ' line (<=256 chars)\\n\"\n",
    "                \"Then: <<<END>>>\\n\\n\"\n",
    "                f\"PROBLEM:\\n{problem_text}\\n\"\n",
    "            )\n",
    "            parsed_text2, raw_text2 = self._stream_completion(\n",
    "                system_prompt=self.cfg.planner_system_prompt,\n",
    "                user_text=repair_user,\n",
    "                seed=seed + 1,\n",
    "                max_tokens=256,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            parsed_text = parsed_text2 or parsed_text\n",
    "            raw_text = raw_text2 or raw_text\n",
    "            plan_part, digest = self._extract_plan_and_digest(parsed_text)\n",
    "\n",
    "        # plan_part = self._strip_answerish_lines(plan_part)\n",
    "        # program-side hard fallback (never return garbage paragraphs)\n",
    "        if not plan_part.strip():\n",
    "            plan_part = self._bulletize(parsed_text) or self._bulletize(problem_text)\n",
    "        if not digest.strip():\n",
    "            digest = self._make_digest_fallback(plan_part)\n",
    "\n",
    "        # truncate plan only (digest stays useful)\n",
    "        # plan_part = plan_part[: self.cfg.serial_context_char_limit].strip()\n",
    "        # if len(digest) > self.cfg.planner_digest_max_chars:\n",
    "        #     digest = digest[: self.cfg.planner_digest_max_chars].strip()\n",
    "\n",
    "        # return both parsed and raw for logging\n",
    "        plan_sanitized = plan_part  # already bullet-only; treat as sanitized\n",
    "        plan_raw = raw_text or parsed_text\n",
    "        return plan_part, digest, plan_raw, plan_sanitized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb440343",
   "metadata": {
    "papermill": {
     "duration": 0.002744,
     "end_time": "2026-01-25T23:24:24.960925",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.958181",
     "status": "completed"
    },
    "tags": [
     "ignore"
    ]
   },
   "source": [
    "#### AIMO3Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cac53cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T23:24:24.967449Z",
     "iopub.status.busy": "2026-01-25T23:24:24.967297Z",
     "iopub.status.idle": "2026-01-25T23:24:24.996585Z",
     "shell.execute_reply": "2026-01-25T23:24:24.996230Z"
    },
    "papermill": {
     "duration": 0.033653,
     "end_time": "2026-01-25T23:24:24.997281",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.963628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Solver:\n",
    "\n",
    "    def __init__(self, cfg, port: int = 8000):\n",
    "        self.cfg = cfg\n",
    "        self.port = getattr(self.cfg, \"vllm_port\", port)\n",
    "        self.base_url = f\"http://localhost:{self.port}/v1\"\n",
    "        self.api_key = 'sk-local'\n",
    "        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key, timeout=self.cfg.session_timeout)\n",
    "\n",
    "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
    "        \n",
    "        self.notebook_start_time = time.time()\n",
    "        self.problems_remaining = 50\n",
    "        self.logger = AIMO3Logger(cfg)\n",
    "        self.planner = AIMO3Planner(cfg)\n",
    "        self._initialize_kernels()\n",
    "    \n",
    "    def _initialize_kernels(self) -> None:\n",
    "        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n",
    "        start_time = time.time()\n",
    "    \n",
    "        self.sandbox_pool = queue.Queue()\n",
    "    \n",
    "        def _create_sandbox():\n",
    "            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n",
    "    \n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
    "            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n",
    "            for future in as_completed(futures):\n",
    "                self.sandbox_pool.put(future.result())\n",
    "    \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n",
    "\n",
    "    def _scan_for_answer(self, text: str) -> int | None:\n",
    "        pattern = r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}'\n",
    "        matches = re.findall(pattern, text)\n",
    "    \n",
    "        if matches:\n",
    "            try:\n",
    "                clean_value = matches[-1].replace(',', '')\n",
    "                value = int(clean_value)\n",
    "    \n",
    "                if 0 <= value <= 99999:\n",
    "                    return value\n",
    "    \n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "        pattern = r'final\\s+answer\\s+is\\s*([0-9,]+)'\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    \n",
    "        if matches:\n",
    "            try:\n",
    "                clean_value = matches[-1].replace(',', '')\n",
    "                value = int(clean_value)\n",
    "    \n",
    "                if 0 <= value <= 99999:\n",
    "                    return value\n",
    "    \n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    def _compute_mean_entropy(self, logprobs_buffer: list) -> float:\n",
    "        if not logprobs_buffer:\n",
    "            return float('inf')\n",
    "    \n",
    "        total_entropy = 0.0\n",
    "        token_count = 0\n",
    "    \n",
    "        for top_logprobs_dict in logprobs_buffer:\n",
    "            \n",
    "            if not isinstance(top_logprobs_dict, dict):\n",
    "                continue\n",
    "            \n",
    "            if not top_logprobs_dict:\n",
    "                continue\n",
    "            \n",
    "            token_entropy = 0.0\n",
    "            \n",
    "            for token_str, log_prob in top_logprobs_dict.items():\n",
    "                prob = math.exp(log_prob)\n",
    "                \n",
    "                if prob > 0:\n",
    "                    token_entropy -= prob * math.log2(prob)\n",
    "            \n",
    "            total_entropy += token_entropy\n",
    "            token_count += 1\n",
    "    \n",
    "        if token_count == 0:\n",
    "            return float('inf')\n",
    "    \n",
    "        return total_entropy / token_count\n",
    "\n",
    "    def _extract_tags(self, result: dict) -> list:\n",
    "        if not self.cfg.attempt_tags: return []\n",
    "        tags = []\n",
    "        code = result.get('Code', '')\n",
    "        err_msg = str(result.get('Error Log', ''))\n",
    "        \n",
    "        # 1. 行为标签\n",
    "        if result.get('Python Calls', 0) > 0: tags.append(\"UsedPython\")\n",
    "        \n",
    "        # 2. 算法/函数标签 (通过正则快速扫描)\n",
    "        if \"range(\" in code and (\"for \" in code or \"[\" in code): tags.append(\"UsedLoop\")\n",
    "        # if re.search(r\"range\\(\\s*\\d{1,4}\\s*\\)\", code) or re.search(r\"range\\(\\s*\\d{1,4}\\s*,\\s*\\d{1,4}\", code): tags.append(\"UsedBruteForceSmall\")\n",
    "        if re.search(r'range\\(\\s*(?:1,)?\\s*(?:[1-9]|[1-4][0-9]|50)\\s*\\)', code): tags.append(\"UsedBruteForceSmall\")\n",
    "        if \"factorint\" in code or \"primefactors\" in code: tags.append(\"UsedFactorint\")\n",
    "        if \"pow(\" in code and \",\" in code and code.count(\",\") >= 2: tags.append(\"UsedModPow\")\n",
    "        if re.search(r\"\\bgcd\\s*\\(\", code) or \"math.gcd\" in code: tags.append(\"UsedGCD\")\n",
    "        if \"itertools\" in code: tags.append(\"UsedItertools\")\n",
    "        if \"sympy\" in code: tags.append(\"UsedSympy\")\n",
    "\n",
    "        # 3. 错误标签\n",
    "        if result.get('Python Errors', 0) > 0:\n",
    "            if \"Timeout\" in err_msg or \"TimeLimit\" in err_msg: tags.append(\"TimedOut\")\n",
    "            elif \"KeyError: slice(None\" in err_msg or \"slice(None\" in err_msg: tags.append(\"SliceDict\")\n",
    "            elif \"KeyError\" in err_msg: tags.append(\"KeyError\")\n",
    "            elif \"OverflowError\" in err_msg: tags.append(\"Overflow\")\n",
    "            elif \"RecursionError\" in err_msg: tags.append(\"RecursionDepth\")\n",
    "            elif \"Exceeds the limit for integer string conversion\" in err_msg: tags.append(\"IntStrLimit\")\n",
    "            else: tags.append(\"OtherPyError\")\n",
    "\n",
    "        # 4. 结果标签\n",
    "        if result.get('Answer') is not None: tags.append(\"FoundAnswer\")\n",
    "        else: tags.append(\"NoAnswer\")\n",
    "\n",
    "        return tags\n",
    "\n",
    "    def _process_attempt(\n",
    "        self, \n",
    "        problem: str, \n",
    "        system_prompt: str, \n",
    "        attempt_index: int, \n",
    "        stop_event: threading.Event, \n",
    "        deadline: float,\n",
    "        problem_id: str,\n",
    "        plan: str = \"\",\n",
    "        plan_digest: str = \"\",\n",
    "        plan_raw: str = \"\",\n",
    "        plan_sanitized: str = \"\",\n",
    "    ) -> dict:\n",
    "        attempt_log = deque([])\n",
    "        attempt_start = time.time()\n",
    "\n",
    "        if stop_event.is_set() or time.time() > deadline:\n",
    "            print(f\"Problem: {problem_id} TIMEOUT!\")\n",
    "            return {\n",
    "                'Attempt': attempt_index + 1, \n",
    "                'Answer': None, \n",
    "                'Python Calls': 0, \n",
    "                'Python Errors': 0, \n",
    "                'Response Length': 0, \n",
    "                'Entropy': float('inf'),\n",
    "                'Log': \"\\n\".join(attempt_log)\n",
    "            }\n",
    "    \n",
    "        local_tool = None\n",
    "        sandbox = None\n",
    "        python_calls = 0\n",
    "        python_errors = 0\n",
    "        total_tokens = 0\n",
    "        final_answer = None\n",
    "        \n",
    "        logprobs_buffer = []\n",
    "        attempt_seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n",
    "    \n",
    "        try:\n",
    "            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n",
    "            local_tool = AIMO3Tool(\n",
    "                self.cfg,\n",
    "                sandbox=sandbox\n",
    "            )\n",
    "            encoding = self.encoding\n",
    "\n",
    "            aug = \"\"\n",
    "            if plan:\n",
    "                aug += f\"\\n\\n=== CURRENT ATTEMPT PLAN ===\\n{plan}\\n\"\n",
    "            if aug:\n",
    "                aug += \"\\nFollow the plan.\\n\"\n",
    "            full_problem = problem + aug\n",
    "            if self.cfg.debug and self.logger:\n",
    "                attempt_log.append(self.logger.log_planner_block(plan_raw, plan_sanitized, plan_digest))\n",
    "                attempt_log.append(\"### Planner Augmentation\\n\")\n",
    "                attempt_log.append(f\"{_format_markdown(aug, mode='text')}\\n\")\n",
    "\n",
    "            sys_content = (\n",
    "                SystemContent.new()\n",
    "                .with_model_identity(system_prompt)\n",
    "                .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
    "                .with_tools(local_tool.tool_config)\n",
    "            )\n",
    "            messages = [\n",
    "                Message.from_role_and_content(Role.SYSTEM, sys_content),\n",
    "                Message.from_role_and_content(Role.USER, full_problem), # problem\n",
    "            ]\n",
    "            conversation = Conversation.from_messages(messages)\n",
    "\n",
    "            for turn_i in range(self.cfg.turns):\n",
    "                if stop_event.is_set() or time.time() > deadline:\n",
    "                    break\n",
    "    \n",
    "                prompt_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
    "                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n",
    "    \n",
    "                if max_tokens < self.cfg.buffer_tokens:\n",
    "                    break\n",
    "\n",
    "                if self.cfg.debug and self.cfg.debug_req:\n",
    "                    # convert prompt_ids (tensors) back to readable text\n",
    "                    # which includes LLM special symbols like <|im_start|>\n",
    "                    full_request_text = encoding.decode(prompt_ids)\n",
    "                    \n",
    "                    snippet = self.logger.get_debug_snippet(full_request_text)\n",
    "                    # for Turn 0, 10, 20,... log full request\n",
    "                    formatted_req = _format_markdown(full_request_text if turn_i % 10 == 0 else snippet)\n",
    "                    attempt_log.append(f\"### Turn {turn_i} - Raw Request to Model:\")\n",
    "                    attempt_log.append(formatted_req)\n",
    "\n",
    "                try:\n",
    "                    req_timeout = max(1.0, deadline - time.time())\n",
    "                    stream = self.client.completions.create(\n",
    "                        model=self.cfg.served_model_name, \n",
    "                        temperature=self.cfg.temperature, \n",
    "                        logprobs=self.cfg.top_logprobs, \n",
    "                        max_tokens=max_tokens, \n",
    "                        prompt=prompt_ids, \n",
    "                        seed=attempt_seed, \n",
    "                        stream=True, \n",
    "                        timeout=req_timeout,\n",
    "                        extra_body={\n",
    "                            'min_p': self.cfg.min_p, \n",
    "                            'stop_token_ids': self.stop_token_ids, \n",
    "                            'return_token_ids': True\n",
    "                        },\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    # stream create failed -> don't hang; record & break this attempt turn\n",
    "                    if self.cfg.debug:\n",
    "                        attempt_log.append(\n",
    "                            f\"### Turn {turn_i} - Stream Create Failed (timeout={req_timeout:.1f}s)\\n\"\n",
    "                            f\"{_format_markdown(str(e), mode='text')}\\n\"\n",
    "                        )\n",
    "                    break\n",
    "\n",
    "                full_response_text = \"\"\n",
    "                try:\n",
    "                    token_buffer = []\n",
    "                    text_chunks = []\n",
    "    \n",
    "                    for chunk in stream:\n",
    "                        if stop_event.is_set() or time.time() > deadline:\n",
    "                            break\n",
    "    \n",
    "                        new_tokens = chunk.choices[0].token_ids\n",
    "                        new_text = chunk.choices[0].text\n",
    "    \n",
    "                        if new_tokens:\n",
    "                            token_buffer.extend(new_tokens)\n",
    "                            total_tokens += len(new_tokens)\n",
    "                            text_chunks.append(new_text)\n",
    "                            if self.cfg.debug and self.cfg.debug_resp:\n",
    "                                full_response_text += new_text\n",
    "\n",
    "                            chunk_logprobs = chunk.choices[0].logprobs\n",
    "                            if chunk_logprobs is not None:\n",
    "                                if chunk_logprobs.top_logprobs:\n",
    "                                    logprobs_buffer.extend(chunk_logprobs.top_logprobs)\n",
    "    \n",
    "                        if '}' in new_text:\n",
    "                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n",
    "                            answer = self._scan_for_answer(search_text)\n",
    "    \n",
    "                            if answer is not None:\n",
    "                                final_answer = answer\n",
    "                                break\n",
    "    \n",
    "                finally:\n",
    "                    stream.close()\n",
    "\n",
    "                if self.cfg.debug and full_response_text:\n",
    "                    attempt_log.append(f\"### Turn {turn_i} - Model Response:\")\n",
    "                    formatted_resp = _format_markdown(full_response_text)\n",
    "                    attempt_log.append(formatted_resp)\n",
    "\n",
    "                if final_answer is not None:\n",
    "                    break\n",
    "    \n",
    "                if not token_buffer:\n",
    "                    break\n",
    "    \n",
    "                new_messages = encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n",
    "                conversation.messages.extend(new_messages)\n",
    "                last_message = new_messages[-1]\n",
    "    \n",
    "                if last_message.channel == 'final':\n",
    "                    answer_text = last_message.content[0].text\n",
    "                    final_answer = self._scan_for_answer(answer_text)\n",
    "                    break\n",
    "    \n",
    "                if last_message.recipient == 'python':\n",
    "                    python_calls += 1\n",
    "                    tool_responses = local_tool.process_sync_plus(last_message, turn_i, attempt_log)\n",
    "                    response_text = tool_responses[0].content[0].text\n",
    "\n",
    "                    has_error = response_text.startswith('[ERROR]') or 'Traceback' in response_text or 'Error:' in response_text\n",
    "\n",
    "                    if self.cfg.debug:\n",
    "                        code_content = last_message.content[0].text\n",
    "                        attempt_log.append(f\"### Turn {turn_i} - Python Call:\")\n",
    "                        attempt_log.append(_format_markdown(code_content))\n",
    "\n",
    "                        emoji_error = '❌' if has_error else ''\n",
    "                        attempt_log.append(f\"### Turn {turn_i} {emoji_error} - Python Output:\")\n",
    "                        snippet_out = self.logger.get_debug_snippet(response_text)\n",
    "                        formatted_out = _format_markdown(snippet_out, mode=\"text\")\n",
    "                        attempt_log.append(f\"{formatted_out}\\n\")\n",
    "\n",
    "                    if has_error:\n",
    "                        python_errors += 1\n",
    "    \n",
    "                    conversation.messages.extend(tool_responses)\n",
    "    \n",
    "        except Exception as exc:\n",
    "            python_errors += 1\n",
    "            if self.cfg.debug:\n",
    "                attempt_log.append(f\"\\n**EXCEPTION:** {str(exc)}\\n\")\n",
    "            print(f\"EXCEPTION: {str(exc)}\")\n",
    "\n",
    "        finally:\n",
    "            if sandbox is not None:\n",
    "                sandbox.reset()\n",
    "                self.sandbox_pool.put(sandbox)\n",
    "    \n",
    "        mean_entropy = self._compute_mean_entropy(logprobs_buffer)\n",
    "        attempt_elapsed = time.time() - attempt_start\n",
    "        attempt_time = _fmt_time(attempt_elapsed)\n",
    "        if self.cfg.debug:\n",
    "            attempt_log.appendleft(f\"Attempt spent time: **{attempt_time}**\\n\")\n",
    "            attempt_log.appendleft(f\"## Attempt {attempt_index + 1}\\n\")\n",
    "    \n",
    "        return {\n",
    "            'Attempt': attempt_index + 1, \n",
    "            'Response Length': total_tokens, \n",
    "            'Python Calls': python_calls, \n",
    "            'Python Errors': python_errors, \n",
    "            'Entropy': mean_entropy, \n",
    "            'Answer': final_answer,\n",
    "            'Plan': plan,\n",
    "            'PlanDigest': plan_digest,\n",
    "            'PlanRaw': plan_raw,\n",
    "            'PlanSanitized': plan_sanitized,\n",
    "            'Log': \"\\n\".join(attempt_log),\n",
    "            'Time': attempt_time\n",
    "        }\n",
    "    \n",
    "    def _select_answer_by_calls(self, detailed_results: list) -> tuple[pd.DataFrame, int]:\n",
    "        stats = defaultdict(lambda: {'votes': 0, 'calls': 0})\n",
    "        for result in detailed_results:\n",
    "            answer = result['Answer']\n",
    "\n",
    "            if answer is not None:\n",
    "                stats[answer]['votes'] += 1\n",
    "                stats[answer]['calls'] += result['Python Calls']\n",
    "\n",
    "        sorted_stats = sorted(\n",
    "            stats.items(), \n",
    "            key=lambda item: (item[1]['votes'], item[1]['calls']), \n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        vote_data = []\n",
    "\n",
    "        for answer, data in sorted_stats:\n",
    "            vote_data.append((answer, data['votes'], data['calls']))\n",
    "\n",
    "        vote_dataframe = pd.DataFrame(vote_data, columns=['Answer', 'Votes', 'Calls'])\n",
    "        display(vote_dataframe)\n",
    "\n",
    "        final_answer = sorted_stats[0][0]\n",
    "        final_votes = sorted_stats[0][1]['votes']\n",
    "        final_calls = sorted_stats[0][1]['calls']\n",
    "\n",
    "        print(f'\\nFinal Result: {final_answer} | Votes: {final_votes} | Calls: {final_calls}\\n')\n",
    "        return vote_dataframe, final_answer\n",
    "\n",
    "    def _calc_score(self, result) -> float:\n",
    "        entropy = result['Entropy']\n",
    "        py_calls = int(result.get('Python Calls', 0) or 0)\n",
    "        py_errs = int(result.get('Python Errors', 0) or 0)\n",
    "        score = 1.0 / max(entropy, 1e-9)\n",
    "\n",
    "        penalty_err_enabled = getattr(self.cfg, \"penalty_err_enabled\", False)\n",
    "        penalty_err_alpha = float(getattr(self.cfg, \"penalty_err_alpha\", 0.05))\n",
    "        if penalty_err_enabled and penalty_err_alpha > 0:\n",
    "            score *= 1.0 / (1.0 + self.cfg.penalty_err_alpha * py_errs)\n",
    "    \n",
    "        mode = getattr(self.cfg, \"vote_py_bonus_mode\", \"none\")\n",
    "        if mode and mode != \"none\":\n",
    "            if mode == \"calls\":\n",
    "                sig = py_calls\n",
    "            elif mode == \"ok_calls\":\n",
    "                sig = max(0, py_calls - py_errs)\n",
    "            else:\n",
    "                sig = 0\n",
    "\n",
    "            cap = int(getattr(self.cfg, \"vote_py_bonus_cap\", 4))\n",
    "            alpha = float(getattr(self.cfg, \"vote_py_bonus_alpha\", 0.05))\n",
    "            sig = min(sig, cap)\n",
    "            score *= (1.0 + alpha * sig)\n",
    "        return score\n",
    "\n",
    "    def _select_answer_by_score(self, detailed_results: list) -> tuple[pd.DataFrame, int]:\n",
    "        answer_score = defaultdict(float)\n",
    "        answer_votes = defaultdict(int)\n",
    "\n",
    "        for result in detailed_results:\n",
    "            answer = result['Answer']\n",
    "            \n",
    "            if answer is not None:\n",
    "                score = self._calc_score(result)\n",
    "                answer_score[answer] += score\n",
    "                answer_votes[answer] += 1\n",
    "\n",
    "        scored_answers = []\n",
    "\n",
    "        for answer, total_score in answer_score.items():\n",
    "            scored_answers.append({\n",
    "                'answer': answer, \n",
    "                'votes': answer_votes[answer], \n",
    "                'score': total_score\n",
    "            })\n",
    "\n",
    "        scored_answers.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        vote_data = []\n",
    "\n",
    "        for item in scored_answers:\n",
    "            vote_data.append((\n",
    "                item['answer'], \n",
    "                item['votes'], \n",
    "                item['score']\n",
    "            ))\n",
    "\n",
    "        vote_dataframe = pd.DataFrame(\n",
    "            vote_data, \n",
    "            columns=['Answer', 'Votes', 'Score']\n",
    "        )\n",
    "\n",
    "        vote_dataframe = vote_dataframe.round({'Score': 3})\n",
    "        display(vote_dataframe)\n",
    "        \n",
    "        if not scored_answers:\n",
    "            print('\\nFinal Answer: 0\\n')\n",
    "            return vote_dataframe, 0\n",
    "\n",
    "        final_answer = scored_answers[0]['answer']    \n",
    "        print(f'\\nFinal Answer: {final_answer}\\n')\n",
    "        return vote_dataframe, final_answer\n",
    "\n",
    "    def _select_answer(self, detailed_results: list) -> tuple[pd.DataFrame, int]:\n",
    "        if self.cfg.vote_policy == 'calls':\n",
    "            return self._select_answer_by_calls(detailed_results)\n",
    "        else:\n",
    "            return self._select_answer_by_score(detailed_results)\n",
    "\n",
    "    def solve_problem(self, problem: str, problem_id: str = \"UNK\") -> int:\n",
    "        print(f'\\nProblem: {problem}\\n')\n",
    "        \n",
    "        p_start = time.time()\n",
    "        user_input = f'{problem}\\n\\n{self.cfg.preference_prompt}'    \n",
    "        elapsed_global = time.time() - self.notebook_start_time\n",
    "        time_left = self.cfg.notebook_limit - elapsed_global\n",
    "        problems_left_others = max(0, self.problems_remaining - 1)\n",
    "        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n",
    "    \n",
    "        budget = time_left - reserved_time\n",
    "        budget = min(budget, self.cfg.high_problem_timeout)\n",
    "        budget = max(budget, self.cfg.base_problem_timeout)\n",
    "    \n",
    "        deadline = time.time() + budget\n",
    "    \n",
    "        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n",
    "    \n",
    "        tasks = []\n",
    "    \n",
    "        for attempt_index in range(self.cfg.attempts):\n",
    "            tasks.append((self.cfg.system_prompt, attempt_index))\n",
    "    \n",
    "        detailed_results = []\n",
    "        valid_answers = []\n",
    "        stop_event = threading.Event()\n",
    "        executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n",
    "    \n",
    "        try:\n",
    "            futures = []    \n",
    "            for (system_prompt, attempt_index) in tasks:\n",
    "                future = executor.submit(\n",
    "                    self._process_attempt, \n",
    "                    user_input, \n",
    "                    system_prompt, \n",
    "                    attempt_index, \n",
    "                    stop_event, \n",
    "                    deadline,\n",
    "                    problem_id\n",
    "                )\n",
    "                futures.append(future)\n",
    "    \n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    detailed_results.append(result)\n",
    "                    if result['Answer'] is not None:\n",
    "                        valid_answers.append(result['Answer'])\n",
    "\n",
    "                    counts = Counter(valid_answers).most_common(1)\n",
    "                    if counts and counts[0][1] >= self.cfg.early_stop:\n",
    "                        stop_event.set()    \n",
    "                        for f in futures: f.cancel()\n",
    "                        break\n",
    "    \n",
    "                except Exception as exc:\n",
    "                    print(f'Future failed: {exc}')\n",
    "                    continue\n",
    "    \n",
    "        finally:\n",
    "            stop_event.set()\n",
    "            executor.shutdown(wait=True, cancel_futures=True)\n",
    "            self.problems_remaining = max(0, self.problems_remaining - 1)\n",
    "\n",
    "        if detailed_results:\n",
    "            results_dataframe = pd.DataFrame(detailed_results)\n",
    "            results_dataframe['Entropy'] = results_dataframe['Entropy'].round(3)\n",
    "            results_dataframe['Answer'] = results_dataframe['Answer'].astype('Int64')\n",
    "            \n",
    "            cols = [c for c in results_dataframe.columns if not c in self.cfg.debug_cols]\n",
    "            display(results_dataframe[cols])\n",
    "\n",
    "        if not valid_answers:\n",
    "            print('\\nResult: 0\\n')\n",
    "            vote_data, final_answer = pd.DataFrame(columns=['Answer', 'Votes', 'Score']), 0\n",
    "        else:\n",
    "            vote_data, final_answer = self._select_answer(detailed_results)\n",
    "        \n",
    "        p_end = time.time()\n",
    "        self.logger.write_debug_logs(detailed_results, vote_data, problem, problem_id, _fmt_time(p_end - p_start))\n",
    "        return final_answer\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'sandbox_pool'):\n",
    "            while not self.sandbox_pool.empty():\n",
    "                try:\n",
    "                    sb = self.sandbox_pool.get_nowait()\n",
    "                    sb.close()\n",
    "                except Exception:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90185c8",
   "metadata": {
    "papermill": {
     "duration": 0.00279,
     "end_time": "2026-01-25T23:24:25.002780",
     "exception": false,
     "start_time": "2026-01-25T23:24:24.999990",
     "status": "completed"
    },
    "tags": [
     "ignore"
    ]
   },
   "source": [
    "### Test & Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f85e3e",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-01-25T23:24:25.009275Z",
     "iopub.status.busy": "2026-01-25T23:24:25.009124Z",
     "iopub.status.idle": "2026-01-25T23:27:40.844350Z",
     "shell.execute_reply": "2026-01-25T23:27:40.843928Z"
    },
    "papermill": {
     "duration": 195.839748,
     "end_time": "2026-01-25T23:27:40.845355",
     "exception": false,
     "start_time": "2026-01-25T23:24:25.005607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights from /kaggle/input/gpt-oss-120b/transformers/default/1 into OS Page Cache...\n",
      "Processed 26 files (65.28 GB) in 75.66 seconds.\n",
      "\n",
      "Waiting for vLLM server...\n",
      "Server is ready (took 116.99 seconds).\n",
      "\n",
      "Initializing 16 persistent Jupyter kernels...\n",
      "Kernels initialized in 2.66 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_delete(\"server\")\n",
    "server = AIMO3Server(CFG)\n",
    "\n",
    "_delete(\"solver\")\n",
    "solver = AIMO3Solver(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d7a5f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T23:27:40.853037Z",
     "iopub.status.busy": "2026-01-25T23:27:40.852883Z",
     "iopub.status.idle": "2026-01-25T23:27:40.918682Z",
     "shell.execute_reply": "2026-01-25T23:27:40.918290Z"
    },
    "papermill": {
     "duration": 0.070889,
     "end_time": "2026-01-25T23:27:40.919729",
     "exception": false,
     "start_time": "2026-01-25T23:27:40.848840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_answers = {}\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
    "    global predict_answers\n",
    "    id_value = id_.item(0)\n",
    "    question_text = question.item(0)\n",
    "    gc.disable()\n",
    "    p_start = time.time()\n",
    "    final_answer = solver.solve_problem(question_text, problem_id=str(id_value))\n",
    "    p_end = time.time()\n",
    "    p_time = p_end - p_start\n",
    "    predict_answers[id_value] = {'id': id_value, 'answer': final_answer, 'time': p_time, 'time_str': _fmt_time(p_time)}\n",
    "    gc.enable()\n",
    "    gc.collect()\n",
    "    return pl.DataFrame({'id': id_value, 'answer': final_answer})\n",
    "\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "def test():\n",
    "    global predict_answers\n",
    "\n",
    "    test_csv = '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv'\n",
    "    # test_csv = '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv'\n",
    "    # test_csv = '/kaggle/input/aimo-p3-hard/easy2.csv'\n",
    "    # test_csv = '/kaggle/input/aimo-p3-hard/test2.csv'\n",
    "    # test_csv = '/kaggle/input/aimo-p3-hard/test3.csv'\n",
    "    # test_csv = '/kaggle/input/aimo-p3-hard/p5.csv'\n",
    "    # test_csv = '/kaggle/input/aimo-p3-hard/p10.csv'\n",
    "\n",
    "    t_start = time.time()\n",
    "    inference_server.run_local_gateway((test_csv,))\n",
    "    t_end = time.time()\n",
    "    t_time = t_end - t_start\n",
    "\n",
    "    df = pd.read_csv(test_csv)\n",
    "    real_answers = dict(zip(df[\"id\"], df[\"answer\"])) if \"answer\" in df.columns else {}\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    # Check accuracy if ground truth available\n",
    "    for id in predict_answers:\n",
    "        pa = predict_answers[id]\n",
    "        if id in real_answers:\n",
    "            total_count += 1\n",
    "            real_answer = real_answers[id]\n",
    "            is_correct = (pa['answer'] == real_answer)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            status = \"✅\" if is_correct else \"❌\"\n",
    "            print(f\"Problem {id}: ({pa['time_str']}) -- Predict Answer: {pa['answer']} | Ground Truth: {real_answer} | {status}\")\n",
    "        else:\n",
    "            print(f\"Problem {id}: ({pa['time_str']}) -- Predict Answer: {pa['answer']}\")\n",
    "    \n",
    "    df2 = pl.DataFrame(list(predict_answers.values()))\n",
    "    stats = df2.select([\n",
    "        pl.col(\"time\").max().alias(\"max_time\"),\n",
    "        pl.col(\"time\").min().alias(\"min_time\"),\n",
    "        pl.col(\"time\").mean().alias(\"avg_time\"),\n",
    "    ])\n",
    "\n",
    "    max_id = df2.filter(pl.col(\"time\") == stats[\"max_time\"][0]).select(\"id\").item()\n",
    "    min_id = df2.filter(pl.col(\"time\") == stats[\"min_time\"][0]).select(\"id\").item()\n",
    "\n",
    "    print(f\"📊 Total {total_count} problems, ⏱️ total time {_fmt_time(t_time)}; \"\n",
    "          f\"Running Accuracy 🎯: {correct_count}/{total_count} ({100*correct_count/total_count:.1f}%)\\n\"\n",
    "          f\"Max time: {_fmt_time(stats['max_time'][0])} (id={max_id}); \"\n",
    "          f\"Min time: {_fmt_time(stats['min_time'][0])} (id={min_id}); \"\n",
    "          f\"Avg time: {_fmt_time(stats['avg_time'][0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90958703",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-01-25T23:27:40.926519Z",
     "iopub.status.busy": "2026-01-25T23:27:40.926375Z",
     "iopub.status.idle": "2026-01-25T23:27:59.448494Z",
     "shell.execute_reply": "2026-01-25T23:27:59.447920Z"
    },
    "papermill": {
     "duration": 18.52654,
     "end_time": "2026-01-25T23:27:59.449342",
     "exception": true,
     "start_time": "2026-01-25T23:27:40.922802",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem: What is $1-1$?\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1769384561.25\n",
      "\n",
      "EXCEPTION: Unexpected EOS while waiting for message header to complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0</td>\n",
       "      <td>0:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0</td>\n",
       "      <td>0:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0</td>\n",
       "      <td>0:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0</td>\n",
       "      <td>0:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer  \\\n",
       "0        2               90             0              0    0.562       0   \n",
       "1        4              101             0              0    0.728       0   \n",
       "2        5              109             0              0    0.699       0   \n",
       "3        1              111             0              0    0.684       0   \n",
       "\n",
       "   Time  \n",
       "0  0:14  \n",
       "1  0:14  \n",
       "2  0:14  \n",
       "3  0:15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0       0      4  6.046"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 0\n",
      "\n",
      "Debug log written to 000aaa.md\n",
      "\n",
      "Problem: What is $0\\times10$?\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1769384576.14\n",
      "\n",
      "EXCEPTION: Unexpected EOS while waiting for message header to complete\n",
      "EXCEPTION: Unexpected EOS while waiting for message header to complete\n",
      "EXCEPTION: Unexpected EOS while waiting for message header to complete\n",
      "EXCEPTION: Unexpected EOS while waiting for message header to complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0</td>\n",
       "      <td>0:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0</td>\n",
       "      <td>0:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0</td>\n",
       "      <td>0:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0</td>\n",
       "      <td>0:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer  \\\n",
       "0        4               72             0              0    0.720       0   \n",
       "1        5               83             0              0    0.541       0   \n",
       "2        1               96             0              0    0.745       0   \n",
       "3        6               96             0              0    0.841       0   \n",
       "\n",
       "   Time  \n",
       "0  0:01  \n",
       "1  0:01  \n",
       "2  0:01  \n",
       "3  0:01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0       0      4  5.769"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 0\n",
      "\n",
      "Debug log written to 111bbb.md\n",
      "\n",
      "Problem: Solve $4+x=4$ for $x$.\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1769384577.43\n",
      "\n",
      "EXCEPTION: Unexpected EOS while waiting for message header to complete\n",
      "EXCEPTION: Unexpected EOS while waiting for message header to complete\n",
      "EXCEPTION: Unexpected EOS while waiting for message header to complete\n",
      "EXCEPTION: Unexpected EOS while waiting for message header to complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0</td>\n",
       "      <td>0:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0</td>\n",
       "      <td>0:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0</td>\n",
       "      <td>0:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0</td>\n",
       "      <td>0:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer  \\\n",
       "0        7              104             0              0    0.793       0   \n",
       "1        6              143             0              0    0.718       0   \n",
       "2        4              150             0              0    0.697       0   \n",
       "3        8              152             0              0    0.657       0   \n",
       "\n",
       "   Time  \n",
       "0  0:01  \n",
       "1  0:01  \n",
       "2  0:01  \n",
       "3  0:01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0       0      4   5.61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 0\n",
      "\n",
      "Debug log written to 222ccc.md\n",
      "Problem 000aaa: (0:15) -- Predict Answer: 0\n",
      "Problem 111bbb: (0:01) -- Predict Answer: 0\n",
      "Problem 222ccc: (0:02) -- Predict Answer: 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44/4154348110.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minference_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_44/2278285577.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     print(f\"📊 Total {total_count} problems, ⏱️ total time {_fmt_time(t_time)}; \"\n\u001b[0;32m---> 64\u001b[0;31m           \u001b[0;34mf\"Running Accuracy 🎯: {correct_count}/{total_count} ({100*correct_count/total_count:.1f}%)\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m           \u001b[0;34mf\"Max time: {_fmt_time(stats['max_time'][0])} (id={max_id}); \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m           \u001b[0;34mf\"Min time: {_fmt_time(stats['min_time'][0])} (id={min_id}); \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    CFG.debug = False\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "isSourceIdPinned": false,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "datasetId": 9245165,
     "sourceId": 14592987,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 289055161,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 422384,
     "modelInstanceId": 404485,
     "sourceId": 510391,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 492.002791,
   "end_time": "2026-01-25T23:28:00.269803",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-25T23:19:48.267012",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
